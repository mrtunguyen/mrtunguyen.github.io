{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /blog/2023/11/23/finetuning_inference_zypher7b/\n",
    "categories:\n",
    "- LLM\n",
    "date: '2023-11-23'\n",
    "description: Finetuning and inference Zypher7B \n",
    "layout: post\n",
    "title: Finetuning and inference Zypher7B \n",
    "toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Zypher7B?\n",
    "Zephyr-7B comprises two models created by the Hugging Face 4 team, derived from the well-known Mistral-7B model: Zephyr-7B-α and Zephyr-7B-β. These models not only outperform the Mistral-7B models but also exhibit performance comparable to LLaMA2-Chat-70B, which are models ten times their size. \n",
    "\n",
    "![Performance of Zephyr-7b compared to other models](Zephyr-7B Fine-T.png)\n",
    "\n",
    "## How it works?\n",
    "More details can be found in the [Zephyr: Direct Distillation of LM Alignment](https://arxiv.org/abs/2310.16944).\n",
    "![Architecture of Zephyr-7b](zypher-archi.png)\n",
    "\n",
    "### Distilled supervised fine-tuning (dSFT) \n",
    "SFT, serving as the initial training phase for instructive/chat models, necessitates an instruction dataset, comprising pairs of instructions or questions alongside responses provided by humans. However, the primary challenge lies in the high cost associated with collecting such a dataset, given the requirement for human labor. An increasingly prevalent and cost-effective alternative is to utilize instruction datasets generated by other Large Language Models (LLMs).\n",
    "\n",
    "We can find many such instruction datasets on the Hugging Face Hub that we can use for SFT, for instance:\n",
    "\n",
    "- OpenAssistant Conversations Dataset (OASST1) (84.4k training examples)\n",
    "- OpenOrca (4.2M training examples)\n",
    "- openassistant-guanaco (9.8k training examples)\n",
    "\n",
    "For Zephyr 7B Beta, Hugging Face fine-tuned Mistral 7B on a custom version of Ultrachat that they aggressively filtered:\n",
    "- HuggingFaceH4/ultrachat_200k (MIT license), use the \"sft\" splits\n",
    "\n",
    "### AI feedback (AIF)\n",
    "For alignment with humans, we need a dataset of prompts paired with ranked answers. It's common to use human feedback to align LLMs. Zephyr, however, uses AI feedback (AIF) since ranking models' answers is an expensive task requiring human labor. \n",
    "\n",
    "Starting with a collection of 4 different models like Claude, Llama, Falcon, etc, each prompt is fed through all 4 models to produce text. The teacher model, GPT-4, then gives a score for each produced text. The highest score of the 4 responses is called $y_w$ and a random lower-scoring response is called $y_l$ \n",
    "Thus, from a list of prompts $\\{x_1, ..., x_j\\}$, we derive a dataset D = $\\{(x_1, y_1^w, y_1^l), ..., (x_j, y_j^w, y_j^l)\\}$. These are 3-tuples of prompts with a stronger and a weaker response.\n",
    "\n",
    "For this step, Hugging Face directly used the dataset UltraFeedback.\n",
    "\n",
    "UltraFeedback contains 74k prompts paired with responses generated by the following models:\n",
    "\n",
    "- LLaMA-2–7B-chat, LLaMA-2–13B-chat, LLaMA-2–70B-chat\n",
    "- UltraLM-13B, UltraLM-65B\n",
    "- WizardLM-7B, WizardLM-13B, WizardLM-70B\n",
    "- Vicuna-33B\n",
    "- Alpaca-7B\n",
    "- Falcon-40B-instruct\n",
    "- MPT-30B-chat\n",
    "- StarChat-Beta\n",
    "- Pythia-12B\n",
    "\n",
    "Each LLM's output is rated by GPT-4 with a score from 1 to 5 (higher is better) for various criteria:\n",
    "\n",
    "- instruction following\n",
    "- helpfulness\n",
    "- honesty\n",
    "- truthfulness\n",
    "\n",
    "### Distilled direct preference optimization (dDPO) \n",
    "Instruct Large Language Models (LLMs), such as chat models, are commonly trained using Reinforcement Learning with Human Feedback (RLHF), employing a technique called Proximal Policy Optimization (PPO). While RLHF effectively aligns LLMs with human preferences, it comes with challenges of instability and complexity. To address these issues, a two-step training process is employed before running RLHF:\n",
    "\n",
    "- Reference Model Training: A reference model is initially trained using Supervised Fine-Tuning (SFT) on an instruction dataset.\n",
    "- Reward Model Training: A reward model is trained to predict human preferences. This involves using training data where humans rank the outputs of models for a given prompt. The reward model is then trained to predict these rankings.\n",
    "\n",
    "After these preliminary steps, RLHF involves the use of four different models:\n",
    "\n",
    "- Reference Model (SFT): The model trained using SFT on the instruction dataset.\n",
    "- Reward Model: The model trained to predict human preferences based on ranked outputs.\n",
    "- Value Model: Typically initialized by the reward model, the value model is an additional component in RLHF.\n",
    "- Policy Model: The model (policy) that undergoes training with RLHF. It is typically initialized by the reference model. \n",
    "Using all these models, RLHF uses RL to optimize a language model policy to produce responses with a high reward (according to the reward model) without drifting excessively far from the original reference model.\n",
    "This multi-step approach, involving reference model training, reward model training, and the use of multiple models in RLHF, is designed to enhance the stability and effectiveness of instruct LLMs, aligning them more closely with human preferences.\n",
    "\n",
    "[DPO](https://arxiv.org/pdf/2305.18290.pdf) is a simple alternative to RLHF. It implicitly optimizes the same objective as existing RLHF algorithms (reward maximization with a KL-divergence constraint). The authors of DPO demonstrate that the constrained reward maximization problem can be exactly optimized by solving a much simpler classification problem on human preferences. DPO is lightweight and is more stable according to the authors. The Zephyr authors call this dDPO because the dataset is distilled from earlier steps, leveraging an AI to provide preference labels. \n",
    "\n",
    "Since it can be reduced to a classification problem, DPO trains the model using a simple binary cross-entropy objective. DPO completely eliminates the need for reinforcement learning. \n",
    "\n",
    "Given a prompt and several LLMs' outputs ranked by humans according to their quality, DPO trains the model to assign a higher reward to the best outputs.\n",
    "\n",
    "DPO only requires two models:\n",
    "- The reference model fine-tuned with SFT on instruct datasets\n",
    "- The base model that we want to train with DPO\n",
    "\n",
    "![DPO illustration by [Rafailov et al](https://arxiv.org/pdf/2305.18290.pdf)](dpo.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning Zypher-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "from copy import deepcopy\n",
    "from random import randrange\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import accelerate\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "from wandb.sdk.data_types.trace_tree import Trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **transformers**: is HuggingFace's most popular library and their hub for models and training, evaluation, preprocessing, and other pipeline components. \n",
    "- **datasets** gives us the power to load in any dataset from the dataset hub.\n",
    "- **peft** is HuggingFace's parameter-efficient fine-tuning library, especially useful for LLMs and limited hardware. \n",
    "- **trl** is HuggingFace's RL training library for language models.\n",
    "- **accelerate** is for distributed configuration and accelerating your PyTorch script.\n",
    "- **bitsandbytes** is an HuggingFace-integrated library for quantization functions to help with reducing our memory footprint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will download the quite big model directly from the Hugging Face Hub, we should configure the environment of `TRANSFORMERS_CACHE` to a folder with enough space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['TRANSFORMERS_CACHE'] = \"./cache\"\n",
    "os.environ['WANDB_API_KEY'] = '6d21064574dc241c28f085109bb8b4351c2b7f8c'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using W&B to log our experiments. You can create a free account at https://wandb.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33muthnaht\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define the model, then load and preprocess the dataset. We will use a sharded Zephyr 7B to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.43k/1.43k [00:00<00:00, 2.13MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.80M/1.80M [00:00<00:00, 4.28MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 168/168 [00:00<00:00, 739kB/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"anakin87/zephyr-7b-alpha-sharded\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bits and Bytes Config & Loading the Model\n",
    "\n",
    "This step is to define our BitsAndBytesConfig. This will significantly reduce memory consumption when we load in our sharded Zypher 7B model. The configure will be as bellow: \n",
    "\n",
    "- load in 4 bits: we can divide the used memory by 4 and import the model on smaller devices.\n",
    "- double quantize (quantize the weights and quantize the first quantization's constants)\n",
    "- use NF4 (normalized fp4)\n",
    "- compute type is bfloat16 (computations run in bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we load our model. We disable the cache to avoid the conflict with gradient checkpoint that we will enable right after. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/thanhtu/Desktop/perso/mrtunguyen.github.io/posts/finetuning-llm/finetuning_inference_zypher7b.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/thanhtu/Desktop/perso/mrtunguyen.github.io/posts/finetuning-llm/finetuning_inference_zypher7b.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thanhtu/Desktop/perso/mrtunguyen.github.io/posts/finetuning-llm/finetuning_inference_zypher7b.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     model_name,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thanhtu/Desktop/perso/mrtunguyen.github.io/posts/finetuning-llm/finetuning_inference_zypher7b.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     quantization_config\u001b[39m=\u001b[39;49mbnb_config,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thanhtu/Desktop/perso/mrtunguyen.github.io/posts/finetuning-llm/finetuning_inference_zypher7b.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     no_cuda\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thanhtu/Desktop/perso/mrtunguyen.github.io/posts/finetuning-llm/finetuning_inference_zypher7b.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     use_mps_device\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,  \u001b[39m# Auto selects device to put model on.\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thanhtu/Desktop/perso/mrtunguyen.github.io/posts/finetuning-llm/finetuning_inference_zypher7b.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thanhtu/Desktop/perso/mrtunguyen.github.io/posts/finetuning-llm/finetuning_inference_zypher7b.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thanhtu/Desktop/perso/mrtunguyen.github.io/posts/finetuning-llm/finetuning_inference_zypher7b.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#model.gradient_checkpointing_enable()\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/perso/finetune_llm/finetune/.venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    567\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    568\u001b[0m     )\n\u001b[1;32m    569\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/perso/finetune_llm/finetune/.venv/lib/python3.11/site-packages/transformers/modeling_utils.py:2714\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2712\u001b[0m \u001b[39mif\u001b[39;00m load_in_8bit \u001b[39mor\u001b[39;00m load_in_4bit:\n\u001b[1;32m   2713\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_accelerate_available() \u001b[39mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[0;32m-> 2714\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m   2715\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2716\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2717\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m pip install bitsandbytes` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2718\u001b[0m         )\n\u001b[1;32m   2720\u001b[0m     \u001b[39mif\u001b[39;00m torch_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2721\u001b[0m         \u001b[39m# We force the `dtype` to be float16, this is a requirement from `bitsandbytes`\u001b[39;00m\n\u001b[1;32m   2722\u001b[0m         logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   2723\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOverriding torch_dtype=\u001b[39m\u001b[39m{\u001b[39;00mtorch_dtype\u001b[39m}\u001b[39;00m\u001b[39m with `torch_dtype=torch.float16` due to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2724\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2725\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2726\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m torch_dtype=torch.float16 to remove this warning.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2727\u001b[0m         )\n",
      "\u001b[0;31mImportError\u001b[0m: Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` "
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    no_cuda=True,\n",
    "    use_mps_device=True,  # Auto selects device to put model on.\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "#model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will do somethings special to enable us train the 7B model in a single GPU.\n",
    "- freezes the model weights\n",
    "- cast all non INT8 parameters (layer norm and lm head) to fp32 if the model is not gptq quantized\n",
    "- enable_input_require_grads: Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while keeping the model weights fixed.\n",
    "- gradient_checkpointing_enable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The gradient checkpoint is a technique to reduce the memory footprint of the model. It will save the activations of the model and recomputes them during the backward pass. This is a trade-off between memory and compute. We will use the gradient checkpoint to reduce the memory footprint of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using LoRA\n",
    "\n",
    "[Lora](https://arxiv.org/abs/2106.09685) is a technique that accelerates the fine-tuning of large models while consuming less memory. \n",
    "\n",
    "In order to enhance the efficiency of fine-tuning, LoRA employs a strategy involving the representation of weight updates using two smaller matrices referred to as **\"update matrices\"** via low-rank decomposition. These newly introduced matrices can be trained to accommodate new data while minimizing the overall magnitude of modifications. The original weight matrix remains unchanged and undergoes no further adjustments. The ultimate results are derived by combining both the original and the adapted weights.\n",
    "\n",
    "First we need to define the LoRa config. \n",
    "LoraConfig allows you to control how LoRA is applied to the base model through the following parameters:\n",
    "\n",
    "- `r` : the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
    "- `target_modules`: The modules (for example, attention blocks) to apply the LoRA update matrices.\n",
    "- `lora_alpha` : LoRA scaling factor.\n",
    "- `lora_dropout``: The dropout probability for Lora layers.\n",
    "- `bias`: Specifies if the bias parameters should be trained. Can be 'none', 'all' or 'lora_only'. If ‘all’ or ‘lora_only’, the corresponding biases will be updated during training. Be aware that this means that, even when disabling the adapters, the model will not produce the same output as the base model would have without adaptation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_peft_config(modules):\n",
    "    lora_alpha = 16\n",
    "    lora_dropout = 0.1\n",
    "    lora_r = 8\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=lora_alpha, # parameter for scaling\n",
    "        lora_dropout=lora_dropout, # dropout probability for layers\n",
    "        target_modules=modules,\n",
    "        r=lora_r, # dimension of the updated matrices\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    return peft_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous function needs the `target modules` to update the necessary matrices. The following function will return a list of layer names for LoRA to be applied to. These include the `q, k, o, v proj` layers and the gated, up, and down layers in the MLPs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    # lm_head is often excluded.\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "modules = find_all_linear_names(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create our LoRA-applied model which is wrapped as PeftModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can know how many parameters are trainable in our model and measure the memory footprint of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable, total = model.get_nb_trainable_parameters()\n",
    "print(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be fine-tuning Mistral 7B on the [Puffin](https://huggingface.co/datasets/LDJnr/Puffin) dataset, 3000 multi-turn conversations between a user and GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"LDJnr/Puffin\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a peak at the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset:\n",
    "  if len(i[\"conversations\"]) > 2:\n",
    "    for j in i[\"conversations\"]:\n",
    "      print(j)  # Conversations are multi-turn (>= 2) and always even in count (human then gpt response).\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
