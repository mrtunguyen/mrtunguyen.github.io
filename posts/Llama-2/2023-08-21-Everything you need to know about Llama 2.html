<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2023-08-21">
<meta name="description" content="LLaMA 2, the successor of the original LLaMA 1, is a large language model created by Meta.">

<title>Machine learning Blog - Everything you need to know about Llama 2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Machine learning Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">Jonathan Tu Nguyen</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/mrtunguyen" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/jonathan_ttu" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Everything you need to know about Llama 2</h1>
                  <div>
        <div class="description">
          LLaMA 2, the successor of the original LLaMA 1, is a large language model created by Meta.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">LLM</div>
                <div class="quarto-category">Llama 2</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 21, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#everything-you-need-to-know-about-llama-2" id="toc-everything-you-need-to-know-about-llama-2" class="nav-link active" data-scroll-target="#everything-you-need-to-know-about-llama-2">Everything you need to know about Llama 2</a></li>
  <li><a href="#what-is-llama-2" id="toc-what-is-llama-2" class="nav-link" data-scroll-target="#what-is-llama-2">What is llama 2?</a>
  <ul class="collapse">
  <li><a href="#is-it-trully-open-sourced" id="toc-is-it-trully-open-sourced" class="nav-link" data-scroll-target="#is-it-trully-open-sourced">Is it trully open sourced?</a></li>
  <li><a href="#what-is-llama-2-base-and-llama-2-chat" id="toc-what-is-llama-2-base-and-llama-2-chat" class="nav-link" data-scroll-target="#what-is-llama-2-base-and-llama-2-chat">What is Llama 2 base and Llama 2 chat?</a></li>
  </ul></li>
  <li><a href="#where-can-i-try-it-for-free" id="toc-where-can-i-try-it-for-free" class="nav-link" data-scroll-target="#where-can-i-try-it-for-free">Where can I try it for free?</a></li>
  <li><a href="#where-i-can-download-it-directly" id="toc-where-i-can-download-it-directly" class="nav-link" data-scroll-target="#where-i-can-download-it-directly">Where I can download it directly?</a></li>
  <li><a href="#how-good-is-llama-2" id="toc-how-good-is-llama-2" class="nav-link" data-scroll-target="#how-good-is-llama-2"><strong>How good is LLaMA 2?</strong></a></li>
  <li><a href="#how-to-finetune-llama-2-with-our-own-data" id="toc-how-to-finetune-llama-2-with-our-own-data" class="nav-link" data-scroll-target="#how-to-finetune-llama-2-with-our-own-data">How to finetune Llama 2 with our own data?</a>
  <ul class="collapse">
  <li><a href="#how-to-extend-context-for-llama-2-beyond-4k" id="toc-how-to-extend-context-for-llama-2-beyond-4k" class="nav-link" data-scroll-target="#how-to-extend-context-for-llama-2-beyond-4k">How to extend context for LLama 2, beyond 4K?</a></li>
  </ul></li>
  <li><a href="#how-to-deploy-llama-2" id="toc-how-to-deploy-llama-2" class="nav-link" data-scroll-target="#how-to-deploy-llama-2"><strong>How to deploy LLama 2?</strong></a></li>
  <li><a href="#deep-dive-into-llama-2" id="toc-deep-dive-into-llama-2" class="nav-link" data-scroll-target="#deep-dive-into-llama-2">Deep dive into Llama 2</a>
  <ul class="collapse">
  <li><a href="#what-is-training-data-for-base-model" id="toc-what-is-training-data-for-base-model" class="nav-link" data-scroll-target="#what-is-training-data-for-base-model">What is training data for base model?</a></li>
  <li><a href="#what-is-preference-data-for-reward-model" id="toc-what-is-preference-data-for-reward-model" class="nav-link" data-scroll-target="#what-is-preference-data-for-reward-model">What is preference data for reward model?</a></li>
  <li><a href="#how-to-train-reward-models" id="toc-how-to-train-reward-models" class="nav-link" data-scroll-target="#how-to-train-reward-models">How to train reward models?</a></li>
  <li><a href="#what-is-training-process-of-llama-2-chat" id="toc-what-is-training-process-of-llama-2-chat" class="nav-link" data-scroll-target="#what-is-training-process-of-llama-2-chat">What is training process of Llama 2-chat?</a></li>
  </ul></li>
  <li><a href="#examples-recipes" id="toc-examples-recipes" class="nav-link" data-scroll-target="#examples-recipes">Examples &amp; Recipes</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="everything-you-need-to-know-about-llama-2" class="level1">
<h1>Everything you need to know about Llama 2</h1>
<p>LLaMA 2, the successor of the original LLaMA 1, is a massive language model created by Meta. It is open for both research and commercial purposes, made available through various providers like AWS and Hugging Face. The pretrained models of LLaMA 2 have undergone extensive training on an impressive 2 trillion tokens, offering twice the context length compared to LLaMA 1. Additionally, its fine-tuned models have been refined using over 1 million human annotations.</p>
</section>
<section id="what-is-llama-2" class="level1">
<h1>What is llama 2?</h1>
<p>LLaMA 2 is the new state-of-the-art open large language model (LLM), released by Meta. LLaMA 2 represents the next iteration of LLaMA and comes with a commercially-permissive license. LLaMA 2 comes in 3 different sizes - 7B, 13B, and 70B parameters. The pretrained models come with significant improvements over the Llama 1 models, including being trained on 40% more tokens (around 2 trillion tokens), having a much longer context length (4k tokens), and using grouped-query attention for fast inference of the 70B model.</p>
<section id="is-it-trully-open-sourced" class="level2">
<h2 class="anchored" data-anchor-id="is-it-trully-open-sourced">Is it trully open sourced?</h2>
<p>Technically, the whole project is not open-source because the development and use of it is not fully available to the entire public. While the model is open to public, it is very <em>useful for the open-source community</em>, but we should call it an open release instead of open source.</p>
</section>
<section id="what-is-llama-2-base-and-llama-2-chat" class="level2">
<h2 class="anchored" data-anchor-id="what-is-llama-2-base-and-llama-2-chat">What is Llama 2 base and Llama 2 chat?</h2>
<p>The base models are uncensored, and are not instruct-tuned or chat-tuned.</p>
<p>The chat models are censored, and have been chat-tuned, are optimized for dialogue use cases.</p>
</section>
</section>
<section id="where-can-i-try-it-for-free" class="level1">
<h1>Where can I try it for free?</h1>
<p>There are several free playgrounds to try out Llama 2:</p>
<ol type="1">
<li><a href="https://huggingface.co/chat">HuggingChat</a> allows you to chat with the LLaMA 2 70B model through Hugging Face’s conversational interface.</li>
<li><a href="https://labs.perplexity.ai/">Perplexity chat</a> has both the 7B, 13B and 70B LLaMA 2 models on their chat interface.</li>
<li><a href="https://llama2.ai/">https://llama2.ai/</a> created by a16z</li>
<li>13B-chat model by <a href="https://twitter.com/skirano/status/1681381051237556224">Pietro</a>: <a href="https://llama-2.replit.app/">https://llama-2.replit.app</a></li>
</ol>
</section>
<section id="where-i-can-download-it-directly" class="level1">
<h1>Where I can download it directly?</h1>
<p>There are several ways to download Llama 2 models directly to your machine:</p>
<ol type="1">
<li>You can download any of the LLaMA 2 models <a href="https://ai.meta.com/llama/">directly from Meta</a>. You’ll be required to input some personal information and agree to the community licence agreement and acceptable use policy first, and this will submit your request. You’ll get a follow-up email giving you the unique URL you need to initiate the download from GitHub.</li>
<li>You can also download the models from <a href="https://huggingface.co/docs/transformers/main/model_doc/llama2">Hugging Face</a>, a community platform for sharing machine learning models and datasets. However, you’ll need to have access granted by Meta first, agreeing to the same terms, and your Hugging Face account email must match what you provided in your request to Meta.</li>
</ol>
</section>
<section id="how-good-is-llama-2" class="level1">
<h1><strong>How good is LLaMA 2?</strong></h1>
<ul>
<li><p>In the <a href="https://ai.meta.com/llama/">annoucement</a> of Meta, they claim that Llama 2 outperforms other open source language models on many external benchmarks, including reasoning, coding, proficiency, and knowledge tests.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="benchmark.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Untitled</figcaption>
</figure>
</div></li>
<li><p>In the <a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">Huggingface Open LLM Leaderboard</a>, the top ranking is flooded by the base model Llama 2 and its instruction-tuned models.</p></li>
<li><p>This is the first time we can see an open LLM model is <strong>on the level of ChatGPT (except in coding), while t</strong>here is still a large gap in performance between Llama 2 70B and GPT-4 and PaLM-2-L. **** <a href="https://arxiv.org/pdf/2307.09288.pdf">Source</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="benchmark2.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Capture d’écran 2023-08-01 à 14.26.23.png</figcaption>
</figure>
</div></li>
</ul>
</section>
<section id="how-to-finetune-llama-2-with-our-own-data" class="level1">
<h1>How to finetune Llama 2 with our own data?</h1>
<p>There is a bunch of great sources available for finetuning Llama 2 with your own data:</p>
<ol type="1">
<li><a href="https://colab.research.google.com/drive/1PEQyJO1-f6j0S_XJ8DV50NkpzasXkrzd?usp=sharing#scrollTo=crj9svNe4hU5">Fine-tune 7B model Llama 2 in Google Colab</a>: This could even run with free version.</li>
<li><a href="https://www.notion.so/Everything-you-need-to-know-about-Llama-2-922143286f6e4663b0a38c3e29d20f56?pvs=21">Extended Guide: Instruction-tune Llama 2</a></li>
<li><a href="https://huggingface.co/blog/llama2#fine-tuning-with-peft">Fine-tuning with PEFT</a></li>
<li><a href="https://www.youtube.com/watch?v=3fsn19OI_C8">The EASIEST way to finetune LLAMA-v2 on local machine!</a></li>
<li><a href="https://github.com/facebookresearch/llama-recipes/tree/main#fine-tuning">Finetuning recipe from Meta</a></li>
<li><a href="https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/">Fine-Tuning LLaMA 2 Models using a single GPU, QLoRA and AI Notebooks</a></li>
</ol>
<section id="how-to-extend-context-for-llama-2-beyond-4k" class="level2">
<h2 class="anchored" data-anchor-id="how-to-extend-context-for-llama-2-beyond-4k">How to extend context for LLama 2, beyond 4K?</h2>
<p>We can extend context from 4K to 8k, 32k or 128k tokens with technique using <a href="https://arxiv.org/abs/2306.15595">Position Interpolation</a></p>
<ul>
<li><strong><a href="https://www.notion.so/Everything-you-need-to-know-about-Llama-2-922143286f6e4663b0a38c3e29d20f56?pvs=21">Extending Context is Hard…but not Impossible</a>:</strong> awesome blog on the subject</li>
<li><strong><a href="https://github.com/abacusai/Long-Context">Long-Context</a>:</strong> Extending LLM Context Length. A range of experiments with different schemes for extending context length capabilities of Llama, which has been pretrained on 2048 context length with the RoPE (Rotary Position Embedding) encoding.</li>
</ul>
</section>
</section>
<section id="how-to-deploy-llama-2" class="level1">
<h1><strong>How to deploy LLama 2?</strong></h1>
<p>LLaMA 2 can be deployed in local environment (<a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>) or using managed services like <a href="https://ui.endpoints.huggingface.co/">Hugging Face Inference Endpoints</a></p>
<p>Some other resources:</p>
<ul>
<li><a href="https://github.com/turboderp/exllama">exllama</a>: A more memory-efficient rewrite of the HF transformers implementation of Llama for use with quantized weights.</li>
<li><a href="https://github.com/OpenNMT/CTranslate2">CTranslate2</a>: Fast inference engine for Transformer models</li>
<li><a href="https://github.com/marella/ctransformers">ctransformers</a>: Python bindings for the Transformer models implemented in C/C++ using GGML library.</li>
<li><a href="https://vllm.readthedocs.io/en/latest/">vLLM</a>: supports distributed inference</li>
<li><a href="https://github.com/huggingface/text-generation-inference">Text Generation Inference</a> from Hugging face, has some nice features like telemetry baked in (<a href="https://opentelemetry.io/docs/concepts/signals/traces/">via OpenTelemetry</a>) and integration with the HF ecosystem like <a href="https://huggingface.co/inference-endpoints">inference endpoints</a>. One thing to note that as of 7/28/2023, the license for TGI was changed to be more <strong><a href="https://github.com/huggingface/text-generation-inference/commit/bde25e62b33b05113519e5dbf75abda06a03328e">restrictive that may interfere with certain commercial uses</a></strong>.</li>
<li><a href="https://github.com/karpathy/llama2.c">llama2.c</a> : Inference Llama 2 in one file of pure C from karpathy</li>
</ul>
<p>This <a href="https://betterprogramming.pub/frameworks-for-serving-llms-60b7f7b23407">blog post</a> covers an overview for these frameworks for serving LLMs.</p>
</section>
<section id="deep-dive-into-llama-2" class="level1">
<h1>Deep dive into Llama 2</h1>
<section id="what-is-training-data-for-base-model" class="level2">
<h2 class="anchored" data-anchor-id="what-is-training-data-for-base-model">What is training data for base model?</h2>
<blockquote class="blockquote">
<p>Our training corpus includes a new mix of data from publicly available sources, which does not include data from Meta’s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this provides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations. <a href="https://arxiv.org/pdf/2307.09288.pdf">source</a></p>
</blockquote>
<p>That’s simply all details that Meta gives in their paper, although we are very curious about which datasets contain a high volume of personal information or their detailed technique about up-sampling the factural sources.</p>
</section>
<section id="what-is-preference-data-for-reward-model" class="level2">
<h2 class="anchored" data-anchor-id="what-is-preference-data-for-reward-model">What is preference data for reward model?</h2>
<p>First of all, the reward model is the key of RLHF. In order to get a good reward model, Meta had to push hard on gathering preference data extremely upgraded from what the open-source community is working with.</p>
<p>In summary, the key points about preference data:</p>
<ul>
<li>Use <strong>multi-turn preferences</strong>, where model responses are taken <strong>from different model checkpoints</strong> with varying temperatures to generate diversity between pairs.</li>
<li>binary comparison: either their choice is significantly better, better, slightly better, or negligibly better/ unsure.</li>
<li>Focus on <strong>helpfulness and safety</strong> (as opposed to honesty), using separate guidelines at data collection time for each data vendor (e.g.&nbsp;safety is often a much more deceptive prompting style). This is most contrasted to Anthropic’s works, where they train a model that is Helpful, Honest, and Harmless.</li>
<li>I<strong>terative collection for distribution management</strong>: “Human annotations were collected in batches on a weekly basis. As we collected more preference data, our reward models improved, and we were able to train progressively better versions for Llama 2-Chat”</li>
<li>The team added additional <strong>safety metadata</strong> to the collection showcasing which responses are safe from the models at each turn. When this is passed to the modeling phase, they “do not include any examples where the chosen response was unsafe and the other response safe, as we believe safer responses will also be better/preferred by humans.”</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="preference-data-training.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Capture d’écran 2023-08-03 à 00.27.00.png</figcaption>
</figure>
</div>
</section>
<section id="how-to-train-reward-models" class="level2">
<h2 class="anchored" data-anchor-id="how-to-train-reward-models">How to train reward models?</h2>
<p>The reward model takes a model response and its corresponding prompt (including contexts from previous turns) as inputs and outputs a scalar score to indicate the quality (e.g.,helpfulness and safety) of the model generation.</p>
<p>They train <strong>two separate reward models</strong>, one optimized for helpfulness (referred to as Helpfulness RM) and another for safety (Safety RM).</p>
<p>They initialize reward models from pretrained chat model checkpoints, as it ensures that both models benefit from knowledge acquired in pretraining. In short, the reward model “knows” what the chat model knows. The model architecture and hyper-parameters are identical to those of the pretrained language models, except that the classification head for next-token prediction is replaced with a regression head for outputting a scalar reward.</p>
</section>
<section id="what-is-training-process-of-llama-2-chat" class="level2">
<h2 class="anchored" data-anchor-id="what-is-training-process-of-llama-2-chat">What is training process of Llama 2-chat?</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="training-llama2-chat.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Untitled</figcaption>
</figure>
</div>
</section>
</section>
<section id="examples-recipes" class="level1">
<h1>Examples &amp; Recipes</h1>
<p><a href="https://github.com/facebookresearch/llama-recipes">Examples and recipes for Llama 2 model</a></p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<ol type="1">
<li><a href="https://arxiv.org/pdf/2307.09288.pdf">Llama 2: Open Foundation and Fine-Tuned Chat Models</a></li>
<li><a href="https://www.interconnects.ai/p/llama-2-from-meta">Llama 2: an incredible open LLM</a></li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>