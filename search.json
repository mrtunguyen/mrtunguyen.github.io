[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "I’m Jonathan Tu Nguyen, a machine learning engineer.\nI’m a senior machine learning engineer at Voodoo, where I build a bidding system that allows app developers to rapidly monetize their apps by leveraging machine learning and predictive algorithms.\nPreviously, I built a system for extracting relevant information from documents and a speech recognition system that automate the manual process which consists of translating audio calls into text and making business insights from that at BNP Paribas. My technical stack includes working with state-of-the-art machine learning models in recommendation systems, bandit, and statistical models (winning rate probability or click-through rate), computer vision, NLP and putting them into deployment at scale with modern tech frameworks to serve our very large base of users.\nIn my free time, I travel, read, build side projects and participate Kaggle.\nThis is a place where I’m documenting my learning notes, sharing my ideas, thoughts about machine learning/deep learning.\nDrop me an email at jonathan.tunguyen AT gmail.com if you have any questions or just want a discussion.\nThanks."
  },
  {
    "objectID": "posts/2019-10-17-Kaggle-Modecular-Competition.html",
    "href": "posts/2019-10-17-Kaggle-Modecular-Competition.html",
    "title": "Kaggle Molecular Competition: Lessons Learned from Finishing in Top 5",
    "section": "",
    "text": "In the recent Kaggle Predicting Molecular Properties Competition, my team has managed to finish in 5th place (out of 2749 teams). It is the first competition that I spent seriously a lot of time and I learned a lot through it. Though I don’t consider myself as a Kaggle expert by any means, I want to share some lessons, insights that hopefully can be helpful for others."
  },
  {
    "objectID": "posts/2019-10-17-Kaggle-Modecular-Competition.html#kaggle-is-the-best-school-participants-are-the-best-teachers-and-my-teammates-are-the-best-companions-ive-had",
    "href": "posts/2019-10-17-Kaggle-Modecular-Competition.html#kaggle-is-the-best-school-participants-are-the-best-teachers-and-my-teammates-are-the-best-companions-ive-had",
    "title": "Kaggle Molecular Competition: Lessons Learned from Finishing in Top 5",
    "section": "Kaggle is the best school, participants are the best teachers and my teammates are the best companions I’ve had",
    "text": "Kaggle is the best school, participants are the best teachers and my teammates are the best companions I’ve had\nKaggle is undoubtly a great platform with all sorts of interesting problems along with datasets, kernels and great discussions. Like what I saw in a post long time ago, Kaggle is definitely a home for data science enthusiats all around the world where they spend their days and nights to challenge themselves. And now I would say that I’m very proud to be one of them. Since I’m not graduated from any school formation of data science like many of others, Kaggle comes to me as a place where I can learn many things, keep me motivated in the field which is moving a lot every day. The people like Heng are the best teachers I’ve had, not only from their insights and sharing about competitions but also from the way how they work. Moreover, one of the most important features I love about Kaggle is Leaderboard where we could see where we’re standing compared to others. During this competition, I admit that the first thing I did when waking up every morning is looking at the leaderboard. Seeing many competitors passing above me in the ranking helps me to benchmark my skills, push me to learn and try new things which is very important in my career. So if you want to really get into Machine learning or Data Science, I believe that Kaggle comes in as one of the best ways."
  },
  {
    "objectID": "posts/2019-10-17-Kaggle-Modecular-Competition.html#a-good-validation-strategy-is-half-of-success",
    "href": "posts/2019-10-17-Kaggle-Modecular-Competition.html#a-good-validation-strategy-is-half-of-success",
    "title": "Kaggle Molecular Competition: Lessons Learned from Finishing in Top 5",
    "section": "1. A good validation strategy is half of success",
    "text": "1. A good validation strategy is half of success\nI read an interview of bestfitting about his strategy of winning competitions in Kaggle. He said that a good CV is half of success. I couldn’t agree more. Every try we made, it should improve both on our local CV and on the public LB. In this competition, we set aside 5000 moleculars for validation and 80000 ones for training. And luckily, validation score and public leaderboard score is very close so that we are very sure about evaluation of our models. This closeness makes us feel confident with our stacking strategy at the end.\nSo, always trust your CV more than the leaderboard. The public leaderboard represents only 29% of the actual test set, so you can’t be sure about the quality of your solution based on this percentage. Sometimes your model might be great overall, but bad on the data, specially in the public test set. The experience from here and recent finished competition make this lesson more valuable."
  },
  {
    "objectID": "posts/2019-10-17-Kaggle-Modecular-Competition.html#classical-machine-learning-ideas-possibly-work-in-deep-learning",
    "href": "posts/2019-10-17-Kaggle-Modecular-Competition.html#classical-machine-learning-ideas-possibly-work-in-deep-learning",
    "title": "Kaggle Molecular Competition: Lessons Learned from Finishing in Top 5",
    "section": "2. Classical Machine learning ideas possibly work in deep learning",
    "text": "2. Classical Machine learning ideas possibly work in deep learning\nWhen using classical machine learning models like xgboost or Lightgbm, we often heard many times about feature importance technique. While they are widely used in many tabular problems, it is less likely happen in deep learning. But my teammate Guillaume has proven the opposite. After testing feature importance (by randomize one feature at the prediction step), he noticed that the most important feature was by far the angle between an edge and the edge with the closest atom to the first edge. This insight gave us a 0.15 improvement for our best single model."
  },
  {
    "objectID": "posts/2019-10-17-Kaggle-Modecular-Competition.html#always-keep-updated-state-of-the-art-of-the-field-either-nlp-or-computer-vision",
    "href": "posts/2019-10-17-Kaggle-Modecular-Competition.html#always-keep-updated-state-of-the-art-of-the-field-either-nlp-or-computer-vision",
    "title": "Kaggle Molecular Competition: Lessons Learned from Finishing in Top 5",
    "section": "3. Always keep updated state-of-the-art of the field (either NLP or Computer Vision)",
    "text": "3. Always keep updated state-of-the-art of the field (either NLP or Computer Vision)\nI encountered many data scientists who said that since they are only working in Computer Vision, they don’t have any interest to invest their time in NLP models. For me, I don’t feel that way. When this competition was finished and the solution of top teams were released, all top 6 teams, except our team, were using a technique that is recently very popular in NLP community - Transformer. The way that they integrated Transformer in their model is quite eye opening for us."
  },
  {
    "objectID": "posts/2019-10-17-Kaggle-Modecular-Competition.html#test-ideas-with-simple-model-before-going-bigger",
    "href": "posts/2019-10-17-Kaggle-Modecular-Competition.html#test-ideas-with-simple-model-before-going-bigger",
    "title": "Kaggle Molecular Competition: Lessons Learned from Finishing in Top 5",
    "section": "4. Test ideas with simple model before going bigger",
    "text": "4. Test ideas with simple model before going bigger\nOne of the big mistakes I made during this competition is implementing a quite big models in first try. In fact, when my teammates told me about megnet models, I read the paper, write code from scatch and run it with 5 layers while trying to add some new ideas. It took me half day to run and realized that it doesn’t converge at all. Since it is quite deep, I’m kind of stuck in finding why this model doesn’t work as expected. After discussing with my team and simplifing model to only 1 layer, I figured out errors in my implementation. One of the important errors is using Normalization. Indeed, BatchNorm makes the loss fluctuate a lot while LayerNorm works much better in this case."
  },
  {
    "objectID": "posts/2019-10-17-Kaggle-Modecular-Competition.html#how-to-structure-code-when-testing-many-ideas",
    "href": "posts/2019-10-17-Kaggle-Modecular-Competition.html#how-to-structure-code-when-testing-many-ideas",
    "title": "Kaggle Molecular Competition: Lessons Learned from Finishing in Top 5",
    "section": "5. How to structure code when testing many ideas",
    "text": "5. How to structure code when testing many ideas\nOne of the problems when working in projects like Kaggle competitions is that how we can make plan of our code so that we can reproduce the results of previous ideas (even idea we tested one month ago) whenever we want. The problem will get bigger and bigger when we’re trying more sophisticated way that make taking notes difficult. The lesson I’ve learned from Heng when looking at his starter code is that create first folder of dataset which contains all dataset, seconde folder which has common functions and a third folder for my principal code. When I want to test new different idea, I will make a copy of the third model and make changes on it. Following this way can make project management easier and I can reproduce my results whenever I need."
  },
  {
    "objectID": "posts/2019-10-17-Kaggle-Modecular-Competition.html#working-in-a-team-helps-you-go-faster-and-further",
    "href": "posts/2019-10-17-Kaggle-Modecular-Competition.html#working-in-a-team-helps-you-go-faster-and-further",
    "title": "Kaggle Molecular Competition: Lessons Learned from Finishing in Top 5",
    "section": "6. Working in a team helps you go faster and further",
    "text": "6. Working in a team helps you go faster and further\nBesides learning the technical parts of the competition, a very important thing I’ve learned was how to work in a team. We all have work during the week, so we can only do Kaggle at our free time and have to do it in a smart way. Luckily, we worked at the same place, communicated every day and bounced ideas off each other. This competition is my first gold medal! I’m very glad I had this experience and very thankful to be a part of a wonderful team with Lam and Guillaume. I hope we will have more opportunities to work together in future competitions!"
  },
  {
    "objectID": "posts/quantized-llms/quantized-llm.html",
    "href": "posts/quantized-llms/quantized-llm.html",
    "title": "Quantization",
    "section": "",
    "text": "Language models are becoming larger and larger all the time.\nTherefore, these models are hard to run on easily accessible devices. For example, just to do inference on BLOOM-176B, you would need to have 8x 80GB A100 GPUs (~$15k each). To fine-tune BLOOM-176B, you’d need 72 of these GPUs! Much larger models, like PaLM would require even more resources. Because these huge models require so many GPUs to run, we need to find ways to reduce these requirements while preserving the model’s performance. Various technologies have been developed that try to shrink the model size, you may have heard of quantization and distillation, and there are many others."
  },
  {
    "objectID": "posts/quantized-llms/quantized-llm.html#what-is-quantization",
    "href": "posts/quantized-llms/quantized-llm.html#what-is-quantization",
    "title": "Quantization",
    "section": "What is Quantization?",
    "text": "What is Quantization?\nQuantization is a technique to reduce the computational and memory costs of running inference by representing the weights and activations with low-precision data types like 8-bit integer (int8) instead of the usual 32-bit floating point (float32).\nReducing the number of bits means the resulting model requires less memory storage, consumes less energy (in theory), and operations like matrix multiplication can be performed much faster with integer arithmetic. It also allows to run models on embedded devices, which sometimes only support integer data types.\n\nBackground of Neural Networks and Precision\nNeural networks rely are computational models that are represented in computers as data structures called Tensors. Tensors are multi-dimension matrices populated by numbers that can be stored as floating numbers using variables such as float 32bit (single precision) or float 64bit (double precision).\n\nInteger\n\n\n\nInteger\n\n\n\n\nFixed-Point Number\n\n\n\nFixed-Point Number\n\n\n\n\nFloating-Point Number\n\n\n\nFloating-Point Number\n\n\n\n\n\nFloating-Point Number"
  },
  {
    "objectID": "posts/quantized-llms/quantized-llm.html#some-quantization-techniques",
    "href": "posts/quantized-llms/quantized-llm.html#some-quantization-techniques",
    "title": "Quantization",
    "section": "Some Quantization Techniques",
    "text": "Some Quantization Techniques\nWe will cover the most basic quantization techniques to advanced ones.\n\n8-bit Quantization\nThe two most common 8-bit quantization techniques are zero-point quantization and absolute maximum (absmax) quantization. Zero-point quantization and absmax quantization map the floating point values into more compact int8 (1 byte) values\n\nZero Point quantization\n\n\n\nimage.png\n\n\n\nFirst, we calculate the scale factor and the zero-point value:\n\n\\[\\begin{align*}\n\\text{scale} &= \\frac{255}{\\max(\\mathbf{X}) - \\min(\\mathbf{X})} \\\\\n\\text{zeropoint} &= - \\text{round}(\\text{scale} \\cdot \\min(\\mathbf{X})) - 128\n\\end{align*} \\]\n\nwe can use these variables to quantize or dequantize our weights: \\[ \\begin{align*}\n\\mathbf{X}_{\\text{quant}} &= \\text{round}\\bigg(\\text{scale} \\cdot \\mathbf{X} + \\text{zeropoint} \\bigg) \\\\\n\\mathbf{X}_{\\text{dequant}} &= \\frac{\\mathbf{X}_{\\text{quant}} - \\text{zeropoint}}{\\text{scale}}\n\\end{align*} \\]\n\n\nimport torch\n\ndef zeropoint_quantize(X):\n    # Calculate value range (denominator)\n    x_range = torch.max(X) - torch.min(X)\n    x_range = 1 if x_range == 0 else x_range\n\n    # Calculate scale\n    scale = 255 / x_range\n\n    # Shift by zero-point\n    zeropoint = (-scale * torch.min(X) - 128).round()\n\n    # Scale and round the inputs\n    X_quant = torch.clip((X * scale + zeropoint).round(), -128, 127)\n\n    # Dequantize\n    X_dequant = (X_quant - zeropoint) / scale\n\n    return X_quant.to(torch.int8), X_dequant\n\n\n\nAbsolute Maximum (absmax) quantization\nWith absmax quantization, the original number is divided by the absolute maximum value of the tensor and multiplied by a scaling factor (127) to map inputs into the range [-127, 127]. To retrieve the original FP16 values, the INT8 number is divided by the quantization factor, acknowledging some loss of precision due to rounding.\n\\[ \\begin{align*}\n\\mathbf{X}_{\\text{quant}} &= \\text{round}\\Biggl ( \\frac{127}{\\max|\\mathbf{X}|} \\cdot \\mathbf{X} \\Biggr ) \\\\\n\\mathbf{X}_{\\text{dequant}} &= \\frac{\\max|\\mathbf{X}|}{127} \\cdot \\mathbf{X}_{\\text{quant}}\n\\end{align*} \\]\n\nimport torch\n\ndef absmax_quantize(X):\n    # Calculate scale\n    scale = 127 / torch.max(torch.abs(X))\n\n    # Quantize\n    X_quant = (scale * X).round()\n\n    # Dequantize\n    X_dequant = X_quant / scale\n\n    return X_quant.to(torch.int8), X_dequant\n\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\ntorch.manual_seed(0)\n\n# Set device to CPU for now\ndevice = 'cpu'\n\n# Load model and tokenizer\nmodel_id = 'gpt2'\nmodel = AutoModelForCausalLM.from_pretrained(model_id).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# Print model size\nprint(f\"Model size: {model.get_memory_footprint():,} bytes\")\n\n/Users/thanhtu/Desktop/perso/llm-from-beginners-to-advanced/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nconfig.json: 100%|██████████| 665/665 [00:00&lt;00:00, 3.36MB/s]\nmodel.safetensors: 100%|██████████| 548M/548M [00:09&lt;00:00, 59.8MB/s] \ngeneration_config.json: 100%|██████████| 124/124 [00:00&lt;00:00, 1.44MB/s]\nvocab.json: 100%|██████████| 1.04M/1.04M [00:00&lt;00:00, 4.00MB/s]\nmerges.txt: 100%|██████████| 456k/456k [00:00&lt;00:00, 2.73MB/s]\ntokenizer.json: 100%|██████████| 1.36M/1.36M [00:00&lt;00:00, 3.33MB/s]\n\n\nModel size: 510,342,192 bytes\n\n\n\n# Extract weights of the first layer\nweights = model.transformer.h[0].attn.c_attn.weight.data\nprint(\"Original weights:\")\nprint(weights)\n\n# Quantize layer using absmax quantization\nweights_abs_quant, _ = absmax_quantize(weights)\nprint(\"\\nAbsmax quantized weights:\")\nprint(weights_abs_quant)\n\n# Quantize layer using absmax quantization\nweights_zp_quant, _ = zeropoint_quantize(weights)\nprint(\"\\nZero-point quantized weights:\")\nprint(weights_zp_quant)\n\nOriginal weights:\ntensor([[-0.4738, -0.2614, -0.0978,  ...,  0.0513, -0.0584,  0.0250],\n        [ 0.0874,  0.1473,  0.2387,  ..., -0.0525, -0.0113, -0.0156],\n        [ 0.0039,  0.0695,  0.3668,  ...,  0.1143,  0.0363, -0.0318],\n        ...,\n        [-0.2592, -0.0164,  0.1991,  ...,  0.0095, -0.0516,  0.0319],\n        [ 0.1517,  0.2170,  0.1043,  ...,  0.0293, -0.0429, -0.0475],\n        [-0.4100, -0.1924, -0.2400,  ..., -0.0046,  0.0070,  0.0198]])\n\nAbsmax quantized weights:\ntensor([[-21, -12,  -4,  ...,   2,  -3,   1],\n        [  4,   7,  11,  ...,  -2,  -1,  -1],\n        [  0,   3,  16,  ...,   5,   2,  -1],\n        ...,\n        [-12,  -1,   9,  ...,   0,  -2,   1],\n        [  7,  10,   5,  ...,   1,  -2,  -2],\n        [-18,  -9, -11,  ...,   0,   0,   1]], dtype=torch.int8)\n\nZero-point quantized weights:\ntensor([[-20, -11,  -3,  ...,   3,  -2,   2],\n        [  5,   8,  12,  ...,  -1,   0,   0],\n        [  1,   4,  18,  ...,   6,   3,   0],\n        ...,\n        [-11,   0,  10,  ...,   1,  -1,   2],\n        [  8,  11,   6,  ...,   2,  -1,  -1],\n        [-18,  -8, -10,  ...,   1,   1,   2]], dtype=torch.int8)\n\n\nWe can compare these techniques by quantizing every layer in GPT-2 (linear layers, attention layers, etc.) and create two new models: model_abs and model_zp. To be precise, we will actually replace the original weights with de-quantized ones. This has two benefits: it allows us to 1/ compare the distribution of our weights (same scale) and 2/ actually run the models.\n\nimport numpy as np\nfrom copy import deepcopy\n\n# Store original weights\nweights = [param.data.clone() for param in model.parameters()]\n\n# Create model to quantize\nmodel_abs = deepcopy(model)\n\n# Quantize all model weights\nweights_abs = []\nfor param in model_abs.parameters():\n    _, dequantized = absmax_quantize(param.data)\n    param.data = dequantized\n    weights_abs.append(dequantized)\n\n# Create model to quantize\nmodel_zp = deepcopy(model)\n\n# Quantize all model weights\nweights_zp = []\nfor param in model_zp.parameters():\n    _, dequantized = zeropoint_quantize(param.data)\n    param.data = dequantized\n    weights_zp.append(dequantized)\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\n# Flatten weight tensors\nweights = np.concatenate([t.cpu().numpy().flatten() for t in weights])\nweights_abs = np.concatenate([t.cpu().numpy().flatten() for t in weights_abs])\nweights_zp = np.concatenate([t.cpu().numpy().flatten() for t in weights_zp])\n\n# Set background style\nplt.style.use('ggplot')\n\n# Create figure and axes\nfig, axs = plt.subplots(2, figsize=(10,10), dpi=300, sharex=True)\n\n# Plot the histograms for original and zero-point weights\naxs[0].hist(weights, bins=150, alpha=0.5, label='Original weights', color='blue', range=(-2, 2))\naxs[0].hist(weights_abs, bins=150, alpha=0.5, label='Absmax weights', color='red', range=(-2, 2))\n\n# Plot the histograms for original and absmax weights\naxs[1].hist(weights, bins=150, alpha=0.5, label='Original weights', color='blue', range=(-2, 2))\naxs[1].hist(weights_zp, bins=150, alpha=0.5, label='Zero-point weights', color='green', range=(-2, 2))\n\n# Add grid\nfor ax in axs:\n    ax.grid(True, linestyle='--', alpha=0.6)\n\n# Add legend\naxs[0].legend()\naxs[1].legend()\n\n# Add title and labels\naxs[0].set_title('Comparison of Original and Absmax Quantized Weights', fontsize=16)\naxs[1].set_title('Comparison of Original and Zeropoint Quantized Weights', fontsize=16)\n\nfor ax in axs:\n    ax.set_xlabel('Weights', fontsize=14)\n    ax.set_ylabel('Count', fontsize=14)\n    ax.yaxis.set_major_formatter(ticker.EngFormatter()) # Make y-ticks more human readable\n\n# Improve font\nplt.rc('font', size=12)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\ndef generate_text(model, input_text, max_length=50):\n    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n    output = model.generate(inputs=input_ids,\n                            max_length=max_length,\n                            do_sample=True,\n                            top_k=30,\n                            pad_token_id=tokenizer.eos_token_id,\n                            attention_mask=input_ids.new_ones(input_ids.shape))\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n\n# Generate text with original and quantized models\noriginal_text = generate_text(model, \"I have a dream\")\nabsmax_text   = generate_text(model_abs, \"I have a dream\")\nzp_text       = generate_text(model_zp, \"I have a dream\")\n\nprint(f\"Original model:\\n{original_text}\")\nprint(\"-\" * 50)\nprint(f\"Absmax model:\\n{absmax_text}\")\nprint(\"-\" * 50)\nprint(f\"Zeropoint model:\\n{zp_text}\")\n\n\nOriginal model:\nI have a dream and I am ready to help you to find it. \"\n\n\n\"If you are able to contact me, please let me know if I am able to speak.\"\n\n\n\"Do not hesitate to share this with me as\n--------------------------------------------------\nAbsmax model:\nI have a dream of becoming an astronaut someday,\" he said. \"I had a lot of fun in Paris until the late 70s. It's my dream, but I just wanted to make it,\" said Mr. Smith, who flew from Toronto\n--------------------------------------------------\nZeropoint model:\nI have a dream that you will be at this stage when you will be able to take your life,\" the statement said.\n\nThe man, whose last name was not revealed, was detained for allegedly attempting to smuggle a package containing cocaine worth\n\n\n\ndef calculate_perplexity(model, text):\n    # Encode the text\n    encodings = tokenizer(text, return_tensors='pt').to(device)\n\n    # Define input_ids and target_ids\n    input_ids = encodings.input_ids\n    target_ids = input_ids.clone()\n\n    with torch.no_grad():\n        outputs = model(input_ids, labels=target_ids)\n\n    # Loss calculation\n    neg_log_likelihood = outputs.loss\n\n    # Perplexity calculation\n    ppl = torch.exp(neg_log_likelihood)\n\n    return ppl\n\nppl     = calculate_perplexity(model, original_text)\nppl_abs = calculate_perplexity(model_abs, absmax_text)\nppl_zp  = calculate_perplexity(model_zp, absmax_text)\n\nprint(f\"Original perplexity:  {ppl.item():.2f}\")\nprint(f\"Absmax perplexity:    {ppl_abs.item():.2f}\")\nprint(f\"Zeropoint perplexity: {ppl_zp.item():.2f}\")\n\nOriginal perplexity:  10.17\nAbsmax perplexity:    13.77\nZeropoint perplexity: 14.14"
  },
  {
    "objectID": "posts/quantized-llms/quantized-llm.html#optimal-brain-quantization",
    "href": "posts/quantized-llms/quantized-llm.html#optimal-brain-quantization",
    "title": "Quantization",
    "section": "Optimal Brain Quantization",
    "text": "Optimal Brain Quantization\nLet’s start by introducing the problem we’re trying to solve. For every layer \\(\\ell\\) in the network, we want to find a quantized version \\(\\widehat{\\mathbf{W}}_\\ell\\) of the original weights \\(\\mathbf{W}_\\ell\\). This is called the layer-wise compression problem. More specifically, to minimize performance degradation, we want the outputs (\\(\\mathbf{\\widehat{W}_\\ell X_\\ell}\\)) of these new weights to be as close as possible to the original ones (\\(\\mathbf{W_\\ell X_\\ell}\\)). In other words, we want to find:\n\\[\\arg \\min_{\\mathbf{\\widehat{W}}_\\ell} \\parallel\\mathbf{W_\\ell X_\\ell} - \\mathbf{\\widehat{W}_\\ell X_\\ell}\\parallel_2^2.\\]\nDifferent approaches have been proposed to solve this problem, but we’re interested in the Optimal Brain Quantizer (OBQ) framework here.\nThis method is inspired by a pruning technique to carefully remove weights from a fully trained dense neural network (Optimal Brain Surgeon). It uses an approximation technique and provides explicit formulas for the best single weight \\(w_q\\) to remove and optimal update \\(\\delta_F\\) to adjust the set of remaining non-quantized weights \\(F\\) to make up for the removal:\n\\[\\begin{align*}\nw_q &= \\arg\\min_{w_q} \\frac{(\\text{quant}(w_q) - w_q)^2}{[\\mathbf{H}_F^{-1}]_{qq}},\\\\ \\quad \\delta_F &= -\\frac{w_q - \\text{quant}(w_q)}{[\\mathbf{H}_F^{-1}]_{qq}} \\cdot (\\mathbf{H}_F^{-1})_{:,q}.\n\\end{align*}\\]\nwhere \\(\\text{quant}(w)\\) is the weight rounding given by the quantization and \\(\\mathbf{H}_F\\) is the Hessian.\nUsing OBQ, we can quantize the easiest weight first and then adjust all remaining non-quantized weights to compensate for this precision loss. Then we pick the next weight to quantize, and so on.\nA potential issue with this approach is when there are outlier weights, which can result in high quantization error. Usually, these outliers would be quantized last, when there are few non-quantized weights left that could be adjusted to compensate for the large error. This effect can worsen when some weights are pushed further outside the grid by intermediate updates. A simple heuristic is applied to prevent this: outliers are quantized as soon as they appear.\nThis process could be computationally heavy, especially for LLMs. To deal with this, the OBQ method uses a trick that avoids redoing the entire computation each time a weight is simplified. After quantizing a weight, it adjusts the matrix used in calculations (the Hessian) by removing the row and column associated with that weight (using Gaussian elimination):\n\\[\n\\mathbf{H}^{-1}_{-q} = \\left( \\mathbf{H}^{-1} - \\frac{1}{[\\mathbf{H}^{-1}]_{qq}} \\mathbf{H}^{-1}_{:,q} \\mathbf{H}^{-1}_{q,:} \\right)_{-p}.\n\\]\nThe method also employs vectorization to process multiple rows of the weight matrix at once. Despite its efficiency, the OBQ’s computation time increases significantly as the size of the weight matrix increases. This cubic growth makes it difficult to use OBQ on very large models with billions of parameters.\n\nK-Means-based Weight Quantization\nThis technique is covered in this paper and is the most basic quantization technique. The idea is to quantize the weights of a model to 8-bit integers. The quantization is done by first clustering the weights into K clusters using K-Means clustering. Then, the weights are quantized to the nearest cluster center. The cluster centers are then used as the quantized weights. The quantization process is illustrated in the figure below.\n\n\n\nK-Means-based Weight Quantization\n\n\n\nK-Means-based Weight Quantization only saves storage cost of a neural network model. It does not save computational cost because the weights are still represented as 32-bit floating point numbers during inference.\n\n\n\nLinear Quantization"
  },
  {
    "objectID": "posts/RAG/What you should know about RAG (Retrieved-Augmente 5ffd27cf98504d6ca45823cc8e3b118d.html",
    "href": "posts/RAG/What you should know about RAG (Retrieved-Augmente 5ffd27cf98504d6ca45823cc8e3b118d.html",
    "title": "What you should know about RAG (Retrieved-Augmented Generation)",
    "section": "",
    "text": "Large language models (LLMs) have unquestionably revolutionized the way we engage with information. Nevertheless, they are not without their limitations, particularly regarding the scope of queries they can effectively handle. Base LLMs (e.g., Llama-2, gpt-series, etc.) are limited to the knowledge contained within their training data, making them less adept at responding to inquiries that demand knowledge beyond their training set.\nRetrieval augmented generation (RAG)-based LLM applications directly tackle this challenge, enabling LLMs to tap into our domain-specific data sources and significantly expand their capacity to provide relevant information."
  },
  {
    "objectID": "posts/RAG/What you should know about RAG (Retrieved-Augmente 5ffd27cf98504d6ca45823cc8e3b118d.html#how-are-embeddings-generated",
    "href": "posts/RAG/What you should know about RAG (Retrieved-Augmente 5ffd27cf98504d6ca45823cc8e3b118d.html#how-are-embeddings-generated",
    "title": "What you should know about RAG (Retrieved-Augmented Generation)",
    "section": "How are embeddings generated?",
    "text": "How are embeddings generated?\n\nOpenAI embeddings\nWith regard to embeddings, the seemingly popular approach is to use text-embedding-ada-002. Its benefits include ease of use via an API and not having to maintain our own embedding infra or self-host embedding models. Nonetheless, personal experience and anecdotes from others suggest there are better alternatives for retrieval.\nThe OG embedding approaches include Word2vec and fastText. FastText is an open-source, lightweight library that enables users to leverage pre-trained embeddings or train new embedding models. It comes with pre-trained embeddings for 157 languages and is extremely fast, even without a GPU. It’s my go-to for early-stage proof of concepts.\n\n\nSentence Transformers\nIt makes it simple to compute embeddings for sentences, paragraphs, and even images. It’s based on workhorse transformers such as BERT and RoBERTa and is available in more than 100 languages.\n\n\nInstructor Model Embedding\nMore recently, instructor models have shown SOTA performance. During training, these models prepend the task description to the text. Then, when embedding new text, we simply have to describe the task to get task-specific embeddings. (Not that different from instruction tuning for embedding models IMHO.)\nAn example is the E5 family of models. For open QA and information retrieval, we simply prepend documents in the index with passage:, and prepend queries with query:. If the task is symmetric (e.g., semantic similarity, paraphrase retrieval) or if we want to use embeddings as features (e.g., classification, clustering), we just use the query: prefix.\nThe Instructor model takes it a step further, allowing users to customize the prepended prompt: “Represent the domain task_type for the task_objective:” For example, “Represent the Wikipedia document for retrieval:”. (The domain and task objective are optional). This brings the concept of prompt tuning into the field of text embedding.\nCheck more embedding models in this Huggingface leaderboard\n\nIt’s important to keep in mind that the lower the dimensionality of the underlying vectors, the more compact the representation is in embedding space, which can affect downstream task quality. Sentence Transformers (sbert) provides embedding models with a dimension n in the range of 384, 512 and 768, and the models are completely free and open-source. OpenAI and Cohere embeddings, which require a paid API call to generate them, can be considered higher quality due to a dimensionality of a few thousand. One reason it makes sense to use a paid API to generate embeddings is if your data is multilingual."
  },
  {
    "objectID": "posts/RAG/What you should know about RAG (Retrieved-Augmente 5ffd27cf98504d6ca45823cc8e3b118d.html#approximate-nearest-neighbors-ann",
    "href": "posts/RAG/What you should know about RAG (Retrieved-Augmente 5ffd27cf98504d6ca45823cc8e3b118d.html#approximate-nearest-neighbors-ann",
    "title": "What you should know about RAG (Retrieved-Augmented Generation)",
    "section": "Approximate nearest neighbors (ANN)",
    "text": "Approximate nearest neighbors (ANN)\nAfter generating and storing the embedding vectors, the objective of similarity search is to retrieve the top-k most similar vectors to the vector associated with a user’s search query.\nThe simplest approach to accomplish this task involves comparing the query vector to each vector in the database, employing the k-nearest neighbor (kNN) method. However, this approach rapidly becomes prohibitively costly as we expand to encompass millions or even billions of data points. This is due to the fact that the number of necessary comparisons increases linearly with the volume of data, making it inefficient at scale. That’s why approximate nearest neighbors (ANN) comes in as a rescue. It optimizes for retrieval speed and returns the approximate (instead of exact) topmost similar neighbors, trading off a little accuracy loss for a large speed up."
  },
  {
    "objectID": "posts/RAG/What you should know about RAG (Retrieved-Augmente 5ffd27cf98504d6ca45823cc8e3b118d.html#indexing-technique",
    "href": "posts/RAG/What you should know about RAG (Retrieved-Augmente 5ffd27cf98504d6ca45823cc8e3b118d.html#indexing-technique",
    "title": "What you should know about RAG (Retrieved-Augmented Generation)",
    "section": "Indexing Technique",
    "text": "Indexing Technique\nThe similarity between two vectors is measured using distance metrics such as cosine distance or dot product. When dealing with vector databases, it is crucial to differentiate between the search algorithm and the underlying index on which the Approximate Nearest Neighbor (ANN) search algorithm is executed.\nWhat is the index?\nData within a vector database is organized through indexing, a process that involves constructing data structures known as indexes. These indexes facilitate efficient vector retrieval by swiftly reducing the search scope.\nThis great blog about indexes from thedataquarry gives a best overview on this subject. In our blog, we will talk about the most popular ones.\n\nIVF-PQ (Inverted file index - product quantization)\nIn a simple terms, the Inverted file index (IVF) part of the index helps us find the right area to search in, like narrowing down which part of a library to look in for a specific book. The product quantization (PQ) part helps us quickly compare the query (what we’re looking for) with the actual books (database vectors), making the search faster. It’s like having a shortcut to check if a book matches what we want.\nWhen we use both IVF and PQ together, we get a big boost in speed because PQ helps with speed, and IVF helps us find more of the right books, improving the chances of finding what we’re looking for.\n\n\nHierarchical Navigable Small Words (HNSW)\nThe Hierarchical Navigable Small-World (HNSW) graph algorithm is widely regarded as one of the most popular methods for constructing vector indexes. In fact, many database vendors currently favor it as their primary choice. The idea comes from this paper Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs.\nLet break down the key concepts in simpler terms:\n\nSmall-World:\nIn a small-world network, most nodes can be reached from every other node by a small number of steps or connections. It’s a concept often seen in social networks where people are surprisingly well-connected even though the world is large.\nNavigable:\nNavigability refers to the ease with which you can travel or find your way through something. In the context of HNSW, it means the structure allows for efficient navigation when searching for similar items.\nHierarchical:\nHierarchy involves organizing elements into levels or layers, where each level represents a different level of detail or abstraction. In HNSW, hierarchical structures are used to organize the data in a way that makes searching for similar items more efficient.\nHNSW in Similarity Search:\n\nIn similarity search, the goal is to find items in a dataset that are similar to a given query item. HNSW achieves this by creating a hierarchical graph where each node represents a data point, and connections between nodes represent similarity.\nThe hierarchy allows for faster search because it narrows down the potential similar items, starting from a coarse level and progressively refining the search. This hierarchical organization helps in quickly discarding non-relevant items.\nThe “navigable” aspect ensures that, even within these hierarchical levels, the structure facilitates efficient movement towards the most similar items.\n\n\nIn simple terms, think of HNSW as a smart way of organizing information in a large dataset so that when you want to find something similar to a given item, you can do it quickly by following a well-organized path through the data, thanks to its small-world and hierarchical properties. It’s like having a well-structured map that helps you navigate efficiently to find what you’re looking for."
  },
  {
    "objectID": "posts/RAG/What you should know about RAG (Retrieved-Augmente 5ffd27cf98504d6ca45823cc8e3b118d.html#retrieval-approaches",
    "href": "posts/RAG/What you should know about RAG (Retrieved-Augmente 5ffd27cf98504d6ca45823cc8e3b118d.html#retrieval-approaches",
    "title": "What you should know about RAG (Retrieved-Augmented Generation)",
    "section": "Retrieval Approaches",
    "text": "Retrieval Approaches\nVarious approaches to the retrieval mechanism have been proposed to get requisite context as well as overcome the challenges involved. They range from simple approaches to sophisticated multi step processes, some of them involving the LLM itself in pre-processing steps.\nSome of the approaches are:\nSimple\n\nFetch relevant documents (top k) based on the query or input.\nConsider all retrieved documents as part of the context for generating the response.\nAll documents are treated equally and incorporated into the generation process.\n\nMap Reduce:\n\nFetch relevant documents based on the query or input.\nRetrieve responses from the model for each document.\nCombine the individual responses generated for each document to produce the final response via LLM.\n\nMap Refine:\n\nFetch relevant documents based on the query or input.\nRetrieve the response from the first document.\nUse this initial response and subsequent documents to refine the response iteratively.\nRepeat this refinement process until a final answer or response is obtained.\n\nMap Rerank:\n\nFetch relevant documents based on the query or input.\nRetrieve responses for all documents individually\nApply a ranking model or algorithm to rank the responses.\nSelect the best-ranked response as the final answer or response.\n\nFiltering:\n\nQuery for relevant documents based on the query or input.\nApply a model to further filter the document set.\nUse the filtered documents as the context for generating the response.\n\nContextual Compression:\n\nQuery for relevant documents based on the query or input.\nApply a model to filter the documents based on relevancy and extract only relevant snippets from each document.\nUtilize these relevant snippets to generate a concise and informative response.\n\nSummary Based Index:\n\nRelevant for a document repository with document summaries.\nLarge documents can be broken down into smaller manageable snippets..\nA summary of the entire document too is created.\nSnippets are linked to the summary.\nIndex document snippets along with document summaries.\nQuery for relevant document summaries.\nCollect the snippets from the relevant summaries to form the document set.\nUse the document set to generate a response.\n\nForward-Looking Active REtrieval augmented generation (FLARE):\n\nLook up an initial set of relevant documents.\nUse this initial set and query to predict the upcoming sentence.\nCheck for the confidence level of the predicted tokens.\nIf the confidence level is high then continue predicting the next sentence.\nElse use the predicted sentence as a query to look up more relevant documents.\nIteratively continue the process to generate the full response."
  },
  {
    "objectID": "posts/RAG/What you should know about RAG (Retrieved-Augmente 5ffd27cf98504d6ca45823cc8e3b118d.html#component-wise-evaluation",
    "href": "posts/RAG/What you should know about RAG (Retrieved-Augmente 5ffd27cf98504d6ca45823cc8e3b118d.html#component-wise-evaluation",
    "title": "What you should know about RAG (Retrieved-Augmented Generation)",
    "section": "Component-wise Evaluation",
    "text": "Component-wise Evaluation\n\nEmbedding\nThe first component that you want to evaluate is the embedding model. It helps to look at how well the model is performing on a diverse set of domains or tasks. A useful benchmark for embeddings is the MTEB Leaderboard.\n\n\nRetrieval\nOne of the most important question is how we can measure the score of similarity between our retrieval predictions and our ground truth answer.\nClassical metrics\nThe commonly used metrics such as BLEU, ROUGE, BERTScore, and MoverScore.\nBLEU (Bilingual Evaluation Understudy) is a precision-based metric: It counts the number of n-grams in the generated output that also show up in the reference, and then divides it by the total number of words in the output. It’s predominantly used in machine translation and remains a popular metric due to its cost-effectiveness.\n\\[\n\\text{precision}_n = \\frac{\\sum_{p \\in \\text{output}} \\sum_{\\text{n-gram} \\in p} \\text{Count}_{\\text{clip}} (\\text{n-gram})}{\\sum_{p \\in \\text{output}} \\sum_{\\text{n-gram} \\in p} \\text{Count}(\\text{n-gram})}\n\\]\nROUGE (Recall-Oriented Understudy for Gisting Evaluation): In contrast to BLEU, ROUGE is recall-oriented. It counts the number of words in the reference that also occur in the output. It’s typically used to assess automatic summarization tasks.\nThere are several ROUGE variants. ROUGE-N is most similar to BLEU in that it also counts the number of matching n-grams between the output and the reference.\n\\[\n\\text{ROUGE-N} = \\frac{\\sum_{s_r \\in \\text{references}} \\sum_{n\\text{-gram} \\in s_r} \\text{Count}_{\\text{match}} (n\\text{-gram})}{\\sum_{s_r \\in \\text{references}} \\sum_{n\\text{-gram} \\in s_r} \\text{Count} (n\\text{-gram})}\n\\]\nOther variants include:\n\nROUGE-L: This measures the longest common subsequence (LCS) between the output and the reference. It considers sentence-level structure similarity and zeros in on the longest series of co-occurring in-sequence n-grams.\nROUGE-S: This measures the skip-bigram between the output and reference. Skip-bigrams are pairs of words that maintain their sentence order regardless of the words that might be sandwiched between them.\n\nBERTScore is an embedding-based metric that uses cosine similarity to compare each token or n-gram in the generated output with the reference sentence. There are three components to BERTScore:\n\nRecall: Average cosine similarity between each token in the reference and its closest match in the generated output.\nPrecision: Average cosine similarity between each token in the generated output and its nearest match in the reference.\nF1: Harmonic mean of recall and precision\n\n\\[\nRecall_{\\text{BERT}} = \\frac{1}{|r|} \\sum_{i \\in r} \\max_{j \\in p} \\vec{i}^T \\vec{j}, \\quad Precision_{\\text{BERT}} = \\frac{1}{|p|} \\sum_{j \\in p} \\max_{i \\in r} \\vec{i}^T \\vec{j}\n\\]\n\\[\n\\text{BERTscore} = F_{\\text{BERT}} = \\frac{2 \\cdot P_{\\text{BERT}} \\cdot R_{\\text{BERT}}}{P_{\\text{BERT}} + R_{\\text{BERT}}}\n\\]\nBERTScore is useful because it can account for synonyms and paraphrasing. Simpler metrics like BLEU and ROUGE can’t do this due to their reliance on exact matches. BERTScore has been shown to have better correlation for tasks such as image captioning and machine translation.\nMoverScore also uses contextualized embeddings to compute the distance between tokens in the generated output and reference. But unlike BERTScore, which is based on one-to-one matching (or “hard alignment”) of tokens, MoverScore allows for many-to-one matching (or “soft alignment”).\n\n\n\nUntitled\n\n\nUse a strong LLM as a reference-free metric\nIdeally, we would have humans interpret if the output quality is good. Human raters, however, are extremely resource-intensive and not practical at scale. GPT-4 has been used as a fairly good proxy to human-raters. You may want to consider prompt engineering techniques such as few shot prompting, chain-of-thought, and self-consistency to generate more reliable evaluation results from GPT-4."
  },
  {
    "objectID": "posts/RAG/What you should know about RAG (Retrieved-Augmente 5ffd27cf98504d6ca45823cc8e3b118d.html#end-to-end-evaluation",
    "href": "posts/RAG/What you should know about RAG (Retrieved-Augmente 5ffd27cf98504d6ca45823cc8e3b118d.html#end-to-end-evaluation",
    "title": "What you should know about RAG (Retrieved-Augmented Generation)",
    "section": "End-to-end evaluation",
    "text": "End-to-end evaluation\nWhen coming to end-to-end evaluation, you can consider Ragas. Ragas is an open-source evaluation framework for RAG pipeline\nRagas measures your pipeline’s performance against different dimensions\n\nFaithfulness: measures the information consistency of the generated answer against the given context. If any claims are made in the answer that cannot be deduced from context is penalized. It is calculated from answer and retrieved context.\nContext Precision: measures how relevant retrieved contexts are to the question. Ideally, the context should only contain information necessary to answer the question. The presence of redundant information in the context is penalized. It is calculated from question and retrieved context.\nContext Recall: measures the recall of the retrieved context using annotated answer as ground truth. Annotated answer is taken as proxy for ground truth context. It is calculated from ground truth and retrieved context.\nAnswer Relevancy: refers to the degree to which a response directly addresses and is appropriate for a given question or context. This does not take the factuality of the answer into consideration but rather penalizes the present of redundant information or incomplete answers given a question. It is calculated from question and answer.\nAspect Critiques: Designed to judge the submission against defined aspects like harmlessness, correctness, etc. You can also define your own aspect and validate the submission against your desired aspect. The output of aspect critiques is always binary. It is calculated from answer."
  },
  {
    "objectID": "posts/RAG/What you should know about RAG (Retrieved-Augmente 5ffd27cf98504d6ca45823cc8e3b118d.html#data-source-and-parameters-tuning",
    "href": "posts/RAG/What you should know about RAG (Retrieved-Augmente 5ffd27cf98504d6ca45823cc8e3b118d.html#data-source-and-parameters-tuning",
    "title": "What you should know about RAG (Retrieved-Augmented Generation)",
    "section": "Data source and parameters tuning",
    "text": "Data source and parameters tuning\n\nPay attention to base documents\nThe most important part of RAG is data source which creates a base knowledge for the LLM to generate the answer. The quality of the base documents will directly affect the quality of the answer. Are documents broken out logically? Are topics covered in one place or many separate places? If you, as a human, can’t easily tell which document you would need to look at to answer common queries, your retrieval system won’t be able to either. To improve the quality of answer, we should do some sorts preprocessing / cleaning / filtering to the base documents.\n\n\nTune different indexing / chunking approaches\nOne of the most common strategies for improving retrieval performance is to experiment with different indexing and chunking approaches. For example, you can try different chunk sizes, chunk overlap, and chunking strategies (e.g., sentence, paragraph, etc.). ### Adding meta-data for filtering\nA highly effective strategy for enhancing the retrieval process involves augmenting your data chunks with metadata, which can subsequently be leveraged to refine result processing. One commonly used metadata tag is the “date” because it facilitates filtering by recency.\n\n\nIterate over your prompt\nYou can hack the RAG such that you do allow the LLM to rely on its own knowledge if it can’t find a good answer in the context or adaptes different styles for the answer.\n\n\nFine-tune your embedding model.\nFinetuning a model means updating the model itself over a set of data to improve the model in a variety of ways. This can include improving the quality of outputs, reducing hallucinations, memorizing more data holistically, and reducing latency/cost. Finetuning the embedding model can allow for more meaningful embedding representations over a training distribution of data –&gt; leads to better retrieval performance."
  },
  {
    "objectID": "posts/RAG/What you should know about RAG (Retrieved-Augmente 5ffd27cf98504d6ca45823cc8e3b118d.html#retrieval-logic",
    "href": "posts/RAG/What you should know about RAG (Retrieved-Augmente 5ffd27cf98504d6ca45823cc8e3b118d.html#retrieval-logic",
    "title": "What you should know about RAG (Retrieved-Augmented Generation)",
    "section": "Retrieval Logic",
    "text": "Retrieval Logic\n\nDecoupling Chunks Used for Retrieval vs. Chunks Used for Synthesis\nThe most effective way to represent information for retrieval could differ from the most effective approach for synthesis. For example, a raw text chunk may provide essential details that help the language model generate a comprehensive response to a query. However, it might also contain extraneous filler words or information that could skew the embedding representation. Additionally, it may lack sufficient global context and might not be retrieved when a relevant query is received. In that sense, we can embed a document summary, which links to chunks associated with the document or embed a sentence which links to a window around the sentence.\n\n\nStructured Retrieval for Larger Document Sets\nA significant drawback of the standard RAG stack, which involves top-k retrieval and basic text splitting, becomes evident when dealing with a large number of documents, such as 100 different PDFs. In this scenario, when you have a query, it’s often essential to utilize structured information to enhance precision in retrieval. For instance, if your question pertains to only two specific PDFs, relying solely on raw embedding similarity with text chunks may not be sufficient.\nOne effective approach to address this issue is to embed document summaries and establish a mapping to text chunks within each document. This enables retrieval at the document level initially, prioritizing the identification of relevant documents before delving into chunk-level analysis.\n\n\nUsing Reranker model\nSemantic search may yield top-k results that are too similar to each other. To ensure a wider array of snippets, it is beneficial to re-rank the results based on other factors such as metadata and keyword matches. This diversification of snippets can lead to a more nuanced and comprehensive context for the LLM to generate responses. Re-ranker can be based on a cross-encoder. Some of the popular reranker models are: - CohereAI - bge-reranker-base / bge-reranker-large"
  },
  {
    "objectID": "posts/RAG/What you should know about RAG (Retrieved-Augmente 5ffd27cf98504d6ca45823cc8e3b118d.html#caching",
    "href": "posts/RAG/What you should know about RAG (Retrieved-Augmente 5ffd27cf98504d6ca45823cc8e3b118d.html#caching",
    "title": "What you should know about RAG (Retrieved-Augmented Generation)",
    "section": "Caching",
    "text": "Caching\n\nSemantic cache results\nTraditional caching systems use various techniques to store data or queries so that when another user asks the same or similar query, you don’t have to make a full round trip to generate the same context. However, traditional caching systems use an exact keyword match, which doesn’t work with LLMs where the queries are in natural language. So, how do you ensure you’re not performing a full retrieval each time when the queries are similar?\nThis is where CacheGPT comes in. CacheGPT uses semantic search to match queries against previously asked queries, and if there’s a match, you simply return the last context instead of performing a full retrieval. CacheGPT is an open-source library, and you can refer to its documentation to configure it to your requirements."
  },
  {
    "objectID": "posts/finetuning-llm/finetuning_inference_zypher7b.html",
    "href": "posts/finetuning-llm/finetuning_inference_zypher7b.html",
    "title": "Finetuning and inference Zypher7B",
    "section": "",
    "text": "Zephyr-7B comprises two models created by the Hugging Face 4 team, derived from the well-known Mistral-7B model: Zephyr-7B-α and Zephyr-7B-β. These models not only outperform the Mistral-7B models but also exhibit performance comparable to LLaMA2-Chat-70B, which are models ten times their size.\n\n\n\nPerformance of Zephyr-7b compared to other models"
  },
  {
    "objectID": "posts/finetuning-llm/finetuning_inference_zypher7b.html#what-is-zypher7b",
    "href": "posts/finetuning-llm/finetuning_inference_zypher7b.html#what-is-zypher7b",
    "title": "Finetuning and inference Zypher7B",
    "section": "",
    "text": "Zephyr-7B comprises two models created by the Hugging Face 4 team, derived from the well-known Mistral-7B model: Zephyr-7B-α and Zephyr-7B-β. These models not only outperform the Mistral-7B models but also exhibit performance comparable to LLaMA2-Chat-70B, which are models ten times their size.\n\n\n\nPerformance of Zephyr-7b compared to other models"
  },
  {
    "objectID": "posts/finetuning-llm/finetuning_inference_zypher7b.html#how-it-works",
    "href": "posts/finetuning-llm/finetuning_inference_zypher7b.html#how-it-works",
    "title": "Finetuning and inference Zypher7B",
    "section": "How it works?",
    "text": "How it works?\nMore details can be found in the Zephyr: Direct Distillation of LM Alignment. \n\nDistilled supervised fine-tuning (dSFT)\nSFT, serving as the initial training phase for instructive/chat models, necessitates an instruction dataset, comprising pairs of instructions or questions alongside responses provided by humans. However, the primary challenge lies in the high cost associated with collecting such a dataset, given the requirement for human labor. An increasingly prevalent and cost-effective alternative is to utilize instruction datasets generated by other Large Language Models (LLMs).\nWe can find many such instruction datasets on the Hugging Face Hub that we can use for SFT, for instance:\n\nOpenAssistant Conversations Dataset (OASST1) (84.4k training examples)\nOpenOrca (4.2M training examples)\nopenassistant-guanaco (9.8k training examples)\n\nFor Zephyr 7B Beta, Hugging Face fine-tuned Mistral 7B on a custom version of Ultrachat that they aggressively filtered: - HuggingFaceH4/ultrachat_200k (MIT license), use the “sft” splits\n\n\nAI feedback (AIF)\nFor alignment with humans, we need a dataset of prompts paired with ranked answers. It’s common to use human feedback to align LLMs. Zephyr, however, uses AI feedback (AIF) since ranking models’ answers is an expensive task requiring human labor.\nStarting with a collection of 4 different models like Claude, Llama, Falcon, etc, each prompt is fed through all 4 models to produce text. The teacher model, GPT-4, then gives a score for each produced text. The highest score of the 4 responses is called \\(y_w\\) and a random lower-scoring response is called \\(y_l\\) Thus, from a list of prompts \\(\\{x_1, ..., x_j\\}\\), we derive a dataset D = \\(\\{(x_1, y_1^w, y_1^l), ..., (x_j, y_j^w, y_j^l)\\}\\). These are 3-tuples of prompts with a stronger and a weaker response.\nFor this step, Hugging Face directly used the dataset UltraFeedback.\nUltraFeedback contains 74k prompts paired with responses generated by the following models:\n\nLLaMA-2–7B-chat, LLaMA-2–13B-chat, LLaMA-2–70B-chat\nUltraLM-13B, UltraLM-65B\nWizardLM-7B, WizardLM-13B, WizardLM-70B\nVicuna-33B\nAlpaca-7B\nFalcon-40B-instruct\nMPT-30B-chat\nStarChat-Beta\nPythia-12B\n\nEach LLM’s output is rated by GPT-4 with a score from 1 to 5 (higher is better) for various criteria:\n\ninstruction following\nhelpfulness\nhonesty\ntruthfulness\n\n\n\nDistilled direct preference optimization (dDPO)\nInstruct Large Language Models (LLMs), such as chat models, are commonly trained using Reinforcement Learning with Human Feedback (RLHF), employing a technique called Proximal Policy Optimization (PPO). While RLHF effectively aligns LLMs with human preferences, it comes with challenges of instability and complexity. To address these issues, a two-step training process is employed before running RLHF:\n\nReference Model Training: A reference model is initially trained using Supervised Fine-Tuning (SFT) on an instruction dataset.\nReward Model Training: A reward model is trained to predict human preferences. This involves using training data where humans rank the outputs of models for a given prompt. The reward model is then trained to predict these rankings.\n\nAfter these preliminary steps, RLHF involves the use of four different models:\n\nReference Model (SFT): The model trained using SFT on the instruction dataset.\nReward Model: The model trained to predict human preferences based on ranked outputs.\nValue Model: Typically initialized by the reward model, the value model is an additional component in RLHF.\nPolicy Model: The model (policy) that undergoes training with RLHF. It is typically initialized by the reference model. Using all these models, RLHF uses RL to optimize a language model policy to produce responses with a high reward (according to the reward model) without drifting excessively far from the original reference model. This multi-step approach, involving reference model training, reward model training, and the use of multiple models in RLHF, is designed to enhance the stability and effectiveness of instruct LLMs, aligning them more closely with human preferences.\n\nDPO is a simple alternative to RLHF. It implicitly optimizes the same objective as existing RLHF algorithms (reward maximization with a KL-divergence constraint). The authors of DPO demonstrate that the constrained reward maximization problem can be exactly optimized by solving a much simpler classification problem on human preferences. DPO is lightweight and is more stable according to the authors. The Zephyr authors call this dDPO because the dataset is distilled from earlier steps, leveraging an AI to provide preference labels.\nSince it can be reduced to a classification problem, DPO trains the model using a simple binary cross-entropy objective. DPO completely eliminates the need for reinforcement learning.\nGiven a prompt and several LLMs’ outputs ranked by humans according to their quality, DPO trains the model to assign a higher reward to the best outputs.\nDPO only requires two models: - The reference model fine-tuned with SFT on instruct datasets - The base model that we want to train with DPO\n\n\n\nDPO illustration by Rafailov et al"
  },
  {
    "objectID": "posts/finetuning-llm/finetuning_inference_zypher7b.html#finetuning-zypher-7b",
    "href": "posts/finetuning-llm/finetuning_inference_zypher7b.html#finetuning-zypher-7b",
    "title": "Finetuning and inference Zypher7B",
    "section": "Finetuning Zypher-7B",
    "text": "Finetuning Zypher-7B\n\nimport os\nimport datetime\n\nfrom copy import deepcopy\nfrom random import randrange\nfrom functools import partial\n\nimport torch\nimport accelerate\nimport bitsandbytes as bnb\n\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling\n)\nfrom peft import (\n    LoraConfig,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n    PeftModel\n)\n\nfrom wandb.sdk.data_types.trace_tree import Trace\n\n\ntransformers: is HuggingFace’s most popular library and their hub for models and training, evaluation, preprocessing, and other pipeline components.\ndatasets gives us the power to load in any dataset from the dataset hub.\npeft is HuggingFace’s parameter-efficient fine-tuning library, especially useful for LLMs and limited hardware.\ntrl is HuggingFace’s RL training library for language models.\naccelerate is for distributed configuration and accelerating your PyTorch script.\nbitsandbytes is an HuggingFace-integrated library for quantization functions to help with reducing our memory footprint.\n\nSince we will download the quite big model directly from the Hugging Face Hub, we should configure the environment of TRANSFORMERS_CACHE to a folder with enough space.\n\nimport os \nos.environ['TRANSFORMERS_CACHE'] = \"./cache\"\nos.environ['WANDB_API_KEY'] = '6d21064574dc241c28f085109bb8b4351c2b7f8c'\n\nWe’ll be using W&B to log our experiments. You can create a free account at https://wandb.ai.\n\nimport wandb\nwandb.login()\n\nFailed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\nwandb: Currently logged in as: uthnaht. Use `wandb login --relogin` to force relogin\n\n\nTrue\n\n\nLet’s first define the model, then load and preprocess the dataset. We will use a sharded Zephyr 7B to save memory.\n\nmodel_name = \"anakin87/zephyr-7b-alpha-sharded\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ntokenizer_config.json: 100%|██████████| 1.43k/1.43k [00:00&lt;00:00, 2.13MB/s]\ntokenizer.json: 100%|██████████| 1.80M/1.80M [00:00&lt;00:00, 4.28MB/s]\nspecial_tokens_map.json: 100%|██████████| 168/168 [00:00&lt;00:00, 739kB/s]\n\n\n\nBits and Bytes Config & Loading the Model\nThis step is to define our BitsAndBytesConfig. This will significantly reduce memory consumption when we load in our sharded Zypher 7B model. The configure will be as bellow:\n\nload in 4 bits: we can divide the used memory by 4 and import the model on smaller devices.\ndouble quantize (quantize the weights and quantize the first quantization’s constants)\nuse NF4 (normalized fp4)\ncompute type is bfloat16 (computations run in bfloat16)\n\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\nFinally we load our model. We disable the cache to avoid the conflict with gradient checkpoint that we will enable right after.\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    no_cuda=True,\n    use_mps_device=True,  # Auto selects device to put model on.\n)\n\nmodel.config.use_cache = False\n#model.gradient_checkpointing_enable()\n\nImportError: Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` \n\n\nNext, we will do somethings special to enable us train the 7B model in a single GPU. - freezes the model weights - cast all non INT8 parameters (layer norm and lm head) to fp32 if the model is not gptq quantized - enable_input_require_grads: Enables the gradients for the input embeddings. This is useful for fine-tuning adapter weights while keeping the model weights fixed. - gradient_checkpointing_enable\n\nThe gradient checkpoint is a technique to reduce the memory footprint of the model. It will save the activations of the model and recomputes them during the backward pass. This is a trade-off between memory and compute. We will use the gradient checkpoint to reduce the memory footprint of our model.\n\n\nmodel = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n\n\n\nUsing LoRA\nLora is a technique that accelerates the fine-tuning of large models while consuming less memory.\nIn order to enhance the efficiency of fine-tuning, LoRA employs a strategy involving the representation of weight updates using two smaller matrices referred to as “update matrices” via low-rank decomposition. These newly introduced matrices can be trained to accommodate new data while minimizing the overall magnitude of modifications. The original weight matrix remains unchanged and undergoes no further adjustments. The ultimate results are derived by combining both the original and the adapted weights.\nFirst we need to define the LoRa config. LoraConfig allows you to control how LoRA is applied to the base model through the following parameters:\n\nr : the rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters.\ntarget_modules: The modules (for example, attention blocks) to apply the LoRA update matrices.\nlora_alpha : LoRA scaling factor.\n`lora_dropout``: The dropout probability for Lora layers.\nbias: Specifies if the bias parameters should be trained. Can be ‘none’, ‘all’ or ‘lora_only’. If ‘all’ or ‘lora_only’, the corresponding biases will be updated during training. Be aware that this means that, even when disabling the adapters, the model will not produce the same output as the base model would have without adaptation.\n\n\ndef create_peft_config(modules):\n    lora_alpha = 16\n    lora_dropout = 0.1\n    lora_r = 8\n\n    peft_config = LoraConfig(\n        lora_alpha=lora_alpha, # parameter for scaling\n        lora_dropout=lora_dropout, # dropout probability for layers\n        target_modules=modules,\n        r=lora_r, # dimension of the updated matrices\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\"\n    )\n    return peft_config\n\nPrevious function needs the target modules to update the necessary matrices. The following function will return a list of layer names for LoRA to be applied to. These include the q, k, o, v proj layers and the gated, up, and down layers in the MLPs.\n\ndef find_all_linear_names(model):\n    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\n    # lm_head is often excluded.\n    if 'lm_head' in lora_module_names:  # needed for 16-bit\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\nmodules = find_all_linear_names(model)\n\nFinally, we can create our LoRA-applied model which is wrapped as PeftModel\n\nmodel = get_peft_model(model, peft_config)\n\nThen we can know how many parameters are trainable in our model and measure the memory footprint of our model.\n\ntrainable, total = model.get_nb_trainable_parameters()\nprint(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")\n\n\n\nTraining Dataset\nWe will be fine-tuning Mistral 7B on the Puffin dataset, 3000 multi-turn conversations between a user and GPT-4.\n\ndataset = load_dataset(\"LDJnr/Puffin\", split=\"train\")\n\n\nTake a peak at the dataset\n\nfor i in dataset:\n  if len(i[\"conversations\"]) &gt; 2:\n    for j in i[\"conversations\"]:\n      print(j)  # Conversations are multi-turn (&gt;= 2) and always even in count (human then gpt response).\n    break"
  },
  {
    "objectID": "posts/time-series/time-series.html",
    "href": "posts/time-series/time-series.html",
    "title": "Everything you need to know about Time Series Forecasting",
    "section": "",
    "text": "Time series exist in a variety of fields from meteorology to finance, econometrics, and marketing. By recording data and analyzing it, we can study time series to analyze industrial processes or track business metrics, such as sales or engagement. Also, with large amounts of data available, data scientists can apply their expertise to techniques for time series forecasting.\nWe will first learn how to make simple forecasts that will serve as benchmarks for more complex models. Then we will use two statistical learning techniques, the moving average model and the autoregressive model, to make forecasts. These will serve as the foundation for the more complex modeling techniques we will cover that will allow us to account for non-stationarity, seasonality effects, and the impact of exogenous variables. Afterwards, we’ll switch from statistical learning techniques to deep learning methods, in order to forecast very large time series with a high dimensionality, a sce- nario in which statistical learning often does not perform as well as its deep learning counterpart."
  },
  {
    "objectID": "posts/time-series/time-series.html#time-series-definition",
    "href": "posts/time-series/time-series.html#time-series-definition",
    "title": "Everything you need to know about Time Series Forecasting",
    "section": "Time Series Definition",
    "text": "Time Series Definition\nA time series is a set of data points ordered in time. The data is equally spaced in time, meaning that it was recorded at every hour, min- ute, month, or quarter. Typical examples of time series include the closing value of a stock, a household’s electricity consumption, or the temperature outside.\n\nWhat makes time series forecasting different from other regression tasks?\n\nTemporal ordering: Time series have an order and the order of the observations matters. You can’t just treat the data as independent and identically distributed (i.i.d.) because the temporal ordering of the data matters.\nTime series sometimes do not have features: With time series, it is quite common to be given a simple dataset with a time column and a value at that point in time. Without any other features, we must learn ways of using past values of the time series to forecast future values.\nSpeciality of time series forecasting:\n\nSeasonality: Many time series exhibit seasonality. For example, the number of airline passengers increases during the summer and decreases during the winter.\nTrends: Many time series exhibit trends. For example, the number of airline passengers has generally increased over time.\nNon-stationarity: Many time series are non-stationary. This means that the statistical properties of the time series, such as the mean or variance, change over time. For example, the number of airline passengers has generally increased over time, so the mean number of passengers is not constant."
  },
  {
    "objectID": "posts/time-series/time-series.html#the-random-walk-process",
    "href": "posts/time-series/time-series.html#the-random-walk-process",
    "title": "Everything you need to know about Time Series Forecasting",
    "section": "The random walk process",
    "text": "The random walk process\nA random walk is a process in which there is an equal chance of going up or down by a random number. Random walks often expose long periods where a positive or negative trend can be observed. They are also often accompanied by sudden changes in direction.\nThis is usually observed in financial and economic data, like the daily closing price of GOOGL.\nIn a random walk process, we say that the present value \\(y_t\\) is a function of the value at the previous timestep \\(y_{t –1}\\), a constant C, and a random number \\(ε_t\\) , also termed white noise. Here, \\(ε_t\\) is the realization of the standard normal distribution, which has a variance of 1 and a mean of 0. Therefore, we can write the random walk process as:\n\\[y_t = C + y_{t-1} + ε_t\\]\nHow to indentify a random walk process?\nA random walk is a series whose first difference is stationary and uncorrelated. This means that the process moves completely at random.\n\nStationarity:\nA stationary process is one whose statistical properties do not change over time. It means that it is said to be stationary if its mean, variance, and autocorrelation do not change over time.\n\n\nHow to test for stionarity\n💡 Augmented Dickey-Fuller (ADF) test\nThe augmented Dickey-Fuller (ADF) test helps us determine if a time series is stationary by testing for the presence of a unit root. If a unit root is present, the time series is not stationary.\nThe null hypothesis states that a unit root is present, meaning that our time series is not stationary.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf\n\nsteps = np.random.standard_normal(1000)\nsteps[0]=0\nrandom_walk = np.cumsum(steps)\n\nfig, ax = plt.subplots()\nax.plot(random_walk)\nax.set_xlabel('Timesteps')\nax.set_ylabel('Value')\nplt.tight_layout()\n\nad_fuller_result = adfuller(random_walk) # test for stationary\nprint(f'ADF Statistic: {ad_fuller_result[0]}')\nprint(f'p-value: {ad_fuller_result[1]}')\n\nADF Statistic: -1.0137668210808588\np-value: 0.7482101114173918\n\n\n\n\n\nThe ADF statistic is not a large negative number, and with a p-value greater than 0.05, we cannot reject the null hypothesis stating that our time series is not stationary. We can further support our conclusion by plotting the ACF function.\n\nplot_acf(random_walk, lags=20);\n\n\n\n\nThe autocorrelation coefficients slowly decrease as the lag increases, which is a clear indicator that our random walk is not a stationary process. The shaded area represents a confidence interval. If a point is within the shaded area, then it is not significantly different from 0. Otherwise, the autocor- relation coefficient is significant.\nBecause our random walk is not stationary, we need to apply a transformation to make it stationary in order to retrieve useful information from the ACF plot. Since our sequence mostly displays changes in the trend without seasonal patterns, we will apply a first-order differencing to our time series.\n\ndiff_random_walk = np.diff(random_walk, n=1)\n\n\nADF_result = adfuller(diff_random_walk)\nprint(f'ADF Statistic: {ADF_result[0]}')\nprint(f'p-value: {ADF_result[1]}')\n\nADF Statistic: -15.250889678624036\np-value: 5.01119615882467e-28\n\n\nThis time the ADF statistic is a large negative number, and the p-value is less than 0.05. Therefore, we reject the null hypothesis, and we can say that this process has no unit root and is thus stationary.\n\nplot_acf(diff_random_walk, lags=20);\n\n\n\n\nYou’ll notice that there are no significant autocorrelation coefficients after lag 0. This means that the stationary process is completely random and can therefore be described as white noise. Each value is simply a random step away from the previous one, with no relation between them.\n\n\nTime series decomposition\nTime series decomposition is a statistical task that separates the time series into its three main components: a trend component, a seasonal component, and the residuals.\n\nThe trend component represents the long-term change in the time series. This component is responsible for time series that increase or decrease over time.\nThe seasonal component is the periodic pattern in the time series. It represents repeated fluctuations that occur over a fixed period of time.\nFinally, the residuals, or the noise, express any irregularity that cannot be explained by the trend or the seasonal component.\n\nWe will take a look at real world dataset. The Airline Passengers dataset describes the total number of airline passengers over a period of time. The units are a count of the number of airline passengers in thousands. There are 144 monthly observations from 1949 to 1960.\n\nimport pandas as pd\nfrom statsmodels.tsa.seasonal import STL\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv', header=0, index_col=0)\n\ndecomposition = STL(df['Passengers'], period=12).fit()\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=4, ncols=1, sharex=True, figsize=(10,8))\n\nax1.plot(decomposition.observed)\nax1.set_ylabel('Observed')\n\nax2.plot(decomposition.trend)\nax2.set_ylabel('Trend')\n\nax3.plot(decomposition.seasonal)\nax3.set_ylabel('Seasonal')\n\nax4.plot(decomposition.resid)\nax4.set_ylabel('Residuals')\n\nplt.xticks(np.arange(0, 145, 12), np.arange(1949, 1962, 1))\n\nfig.autofmt_xdate()\nplt.tight_layout()"
  },
  {
    "objectID": "posts/time-series/time-series.html#statistical-models",
    "href": "posts/time-series/time-series.html#statistical-models",
    "title": "Everything you need to know about Time Series Forecasting",
    "section": "Statistical Models",
    "text": "Statistical Models\n\nMoving average\nA moving average process, or the moving average (MA) model, states that the current value is linearly dependent on the current and past error terms. The error terms are assumed to be mutually independent and normally distributed, just like white noise.\nA moving average model is denoted as \\(MA(q)\\), where \\(q\\) is the order. The model expresses the present value as a linear combination of the mean of the series \\(\\mu\\), the present error term \\(\\epsilon_t\\), and past error terms \\(\\epsilon_t - q\\). The magnitude of the impact of past errors on the present value is quantified using a coefficient denoted as \\(\\theta_q\\).\n\\[y_t = \\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + ... + \\theta_q \\epsilon_{t-q}\\]\n\n\n\nmoving average\n\n\n\n💡 Autocorrelation function\nThe autocorrelation function (ACF) measures the linear relationship between lagged values of a time series.\nIn other words, it measures the correlation of the time series with itself.\n\n\n\nAutoregressive Process\nAn autoregressive process establishes that the output variable depends linearly on its own previous values. In other words, it is a regression of the variable against itself.\nAn autoregressive process is denoted as an \\(AR(p)\\) process, where p is the order. In such a process, the present value \\(y_t\\) is a linear combination of a constant \\(C\\), the present error term \\(\\epsilon_t\\), which is also white noise, and the past values of the series \\(y_t–p\\). The magnitude of the influence of the past values on the present value is denoted as \\(\\phi_p\\), which represents the coefficients of the \\(AR(p)\\) model.\n\\[y_t = C + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ... + \\phi_p y_{t-p} + \\epsilon_t\\]\nSimilar to the moving average process, the order \\(p\\) of an autoregressive process determines the number of past values that affect the present value\n\n\n\nAutoregressive step-by-step schema.\n\n\n\n\nAutoregressive Moving Average (ARMA) Process\nThe autoregressive moving average process is a combination of the autoregressive process and the moving average process. It states that the present value is linearly dependent on its own previous values and a constant, just like in an autoregressive process, as well as on the mean of the series, the current error term, and past error terms, like in a moving average process.\nThe \\(\\text{ARMA(p, q)}\\) model consists of two types of lagged values, one for the autoregressive component and the other for the moving average component. The first parameter \\(p\\) indicate the order of the autoregression, and the second \\(q\\) the order of the moving average\n\\[y_t = C + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ... + \\phi_p y_{t-p} + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + ... + \\theta_q \\epsilon_{t-q}\\]\n\n\n\nARMA steps-by-steps schema\n\n\nUnderstanding the Akaike information criterion (AIC)\nThe AIC estimates the quality of a model relative to other models. Given that there will be some information lost when a model is fitted to the data, the AIC quantifies the relative amount of information lost by the model. The less information lost, the lower the AIC value and the better the model. The AIC is calculated as follows:\n\\[AIC = 2k - 2ln(\\hat{L})\\]\nwhere k is the number of parameters in the model and \\(\\hat{L}\\) is the maximum value of the likelihood function for the model.\nUnderstanding residual analysis\nResidual analysis is a statistical technique used to assess the quality of a model. It is used to determine whether the assumptions of the model are valid and to identify any patterns in the residuals. The residuals are the differences between the observed values and the predicted values.\nThe residuals should be normally distributed, with a mean of 0 and a constant variance. If the residuals are not normally distributed, then the model is not capturing all the information in the data. If the residuals have a mean different from 0, then the model is biased. If the variance of the residuals is not constant, then the model is not capturing all the information in the data.\n\nThe first step in residual analysis is to plot the quantile-quantile (Q-Q) plot of the residuals. The Q-Q plot is a graphical tool used to compare the distribution of the residuals to the normal distribution. If the residuals are normally distributed, the points in the Q-Q plot will fall on a straight line. If the residuals are not normally distributed, the points will deviate from the straight line.\nThe second step is to determinate if the residuals are uncorrelated. We will use the Ljung-Box test to determine if the residuals are uncorrelated. The null hypothesis of the Ljung-Box test is that the data is independently distributed, meaning that there is no autocorrelation. If the p-value of the Ljung-Box test is less than 0.05, then we reject the null hypothesis and conclude that the residuals are correlated.\n\n\n\nAutoregressive Integrated Moving Average (ARIMA) Process\nSo far, we have seen the autoregressive process, the moving average process, and the autoregressive moving average process. These models can only be applied to stationary time series, which required us to apply transformations, mainly differencing, and test for stationarity using the ADF test. The forecasts from each model returned differenced values, which we had to reverse to obtain the original values.\nThe autoregressive integrated moving average process, or ARIMA, is a generalization of the autoregressive moving average process that can be applied to non-stationary time series. Using this model, we can take into account non-stationary time series and avoid the steps of modeling on differenced data and having to inverse transform the forecasts.\nThe ARIMA model is denoted as ARIMA(p, d, q), where p is the order of the autoregressive component AR(p), d is the degree of differencing, and q is the order of the moving average component MA(q).\n\\[y'_t = C + \\phi_1 y'_{t-1} + \\phi_2 y'_{t-2} + ... + \\phi_p y'_{t-p} + \\epsilon_t + \\theta_1 \\epsilon'_{t-1} + \\theta_2 \\epsilon'_{t-2} + ... + \\theta_q \\epsilon'_{t-q}\\]\nwhere \\(y'_t\\) is the differenced time series, and \\(\\epsilon'_t\\) is the differenced error term.\nIn simple terms, the ARIMA model is simply an ARMA model that can be applied on non-stationary time series.\n\n\n\nARIMA steps-by-steps schema\n\n\n\n\nSeasonal Autoregressive Integrated Moving Average (SARIMA) Process\n\\(\\text{SARIMA(p,d,q)}(P,D,Q)_m\\) model expands on the ARIMA(p,d,q) model by adding seasonal parameters, where P is the order of the seasonal AR(P) process, D is the seasonal order of integration, Q is the order of the seasonal MA(Q) process, and m is the frequency, or the number of observations per seasonal cycle.\nNote that a \\(\\text{SARIMA(p,d,q)}(0,0,0)_m\\) model is equivalent to an ARIMA(p,d,q) model.\nThe parameter m stands for the frequency, the number of observations per cycle. For data that was recorded every year, quarter, month, or week, the length of a cycle is considered to be 1 year. If the data was recorded annually, m = 1 since there is only one observation per year. If the data was recorded quarterly, m = 4 since there are four quarters in a year, and therefore four observations per year. Daily data can have a weekly seasonality. In that case, the frequency is m = 7 because there would be seven observations in a full cycle of 1 week. It could also have a yearly seasonality, meaning that m = 365. Thus, you can see that daily and sub-daily data can have a different cycle length, and therefore a different frequency m.\n\n\n\nSARIMA steps-by-steps schema\n\n\n\n\nSARIMA with external variables (SARIMAX)\nEach model that we have explored and used to produce forecasts has considered only the time series itself. In other words, past values of the time series were used as predictors of future values. However, it is possible that external variables also have an impact on our time series and can therefore be good predictors of future values.\nThe SARIMAX model simply adds a linear combination of exogenous variables to the SARIMA model. This allows us to model the impact of external variables on the future value of a time series.\n\\[x_t = \\text{SARIMA(p, d, q) (P, D, Q)}_m + \\sum_{i=1}^n \\beta_i X_t^i\\]\n\nCaveat using SARIMAX\n\nRecall that the SARIMAX model uses the \\(\\text{SARIMA(p,d,q)}(P,D,Q)_m\\) **model and a linear combination of exogenous variables to predict one timestep into the future. But what if you wish to predict two timesteps into the future? While this is possible with a SARIMA model, the SARIMAX model requires us to forecast the exogenous variables too.\nit can be forecast using a version of the SARIMA model. Nevertheless, we know that our forecast always has some error associated with it. Therefore, having to forecast an exogenous variable to forecast our target variable can magnify the prediction error of our target, meaning that our predictions can quickly degrade as we predict more timesteps into the future.\n\n→ The only way to avoid that situation is to predict only one timestep into the future and wait to observe the exogenous variable before predicting the target for another timestep into the future.\n\n\n\nVector AutoRegression (VAR) model\nWith the SARIMAX model, the relationship is unidirectional: we assume that the exogenous variable has an impact on the target only.\nHowever, it is possible that two time series have a bidirectional relationship, meaning that time series t1 is a predictor of time series t2, and time series t2 is also a predictor for time series t1. In such a case, it would be useful to have a model that can take this bidirectional relationship into account and output predictions for both time series simultaneously.\n→ Vector AutoRegression (VAR) allows us to capture the relationship between multiple time series as they change over time. That, in turn, allows us to produce forecasts for many time series simultaneously, therefore performing multivariate forecasting.\nThe VAR(p) model can be seen as a generalization of the AR(p) model that allows for multiple time series. Just like in the AR(p) model, the order p of the VAR(p) model determines how many lagged values impact the present value of a series. In this model, however, we also include lagged values of other time series.\nFor two time series, the general equation for the VAR(p) model is a linear combination of a vector of constants, past values of both time series, and a vector of error terms:\n\\[\\begin{bmatrix} y_{1,t} \\\\ y_{2,t} \\end{bmatrix} = \\begin{bmatrix} C_1 \\\\ C_2 \\end{bmatrix} + \\begin{bmatrix} A^1_{11} & A^1_{12} \\\\ A^1_{21} & A^1_{22} \\end{bmatrix} \\begin{bmatrix} y_{1,t-1} \\\\ y_{2,t-1} \\end{bmatrix} + \\begin{bmatrix} A^2_{11} & A^2_{12} \\\\ A^2_{21} & A^2_{22} \\end{bmatrix} \\begin{bmatrix} y_{1,t-2} \\\\ y_{2,t-2} \\end{bmatrix} + ... + \\begin{bmatrix} \\epsilon_{1,t} \\\\ \\epsilon_{2,t} \\end{bmatrix}\\]\nwhere \\(y_{1,t}\\) and \\(y_{2,t}\\) are the two time series, \\(A_{ij}\\) are the coefficients of the autoregressive model, and \\(\\epsilon_{t}\\) is the error term.\nSome importants points to take into account when using VAR model: - the time series must be stationary to apply the VAR model. - each time series has an impact on another. - it is important to test if whether past values of a time series are statistically significant in forecasting another time series -&gt; Granger causality test.\n\n\n\nVAR\n\n\n\n\nProphet\nProphet is a forecasting tool developed by Facebook. It is designed for forecasting time series data that display patterns on different time scales, such as yearly, weekly, and daily. It is based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.\nUnder the hood, Prophet implements a general additive model where each time series \\(y(t)\\) is modeled as the linear combination of a trend \\(g(t)\\), a seasonal component \\(s(t)\\), holiday effects \\(h(t)\\), and an error term \\(\\epsilon_t\\), which is normally distributed.\n\\[y(t) = g(t) + s(t) + h(t) + \\epsilon_t\\]\n\nThe trend component models the non-periodic long-term changes in the time series.\nThe seasonal component models the periodic change, whether it is yearly, monthly, or weekly, or daily. The holiday effect occurs irregularly and potentially on more than one day. Finally, the error term represents any change in value that cannot be explained by the previous three components.\n\nSeasonal periods\nThe inclusion of multiple seasonal periods was motivated by the observation that human behavior produced multi-period seasonal time series. For example, the five day work week can produce a pattern that repeats every week, while school break can produce a pattern that repeats every year.\nThus, to take multiple seasonal periods into account, Prophet uses the Fourier series to model multiple periodic effects.\n\\[s(t) = \\sum_{n=1}^N \\Big( a_n cos(\\frac{2\\pi nt}{P}) + b_n sin(\\frac{2\\pi nt}{P}) \\Big)\\]\n\nN is simply the number of parameters we wish to use to estimate the seasonal component. The larger the value of N, the more complex the seasonal component will be.\nif we have a yearly seasonality, P = 365.25, as there are 365.25 days in a year. For a weekly seasonality, P = 7\n\nHolidays effect\nHolidays are irregular events that can have a clear impact on a time series.\n\nevents such as Black Friday in the United States can dramatically increase the attendance in stores or the sales on an ecommerce website\nValentine’s Day is probably a strong indicator of an increase in sales of chocolates and flowers\n\nProphet lets us define a list of holidays for a specific country. Holiday effects are then incorporated in the model, assuming that they are all independent. If a data point falls on a holiday date, a parameter \\(K_i\\) is calculated to represent the change in the time series at that point in time. The larger the change, the greater the holiday effect.\nIf you want to deep dive into Prophet, check the official repo"
  },
  {
    "objectID": "posts/time-series/time-series.html#deep-learning",
    "href": "posts/time-series/time-series.html#deep-learning",
    "title": "Everything you need to know about Time Series Forecasting",
    "section": "Deep learning",
    "text": "Deep learning\n\nWhen to use deep learning for time series forecasting?\n\nDeep learning shines when we have large complex datasets (supposed to have more than 10K data points). In those situations, deep learning can leverage all the available data to infer relationships between each feature and the target, usually resulting in good forecasts.\n\nWhen we have a large enough dataset, a statistical model with take a long time to fit and forecast. In such a case, deep learning can be a good alternative.\nWhen the dataset has a non-linear relationship between the features and the target, we should consider using deep learning.\nWhen the data has multiple seasonal periods (hourly, daily, weekly, monthly, yearly), a SARIMAX model cannot be used. In such a case, deep learning should be considered.\n\n\nMultilayer Perceptron (MLP)\nThe most basic deep learning model is the multilayer perceptron (MLP). It is a feedforward neural network that consists of an input layer, one or more hidden layers, and an output layer. The input layer takes the features of the dataset, and the output layer returns the forecast. The hidden layers are responsible for learning the relationships between the features and the target.\nIn case of time series forecasting, the input layer takes the past values of the time series, and the output layer returns the forecast for the next value of the time series. The hidden layers are responsible for learning the relationships between the past values of the time series and the future value.\ndef compile_and_fit(model, window, patience=3, max_epochs=50):\n    early_stopping = EarlyStopping(monitor='val_loss',\n                                   patience=patience,\n                                   mode='min')\n    \n    model.compile(loss=MeanSquaredError(),\n                  optimizer=Adam(),\n                  metrics=[MeanAbsoluteError()])\n    \n    history = model.fit(window.train,\n                       epochs=max_epochs,\n                       validation_data=window.val,\n                       callbacks=[early_stopping])\n    \n    return history\n\ndense = Sequential([\n    Dense(units=64, activation='relu'),\n    Dense(units=64, activation='relu'),\n    Dense(units=1)\n])\n\n# one step\nhistory = compile_and_fit(linear, single_step_window)\n\nval_performance['Linear'] = linear.evaluate(single_step_window.val)\nperformance['Linear'] = linear.evaluate(single_step_window.test, verbose=0)\n\n\n# multi step \nhistory = compile_and_fit(ms_dense, multi_window)\n\nms_val_performance['Dense'] = ms_dense.evaluate(multi_window.val)\nms_performance['Dense'] = ms_dense.evaluate(multi_window.test, verbose=0)\n\n\nRecurrent Neural Network (RNN)\nThe recurrent neural network (RNN) is a type of neural network that is well-suited for time series forecasting. It is designed to take into account the sequential nature of time series data. The RNN has a feedback loop that allows information to be passed from one step of the network to the next. This feedback loop allows the RNN to take into account the past values of the time series when making a forecast.\nThe RNN is composed of a cell that takes the input and the hidden state from the previous timestep and returns the output and the hidden state for the current timestep. The hidden state is then passed to the next timestep, and so on. The output of the RNN is the forecast for the next value of the time series. There are many types of RNN cells, such as the simple RNN cell, the long short-term memory (LSTM) cell, and the gated recurrent unit (GRU) cell.\nIf you want to know more about LSTM, check this awsesome blog Understanding LSTM Networks by Colah\nlstm_model = Sequential([\n    LSTM(32, return_sequences=False),\n    Dense(units=1)\n])\n\nhistory = compile_and_fit(lstm_model, single_step_window)\n\n\nConvolutional Neural Network (CNN)\nA convolutional neural network is a deep learning architecture that makes use of convolutional operation. The convolution operation allows the network to create a reduced set of features. Therefore, it is a way of regularizing the network, preventing overfitting, and effectively filtering the inputs.\nThe convolution is performed with a kernel, which is also trained during model fitting. The stride of the kernel determines the number of steps it shifts at each step of the convolution. In time series forecasting, only 1D convolution is used. To avoid reducing the feature space too quickly, we can use padding, which adds zeros before and after the input vector. This keeps the output dimension the same as the original feature vector, allowing us to stack more convolution layers, which in turn allows the network to process the features for a longer time.\n\nA 1D convolutional network takes as input a 3-dimensional tensor and also outputs a 3-dimensional tensor. The input tensor of our TCN implementation has the shape (batch_size, input_length, input_size) and the output tensor has the shape (batch_size, input_length, output_size). Since every layer in a TCN has the same input and output length, only the third dimension of the input and output tensors differs. In the univariate case, input_size and output_size will both be equal to one. In the more general multivariate case, input_size and output_size might differ since we might not want to forecast every component of the input sequence.\nOne single 1D convolutional layer receives an input tensor of shape (batch_size, input_length, nr_input_channels) and outputs a tensor of shape (batch_size, input_length, nr_output_channels)\n\nKERNEL_WIDTH = 3\nLABEL_WIDTH = 24\nINPUT_WIDTH = LABEL_WIDTH + KERNEL_WIDTH - 1\n\ncnn_model = Sequential([\n    Conv1D(filters=32,\n          kernel_size=(KERNEL_WIDTH,),\n          activation='relu'),\n    Dense(units=32, activation='relu'),\n    Dense(units=1)\n])\n\nhistory = compile_and_fit(cnn_model, conv_window)\n\n\nTemporal Convolutional Network (TCN)\nTCN consists of dilated, causal 1D convolutional layers with the same input and output lengths.\nAdvantages of TCN: - TCNs exhibit longer memory than recurrent architectures with the same capacity.\n- Performs better than LSTM/GRU on long time series (Seq. MNIST, Adding Problem, Copy Memory, Word-level PTB…).\n- Parallelism (convolutional layers), flexible receptive field size (how far the model can see), stable gradients (compared to backpropagation through time, vanishing gradients)…\nA disadvantage of this basic design is that in order to achieve a long effective history size, we need an extremely deep network or very large filters.\nDilated Casual convolutions\nThe dilation part refers to the fact that points are skipped in the arrangement of the convolutional filters, such that any given point goes only into one convolutional filter in each level of layering. This promotes model sparsity and reduces redundant or overlapping convolutions, allowing the model to look further back in time while keeping overall computations reasonably contained.\n\n\n\nDilated Casual Convolutions\n\n\nimport numpy as np\nimport pandas as pd\nfrom keras.layers import Conv1D, Input, Add, Activation, Dropout\nfrom keras.models import Sequential, Model\nfrom keras.layers.advanced_activations import LeakyReLU, ELU\nfrom keras import optimizers\nimport tensorflow as tf\n\ndef DC_CNN_Block(nb_filter, filter_length, dilation):\n    def f(input_):\n        residual =  input_\n        layer_out = Conv1D(\n            filters=nb_filter, kernel_size=filter_length,\n            dilation_rate=dilation,\n            activation='linear', padding='causal', use_bias=False\n            )(input_)\n        layer_out = Activation('selu')(layer_out)\n        skip_out = Conv1D(1, 1, activation='linear', use_bias=False)(layer_out)\n        network_in = Conv1D(1, 1, activation='linear', use_bias=False)(layer_out)\n        network_out = Add()([residual, network_in])\n        return network_out, skip_out\n    return f\n\ndef DC_CNN_Model(length):\n    input = Input(shape=(length,1))\n    l1a, l1b = DC_CNN_Block(32, 2, 1)(input)\n    l2a, l2b = DC_CNN_Block(32, 2, 2)(l1a)\n    l3a, l3b = DC_CNN_Block(32, 2, 4)(l2a)\n    l4a, l4b = DC_CNN_Block(32, 2, 8)(l3a)\n    l5a, l5b = DC_CNN_Block(32, 2, 16)(l4a)\n    l6a, l6b = DC_CNN_Block(32, 2, 32)(l5a)\n    l6b = Dropout(0.8)(l6b)\n    l7a, l7b = DC_CNN_Block(32, 2, 64)(l6a)\n    l7b = Dropout(0.8)(l7b)\n    l8 =   Add()([l1b, l2b, l3b, l4b, l5b, l6b, l7b])\n    l9 =   Activation('relu')(l8)\n    l21 =  Conv1D(1, 1, activation='linear', use_bias=False)(l9)\n    model = Model(inputs=input, outputs=l21)\n    model.compile(loss='mae', optimizer=optimizers.Adam(),\nmetrics=['mse'])\n    return model\n\n\nTransformer\nThe transformer architecture was introduced in the paper “Attention is All You Need” by Vaswani et al. in 2017. It is a deep learning architecture that is based on the self-attention mechanism. The self-attention mechanism allows the network to weigh the importance of each input feature when making a prediction.\nTransformers introduced two building blocks – multi-head attention and positional embeddings. Rather than working sequentially, sequences are processed as a whole rather than item by item. They employ self-attention, where similarity scores between items in a sentence are stored.\nThere are a lot of works that have been done to adapt the transformer architecture to time series forecasting. The main idea is to use the transformer architecture to capture the temporal dependencies in the time series. Check this blog Yes, Transformers are Effective for Time Series Forecasting (+ Autoformer) or this paper Transformers in Time Series: A Survey\n\n\nDeepAR\nDeepAR is a probabilistic forecasting algorithm developed by Amazon Web Services (AWS) as part of their SageMaker platform. It’s specifically considered as a model to benchmark for many time-series forecasting problems.\n\nDeepAR utilizes long short-term memory (LSTM) networks, to capture complex patterns and dependencies in the time series data. Instead of using LSTMs to calculate predictions directly, DeepAR leverages LSTMs to parameterize a Gaussian likelihood function. That is, to estimate the \\(\\theta = (\\phi, \\gamma)\\) parameters (mean and standard deviation) of the Gaussian function.\nOne of the key features of DeepAR is its ability to handle uncertainty in forecasts by generating probabilistic predictions. Instead of just providing a single point estimate for each forecast, DeepAR produces a full probability distribution over future values, allowing users to understand the range of possible outcomes and their associated probabilities. This can be particularly useful in various applications such as demand forecasting, financial forecasting, and resource planning, where understanding uncertainty is crucial for decision-making.\n\n\n\n\nSummary of the DeepAR model for training (left) and inference (right)\n\n\nHow to train DeepAR?\n\nFirst, the LSTM cell takes as input the covariates \\(x_{i,t}\\) of the current time step \\(t\\) and the target variable \\(z_{i, t-1}\\) of the previous time step \\(t-1\\). Also, the LSTM receives the hidden state \\(h_{i, t-1}\\) from the previous time step. The LSTM cell outputs the hidden state \\(h_{i, t}\\) for the current time step \\(t\\).\nThe \\((\\phi, \\gamma)\\) values are indirectly computed from \\(h_{i, t}\\) and become the parameters of the Gaussian likelihood function \\(p(y_i | \\theta_i) = l(z_{i, t} | \\theta_{i, t})\\). The model tries to answer this question: What are the best parameters \\((\\phi, \\gamma)\\) that construct a gaussian distribution which outputs predictions as close as possible to the real values?\nThe current target value \\(z_{i, t}\\) is then sampled from the Gaussian likelihood function \\(p(y_i | \\theta_i)\\), and the process is repeated for the next time step.\n\nSince DeepAR trains (and predicts) a single data point each time, the model is called autoregressive.\nHow to do inference?\nThe steps for inference are pretty much the same as training steps. But one thing changes: the model does not sample the target value \\(z_{i, t}\\) from the Gaussian likelihood function \\(p(y_i | \\theta_i)\\). Instead, it uses the mean of the Gaussian likelihood function as the prediction for the target value \\(z_{i, t}\\)."
  },
  {
    "objectID": "posts/time-series/time-series.html#time-series-frameworks-and-libraries",
    "href": "posts/time-series/time-series.html#time-series-frameworks-and-libraries",
    "title": "Everything you need to know about Time Series Forecasting",
    "section": "Time-series frameworks and libraries",
    "text": "Time-series frameworks and libraries\n\nProphet\nAs we described earlier in section above, Prophet is a simple but powerful tool for time-series forcasting. If you want to deep dive into Prophet, check the official repo\n\n\nDarts\nIt contains a variety of models, from classics such as ARIMA to deep neural networks. The forecasting models can all be used in the same way, using fit() and predict() functions, similar to scikit-learn. The library also makes it easy to backtest models, combine the predictions of several models, and take external data into account. Darts supports both univariate and multivariate time series and models. The ML-based models can be trained on potentially large datasets containing multiple time series, and some of the models offer a rich support for probabilistic forecasting.\nCheck its repo\n\n\nNeuralProphet\nNeuralProphet is a framework for interpretable time series forecasting. NeuralProphet is built on PyTorch and combines Neural Networks and traditional time-series algorithms, inspired by Facebook Prophet and AR-Net.\nCheck the official repo for more details.\n\n\npytorch-forecasting\nThis repo is quite nice since it provides a training pipeline on pandas dataframes and leverages Pytorch Lightning for scalable training on multiple GPUs. It includes DeepAR, N-BEATS, and Temporal Fusion Transformers."
  },
  {
    "objectID": "posts/time-series/time-series.html#practical-example",
    "href": "posts/time-series/time-series.html#practical-example",
    "title": "Everything you need to know about Time Series Forecasting",
    "section": "Practical example",
    "text": "Practical example\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose, STL\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.arima_process import ArmaProcess\nfrom statsmodels.graphics.gofplots import qqplot\nfrom statsmodels.tsa.stattools import adfuller\nfrom tqdm import tqdm\nfrom itertools import product\nfrom typing import Union\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nDataset overview\nWe will go back to our dataset in the first section to try out some practices.\n\nThe Airline Passengers dataset describes the total number of airline passengers over a period of time. The units are a count of the number of airline passengers in thousands. There are 144 monthly observations from 1949 to 1960.\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv', header=0, index_col=0)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nPassengers\n\n\nMonth\n\n\n\n\n\n1949-01\n112\n\n\n1949-02\n118\n\n\n1949-03\n132\n\n\n1949-04\n129\n\n\n1949-05\n121\n\n\n\n\n\n\n\n\n\n1. Identify and visualize the seasonal component of the time series\n\nfrom statsmodels.tsa.seasonal import STL\n\ndecomposition = STL(df['Passengers'], period=12).fit()\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=4, ncols=1, sharex=True, figsize=(10,8))\n\nax1.plot(decomposition.observed)\nax1.set_ylabel('Observed')\n\nax2.plot(decomposition.trend)\nax2.set_ylabel('Trend')\n\nax3.plot(decomposition.seasonal)\nax3.set_ylabel('Seasonal')\n\nax4.plot(decomposition.resid)\nax4.set_ylabel('Residuals')\n\nplt.xticks(np.arange(0, 145, 12), np.arange(1949, 1962, 1))\n\nfig.autofmt_xdate()\nplt.tight_layout()\n\n\n\n\n\n\n2. Apply the ADF test to check if the time series is stationary\nThe presenece of a trend means that our series is likely non-stationary. Let’s verify that using the Augmented Dickey-Fuller (ADF) test.\n\nfrom statsmodels.tsa.stattools import adfuller\n\nADF_result = adfuller(df['Passengers'])\n\nprint(f'ADF Statistic: {ADF_result[0]}')\nprint(f'p-value: {ADF_result[1]}')\n\nADF Statistic: 0.8153688792060447\np-value: 0.9918802434376409\n\n\nThe ADF statistic of 0.815 is not a large negative number and p-value of 0.991, which is greater than 0.05, we cannot reject the null hypothesis stating that our time series is not stationary. Therefore, our series is not stationary.\n\n\n3. Apply a transformation to make the time series stationary\nTo remove the effect of the trend and stabilize the mean of the series, we will use differencing.\n\ndf_diff = np.diff(df['Passengers'], n=1) # n=1 for first order differencing\n\nADF_result = adfuller(df_diff)\nprint(f'ADF Statistic: {ADF_result[0]}')\nprint(f'p-value: {ADF_result[1]}')\n\nADF Statistic: -2.8292668241699794\np-value: 0.054213290283828236\n\n\nThis returns an ADF statistic of -2.83 and a p-value of 0.054. We can’t reject the null hypothesis, and differencing the series once did not make it stationary. We will try differencing the series again.\n\ndf_diff_second = np.diff(df_diff, n=1) # n=1 for first order differencing\n\nADF_result = adfuller(df_diff_second)\nprint(f'ADF Statistic: {ADF_result[0]}')\nprint(f'p-value: {ADF_result[1]}')\n\nADF Statistic: -16.38423154246855\np-value: 2.732891850013928e-29\n\n\nNow, we can reject the null hypothesis, and our series is considered to be stationary. Since the series was differenced twice to become stationary, thus d = 2\n\n\nFitting ARIMA model\n\ndef optimize_ARIMA(endog: Union[pd.Series, list], order_list: list, d: int) -&gt; pd.DataFrame:\n    \n    results = []\n    \n    for order in tqdm(order_list):\n        try: \n            model = SARIMAX(endog, order=(order[0], d, order[1]), simple_differencing=False).fit(disp=False)\n        except:\n            continue\n            \n        aic = model.aic\n        results.append([order, aic])\n        \n    result_df = pd.DataFrame(results)\n    result_df.columns = ['(p,q)', 'AIC']\n    \n    #Sort in ascending order, lower AIC is better\n    result_df = result_df.sort_values(by='AIC', ascending=True).reset_index(drop=True)\n    \n    return result_df\n\n\nps = range(0, 13, 1)\nqs = range(0, 13, 1)\n\nd = 2\n\n\nARIMA_order_list = list(product(ps, qs))\n\ntrain = df['Passengers'][:-12]\n\nARIMA_result_df = optimize_ARIMA(train, ARIMA_order_list, d)\nARIMA_result_df\n\n  2%|▏         | 4/169 [00:00&lt;00:10, 15.99it/s]100%|██████████| 169/169 [19:08&lt;00:00,  6.79s/it]\n\n\n\n\n\n\n\n\n\n(p,q)\nAIC\n\n\n\n\n0\n(11, 3)\n1016.838772\n\n\n1\n(11, 4)\n1019.018847\n\n\n2\n(11, 5)\n1020.377574\n\n\n3\n(11, 1)\n1021.028121\n\n\n4\n(12, 0)\n1021.231718\n\n\n...\n...\n...\n\n\n164\n(5, 0)\n1281.732157\n\n\n165\n(3, 0)\n1300.282335\n\n\n166\n(2, 0)\n1302.913196\n\n\n167\n(1, 0)\n1308.152194\n\n\n168\n(0, 0)\n1311.919269\n\n\n\n\n169 rows × 2 columns\n\n\n\n\nprint(model_fit.summary())\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:             Passengers   No. Observations:                  132\nModel:                ARIMA(11, 2, 3)   Log Likelihood                -493.419\nDate:                Mon, 19 Feb 2024   AIC                           1016.839\nTime:                        23:33:01   BIC                           1059.852\nSample:                    01-01-1949   HQIC                          1034.316\n                         - 12-01-1959                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.L1         -0.8239      0.100     -8.217      0.000      -1.020      -0.627\nar.L2         -0.9630      0.049    -19.750      0.000      -1.059      -0.867\nar.L3         -0.8520      0.087     -9.749      0.000      -1.023      -0.681\nar.L4         -0.9539      0.047    -20.370      0.000      -1.046      -0.862\nar.L5         -0.8331      0.092     -9.102      0.000      -1.013      -0.654\nar.L6         -0.9500      0.043    -22.211      0.000      -1.034      -0.866\nar.L7         -0.8361      0.089     -9.406      0.000      -1.010      -0.662\nar.L8         -0.9624      0.049    -19.607      0.000      -1.059      -0.866\nar.L9         -0.8250      0.086     -9.602      0.000      -0.993      -0.657\nar.L10        -0.9581      0.031    -30.930      0.000      -1.019      -0.897\nar.L11        -0.8087      0.096     -8.468      0.000      -0.996      -0.622\nma.L1         -0.3339      0.136     -2.456      0.014      -0.600      -0.067\nma.L2          0.2206      0.161      1.374      0.170      -0.094       0.535\nma.L3         -0.2910      0.141     -2.061      0.039      -0.568      -0.014\nsigma2       104.3926     17.689      5.901      0.000      69.722     139.063\n===================================================================================\nLjung-Box (L1) (Q):                   0.00   Jarque-Bera (JB):                 3.82\nProb(Q):                              0.96   Prob(JB):                         0.15\nHeteroskedasticity (H):               2.23   Skew:                            -0.02\nProb(H) (two-sided):                  0.01   Kurtosis:                         3.84\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\nThe result shows that a value of p = 11 and q = 3 is the best fit for our model, which results in the lowest AIC value. Therefore, and ARIMA(11,2,3) model is the most suitable for our time series.\nWe then fit an ARIMA(11,2,3) model to our time series and plot the residuals. The residuals should be normally distributed, with a mean of 0 and a constant variance. If the residuals are not normally distributed, then the model is not capturing all the information in the data. If the residuals have a mean different from 0, then the model is biased. If the variance of the residuals is not constant, then the model is not capturing all the information in the data.\n\nfrom statsmodels.tsa.arima.model import ARIMA \n\nmodel = ARIMA(train, order=(11, 2, 3))\nmodel_fit = model.fit()\n\n_ = model_fit.plot_diagnostics(figsize=(10,8))\n\n\n\n\n\nThe top-left plot shows the residuals over time. While there is no trend in the residuals, the variance does not seem to be constant, which is a discrepancy in comparison to white noise\nThe top-right plot shows the distribution of the residuals, which is close to a normal distribution.\nThe bottom-left plot (Q-Q plot) displays a line that is fairly straight, indicating that the residuals are normally distributed.\nThe bottom-right plot shows the autocorrelation of the residuals, which is close to 0 for all lags, indicating that the residuals are uncorrelated.\n\nTherefore, from a qualitative standpoint, it seems that our residuals are close to white noise, which is a good sign, as it means that the model’s errors are random.\nOur model then can be used to forecast future values of the time series.\n\ntest = df.iloc[-12:]\n\ntest['naive_seasonal'] = df['Passengers'].iloc[120:132].values\n\nARIMA_pred = model_fit.get_prediction(132, 143).predicted_mean\n\ntest['ARIMA_pred'] = ARIMA_pred.values\ntest\n\n\n\n\n\n\n\n\nPassengers\nnaive_seasonal\nARIMA_pred\n\n\nMonth\n\n\n\n\n\n\n\n1960-01\n417\n360\n422.368712\n\n\n1960-02\n391\n342\n410.637667\n\n\n1960-03\n419\n406\n461.827488\n\n\n1960-04\n461\n396\n457.827939\n\n\n1960-05\n472\n420\n481.698510\n\n\n1960-06\n535\n472\n531.108649\n\n\n1960-07\n622\n548\n606.151268\n\n\n1960-08\n606\n559\n615.479569\n\n\n1960-09\n508\n463\n525.628309\n\n\n1960-10\n461\n407\n467.123847\n\n\n1960-11\n390\n362\n425.079157\n\n\n1960-12\n432\n405\n467.360271\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\n\nax.plot(df.index, df['Passengers'])\nax.plot(test['Passengers'], 'b-', label='actual')\nax.plot(test['naive_seasonal'], 'r:', label='naive seasonal')\nax.plot(test['ARIMA_pred'], 'k--', label='ARIMA(11,2,3)')\n\nax.set_xlabel('Date')\nax.set_ylabel('Number of air passengers')\nax.axvspan(132, 143, color='#808080', alpha=0.2)\n\nax.legend(loc=2)\n\nplt.xticks(np.arange(0, 145, 12), np.arange(1949, 1962, 1))\nax.set_xlim(120, 143)\n\nfig.autofmt_xdate()\nplt.tight_layout()\n\n\n\n\n\n\nFitting SARIMA model\n\ndef optimize_SARIMA(endog: Union[pd.Series, list], order_list: list, d: int, D: int, s: int) -&gt; pd.DataFrame:\n    \n    results = []\n    \n    for order in tqdm(order_list):\n        try: \n            model = SARIMAX(\n                endog, \n                order=(order[0], d, order[1]),\n                seasonal_order=(order[2], D, order[3], s),\n                simple_differencing=False).fit(disp=False)\n        except:\n            continue\n            \n        aic = model.aic\n        results.append([order, aic])\n        \n    result_df = pd.DataFrame(results)\n    result_df.columns = ['(p,q,P,Q)', 'AIC']\n    \n    #Sort in ascending order, lower AIC is better\n    result_df = result_df.sort_values(by='AIC', ascending=True).reset_index(drop=True)\n    \n    return result_df\n\nps = range(0, 4, 1)\nqs = range(0, 4, 1)\nPs = range(0, 4, 1)\nQs = range(0, 4, 1)\n\nSARIMA_order_list = list(product(ps, qs, Ps, Qs))\n\ntrain = df['Passengers'][:-12]\n\nd = 1\nD = 1\ns = 12\n\nSARIMA_result_df = optimize_SARIMA(train, SARIMA_order_list, d, D, s)\nSARIMA_result_df\n\n100%|██████████| 256/256 [1:06:31&lt;00:00, 15.59s/it]\n\n\n\n\n\n\n\n\n\n(p,q,P,Q)\nAIC\n\n\n\n\n0\n(2, 1, 1, 2)\n892.232482\n\n\n1\n(2, 1, 2, 1)\n893.527182\n\n\n2\n(2, 1, 1, 3)\n894.093579\n\n\n3\n(1, 0, 1, 2)\n894.287516\n\n\n4\n(0, 1, 1, 2)\n894.987878\n\n\n...\n...\n...\n\n\n250\n(0, 0, 2, 0)\n906.940147\n\n\n251\n(3, 2, 0, 3)\n907.181875\n\n\n252\n(0, 0, 3, 2)\n907.514319\n\n\n253\n(0, 0, 3, 0)\n908.742583\n\n\n254\n(0, 0, 0, 3)\n908.781405\n\n\n\n\n255 rows × 2 columns\n\n\n\n\nSARIMA_model = SARIMAX(train, order=(2,1,1), seasonal_order=(1,1,2,12), simple_differencing=False)\nSARIMA_model_fit = SARIMA_model.fit(disp=False)\n\nprint(SARIMA_model_fit.summary())\n\n                                        SARIMAX Results                                        \n===============================================================================================\nDep. Variable:                              Passengers   No. Observations:                  132\nModel:             SARIMAX(2, 1, 1)x(1, 1, [1, 2], 12)   Log Likelihood                -439.116\nDate:                                 Tue, 20 Feb 2024   AIC                            892.232\nTime:                                         10:19:10   BIC                            911.686\nSample:                                     01-01-1949   HQIC                           900.132\n                                          - 12-01-1959                                         \nCovariance Type:                                   opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.L1         -1.2665      0.085    -14.945      0.000      -1.433      -1.100\nar.L2         -0.3404      0.077     -4.401      0.000      -0.492      -0.189\nma.L1          0.9997      0.666      1.502      0.133      -0.305       2.305\nar.S.L12       0.9993      0.111      9.043      0.000       0.783       1.216\nma.S.L12      -1.3391      2.067     -0.648      0.517      -5.390       2.712\nma.S.L24       0.3581      0.655      0.546      0.585      -0.926       1.642\nsigma2        78.2020    148.583      0.526      0.599    -213.015     369.419\n===================================================================================\nLjung-Box (L1) (Q):                   0.06   Jarque-Bera (JB):                 0.90\nProb(Q):                              0.80   Prob(JB):                         0.64\nHeteroskedasticity (H):               1.56   Skew:                            -0.08\nProb(H) (two-sided):                  0.16   Kurtosis:                         3.39\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\nSARIMA_model_fit.plot_diagnostics(figsize=(10,8));\n\n\n\n\nThe final test to determine whether we can use this model for forecasting or not is the Ljung-Box test.\n\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\n\nresiduals = SARIMA_model_fit.resid\n\ndf_stats = acorr_ljungbox(residuals.values, lags= np.arange(1, 11, 1))\n\ndf_stats\n\n\n\n\n\n\n\n\nlb_stat\nlb_pvalue\n\n\n\n\n1\n0.004873\n0.944347\n\n\n2\n0.745767\n0.688746\n\n\n3\n1.022897\n0.795712\n\n\n4\n1.227105\n0.873616\n\n\n5\n1.436496\n0.920280\n\n\n6\n1.712124\n0.944181\n\n\n7\n2.308987\n0.940781\n\n\n8\n2.719618\n0.950703\n\n\n9\n2.736189\n0.973843\n\n\n10\n4.969893\n0.893180\n\n\n\n\n\n\n\nThe returned p-values are all greater than 0.05. Therefore, we do not reject the null hypothesis, and we conclude that the residuals are independent and uncorrelated, just like white noise. Our model has passed all the tests from the residuals analysis, and we are ready to use it for forecasting.\n\nSARIMA_pred = SARIMA_model_fit.get_prediction(132, 143).predicted_mean\n\ntest['SARIMA_pred'] = SARIMA_pred.values\ntest\n\n\n\n\n\n\n\n\nPassengers\nnaive_seasonal\nARIMA_pred\nSARIMA_pred\n\n\nMonth\n\n\n\n\n\n\n\n\n1960-01\n417\n360\n422.368712\n418.513901\n\n\n1960-02\n391\n342\n410.637667\n399.577187\n\n\n1960-03\n419\n406\n461.827488\n461.313429\n\n\n1960-04\n461\n396\n457.827939\n451.452634\n\n\n1960-05\n472\n420\n481.698510\n473.744198\n\n\n1960-06\n535\n472\n531.108649\n538.788480\n\n\n1960-07\n622\n548\n606.151268\n612.440323\n\n\n1960-08\n606\n559\n615.479569\n624.594874\n\n\n1960-09\n508\n463\n525.628309\n520.173138\n\n\n1960-10\n461\n407\n467.123847\n462.852995\n\n\n1960-11\n390\n362\n425.079157\n412.723148\n\n\n1960-12\n432\n405\n467.360271\n454.252799\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\n\nax.plot(df.index, df['Passengers'])\nax.plot(test['Passengers'], 'b-', label='actual')\nax.plot(test['naive_seasonal'], 'r:', label='naive seasonal')\nax.plot(test['ARIMA_pred'], 'k--', label='ARIMA(11,2,3)')\nax.plot(test['SARIMA_pred'], 'g-.', label='SARIMA(2,1,1)(1,1,2,12)')\n\nax.set_xlabel('Date')\nax.set_ylabel('Number of air passengers')\nax.axvspan(132, 143, color='#808080', alpha=0.2)\n\nax.legend(loc=2)\n\nplt.xticks(np.arange(0, 145, 12), np.arange(1949, 1962, 1))\nax.set_xlim(120, 143)\n\nfig.autofmt_xdate()\nplt.tight_layout()\n\n\n\n\nWe’ll use the mean absolute percentage error (MAPE) to evaluate each model.\n\n\ndef mape(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\nmape_naive_seasonal = mape(test['Passengers'], test['naive_seasonal'])\nmape_ARIMA = mape(test['Passengers'], test['ARIMA_pred'])\nmape_SARIMA = mape(test['Passengers'], test['SARIMA_pred'])\n\nprint(f'MAPE Naive Seasonal: {mape_naive_seasonal}')\nprint(f'MAPE ARIMA: {mape_ARIMA}')\nprint(f'MAPE SARIMA: {mape_SARIMA}')\n\nMAPE Naive Seasonal: 9.987532920823485\nMAPE ARIMA: 3.8410155255655063\nMAPE SARIMA: 2.8487689091201363\n\n\n\nfig, ax = plt.subplots()\n\nx = ['naive seasonal', 'ARIMA(11,2,3)', 'SARIMA(2,1,1)(1,1,2,12)']\ny = [mape_naive_seasonal, mape_ARIMA, mape_SARIMA]\n\nax.bar(x, y, width=0.4)\nax.set_xlabel('Models')\nax.set_ylabel('MAPE (%)')\nax.set_ylim(0, 15)\n\nfor index, value in enumerate(y):\n    plt.text(x=index, y=value + 1, s=str(round(value,2)), ha='center')\n\nplt.tight_layout()\n\n\n\n\nSo the SARIMA model is the better performing method for this situation compared to our naive and ARIMA models. This makes sense, since our dataset had clear seasonality, and the SARIMA model is built to use the seasonal properties of time series to make forecasts.\n\n\nProphet\nProphet follows the sklearn API, where a model is initialized by creating an instance of the Prophet class, the model is trained using the fit method, and predictions are generated using the predict method. Therefore, we’ll first initialize a Prophet model by creating an instance of the Prophet class\n\nfrom prophet import Prophet\n\nmodel = Prophet()\n\ntrain = train.reset_index().rename(columns={'Month':'ds', 'Passengers':'y'})\ntest = test.reset_index().rename(columns={'Month':'ds', 'Passengers':'y'})\n\nmodel.fit(train)\n\n14:07:39 - cmdstanpy - INFO - Chain [1] start processing\n14:07:41 - cmdstanpy - INFO - Chain [1] done processing\n\n\n&lt;prophet.forecaster.Prophet at 0x2a3f1b810&gt;\n\n\n\nprophet_pred = model.predict(test)\n\n\nprint(f\"MAPE prophet: {mape(test['y'], prophet_pred['yhat'])}\")\n\nMAPE prophet: 6.615923942230125\n\n\nProphet comes with many methods that allow us to quickly visualize a model’s predic- tions or its different components.\n\n_ = model.plot(prophet_pred)\n\n\n\n\nWe can also display the different components used in our model\n\nfig2 = model.plot_components(prophet_pred)"
  },
  {
    "objectID": "posts/time-series/time-series.html#references",
    "href": "posts/time-series/time-series.html#references",
    "title": "Everything you need to know about Time Series Forecasting",
    "section": "References",
    "text": "References\n\nTime Series Forecasting in Python by Marco Peixeiro\nMachine Learning for Time Series with Python By Ben Auffarth\nDarts\n\nIf you liked this article, please follow me on Medium and Twitter @jonathan_ttu."
  },
  {
    "objectID": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html",
    "href": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html",
    "title": "Everything you need to know about Llama 2",
    "section": "",
    "text": "LLaMA 2, the successor of the original LLaMA 1, is a massive language model created by Meta. It is open for both research and commercial purposes, made available through various providers like AWS and Hugging Face. The pretrained models of LLaMA 2 have undergone extensive training on an impressive 2 trillion tokens, offering twice the context length compared to LLaMA 1. Additionally, its fine-tuned models have been refined using over 1 million human annotations."
  },
  {
    "objectID": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#is-it-trully-open-sourced",
    "href": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#is-it-trully-open-sourced",
    "title": "Everything you need to know about Llama 2",
    "section": "Is it trully open sourced?",
    "text": "Is it trully open sourced?\nTechnically, the whole project is not open-source because the development and use of it is not fully available to the entire public. While the model is open to public, it is very useful for the open-source community, but we should call it an open release instead of open source."
  },
  {
    "objectID": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#what-is-llama-2-base-and-llama-2-chat",
    "href": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#what-is-llama-2-base-and-llama-2-chat",
    "title": "Everything you need to know about Llama 2",
    "section": "What is Llama 2 base and Llama 2 chat?",
    "text": "What is Llama 2 base and Llama 2 chat?\nThe base models are uncensored, and are not instruct-tuned or chat-tuned.\nThe chat models are censored, and have been chat-tuned, are optimized for dialogue use cases."
  },
  {
    "objectID": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#how-to-extend-context-for-llama-2-beyond-4k",
    "href": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#how-to-extend-context-for-llama-2-beyond-4k",
    "title": "Everything you need to know about Llama 2",
    "section": "How to extend context for LLama 2, beyond 4K?",
    "text": "How to extend context for LLama 2, beyond 4K?\nWe can extend context from 4K to 8k, 32k or 128k tokens with technique using Position Interpolation\n\nExtending Context is Hard…but not Impossible: awesome blog on the subject\nLong-Context: Extending LLM Context Length. A range of experiments with different schemes for extending context length capabilities of Llama, which has been pretrained on 2048 context length with the RoPE (Rotary Position Embedding) encoding."
  },
  {
    "objectID": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#what-is-training-data-for-base-model",
    "href": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#what-is-training-data-for-base-model",
    "title": "Everything you need to know about Llama 2",
    "section": "What is training data for base model?",
    "text": "What is training data for base model?\n\nOur training corpus includes a new mix of data from publicly available sources, which does not include data from Meta’s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this provides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations. source\n\nThat’s simply all details that Meta gives in their paper, although we are very curious about which datasets contain a high volume of personal information or their detailed technique about up-sampling the factural sources."
  },
  {
    "objectID": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#what-is-preference-data-for-reward-model",
    "href": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#what-is-preference-data-for-reward-model",
    "title": "Everything you need to know about Llama 2",
    "section": "What is preference data for reward model?",
    "text": "What is preference data for reward model?\nFirst of all, the reward model is the key of RLHF. In order to get a good reward model, Meta had to push hard on gathering preference data extremely upgraded from what the open-source community is working with.\nIn summary, the key points about preference data:\n\nUse multi-turn preferences, where model responses are taken from different model checkpoints with varying temperatures to generate diversity between pairs.\nbinary comparison: either their choice is significantly better, better, slightly better, or negligibly better/ unsure.\nFocus on helpfulness and safety (as opposed to honesty), using separate guidelines at data collection time for each data vendor (e.g. safety is often a much more deceptive prompting style). This is most contrasted to Anthropic’s works, where they train a model that is Helpful, Honest, and Harmless.\nIterative collection for distribution management: “Human annotations were collected in batches on a weekly basis. As we collected more preference data, our reward models improved, and we were able to train progressively better versions for Llama 2-Chat”\nThe team added additional safety metadata to the collection showcasing which responses are safe from the models at each turn. When this is passed to the modeling phase, they “do not include any examples where the chosen response was unsafe and the other response safe, as we believe safer responses will also be better/preferred by humans.”\n\n\n\n\nCapture d’écran 2023-08-03 à 00.27.00.png"
  },
  {
    "objectID": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#how-to-train-reward-models",
    "href": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#how-to-train-reward-models",
    "title": "Everything you need to know about Llama 2",
    "section": "How to train reward models?",
    "text": "How to train reward models?\nThe reward model takes a model response and its corresponding prompt (including contexts from previous turns) as inputs and outputs a scalar score to indicate the quality (e.g.,helpfulness and safety) of the model generation.\nThey train two separate reward models, one optimized for helpfulness (referred to as Helpfulness RM) and another for safety (Safety RM).\nThey initialize reward models from pretrained chat model checkpoints, as it ensures that both models benefit from knowledge acquired in pretraining. In short, the reward model “knows” what the chat model knows. The model architecture and hyper-parameters are identical to those of the pretrained language models, except that the classification head for next-token prediction is replaced with a regression head for outputting a scalar reward."
  },
  {
    "objectID": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#what-is-training-process-of-llama-2-chat",
    "href": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#what-is-training-process-of-llama-2-chat",
    "title": "Everything you need to know about Llama 2",
    "section": "What is training process of Llama 2-chat?",
    "text": "What is training process of Llama 2-chat?\n\n\n\nUntitled"
  },
  {
    "objectID": "posts/2020-05-20-autoregressive-generative-models.html",
    "href": "posts/2020-05-20-autoregressive-generative-models.html",
    "title": "Autoregressive Generative Models",
    "section": "",
    "text": "Generative model is a subset of unsupervised learning which has been recieving a lot of attention for last few years. The idea is that given a training dataset, we will use models or algorithms to generate new samples with the same distribution.\n“What I cannot create, I do not understand.”\n—Richard Feynman\nSuppose we have a dataset containing images of dogs. We may wish to build a model that can generate a new image of a dog that has never existed but still looks real because the model has learned the general rules that govern the appearance of a dog. This is the kind of problem that can be solved using generative modeling.\nIn mathematical terms, generative modeling estimates \\(p(x)\\) —the probability of observing observation \\(x\\). In fact, our model tries to learn to construct an estimate \\(p_{model}(x)\\) as similar as possible to the probability density function \\(p_{data}(x)\\).\nIn this blog, we will take a deep look at one of popular approaches to tackle this problem which is Autoregressive Generative Models"
  },
  {
    "objectID": "posts/2020-05-20-autoregressive-generative-models.html#masked-autoencoder-for-distribution-estimation-made",
    "href": "posts/2020-05-20-autoregressive-generative-models.html#masked-autoencoder-for-distribution-estimation-made",
    "title": "Autoregressive Generative Models",
    "section": "Masked Autoencoder for Distribution Estimation (MADE)",
    "text": "Masked Autoencoder for Distribution Estimation (MADE)\nThe idea of MADE is built on top of autoencoder architecture. So, we’ll have a quick look on vanila autoencoder.\n\nAutoencoder\nOur primary goal is take an input sample \\(\\mathbf{x}\\) and transform it to some latent dimension \\(\\mathbf{z}\\) (encoder), which hopefully is a good representation of the original data.\nwhat is a good representation?  \n\"A good representation is one where you can reconstruct the original input!\". \nThe process of transforming the latent dimension \\(\\mathbf{z}\\) back to a reconstructed version of the input \\(\\mathbf{\\hat{x}}\\) is called the decoder. It’s an autoencoder because it’s using the same value \\(\\mathbf{x}\\) value on the input and output.\nMathematically, an encoder is mapping an input \\(\\mathbf{x}\\) to a feature vector \\(\\mathbf{h} = \\mathbf{f}_{\\theta}(\\mathbf{x})\\) while a decoder tries to map from feature space back into input space producing a reconstruction \\(\\mathbf{\\hat{x}}= \\mathbf{g}_{\\theta}(\\mathbf{h}) = \\mathbf{g}_{\\theta}\\big(\\mathbf{f}_{\\theta}(\\mathbf{x})\\big)\\). The set of parameters \\(\\theta\\) of the encoder and decoder are learned simultaneously on the task of reconstructing as well as possible the original input, i.e.attempting to incur the lowest possible reconstruction error \\(\\mathcal{L}(\\mathbf{x},\\mathbf{\\hat{x}})\\) - a measure of the discrepancy between \\(\\mathbf{x}\\) and its reconstruction \\(\\mathbf{\\hat{x}}\\) - over training examples\n\nTo train autoencoder, we use cross-entropy loss: \\[\\begin{align*}\n\\mathcal{L_{\\text{binary}}}({\\bf \\mathbf{x}}) &= \\sum_{i=1}^N -\\mathbf{x}_i\\log \\hat{\\mathbf{x}}_i - (1-\\mathbf{x}_i)\\log(1-\\hat{\\mathbf{x}_i}) \\tag{2} \\\\\n\\end{align*}\\]\nTo capture the structure of the data-generating distribution, it is therefore important that something in the training criterion or the parametrization prevents the auto-encoder from learning the identity function, which has zero reconstruction error everywhere. This is achieved through various means regularized autoencoders (refer to [3] to see more about it)\n\n\nMasked Autoencoders\nSince autoencoder is to reconstruct the input from learning a latent representation of data in an unsupervised manner, it can’t provide a proper probability distribution. Therefore, it can usually be used in applications such as denoising images but can’t generate a total new sample.\nThe reason is that in autoencoder, each output \\(\\mathbf{\\hat{x}}_i\\) could depend on any of the components input \\(\\mathbf{x}_1,…,\\mathbf{x}_n\\). So in order to convert to our autoregressive models as defined above, we can modify the structure so that \\(\\mathbf{\\hat{x}}_i\\) only depend on previous components \\(\\mathbf{x}_1,…,\\mathbf{x}_{i-1}\\) but not the future ones \\(\\mathbf{x}_{i+1},…,\\mathbf{x}_n\\).\nThe principle becomes as following: - Each output of the network \\(\\mathbf{\\hat{x}}_i\\) represents the probability distribution \\(\\mathbf{p}\\big(\\mathbf{x}_i | \\mathbf{x}_{&lt;i}\\big)\\)\n\nEach output \\(\\mathbf{\\hat{x}}_i\\) can only have connections (recursively) to smaller indexed inputs \\(\\mathbf{x}_{&lt;i}\\) and not any of the other ones.\n\nTo persuit this principle, the MADE authors came up with the idea of masked autoencoders. Since output \\(\\mathbf{\\hat{x}}_i\\) must depend only on the preceding inputs \\(\\mathbf{x}_{&lt;i}\\), it means that there must be no computational path between output unit \\(\\mathbf{\\hat{x}}_i\\) and any of the input units \\(\\mathbf{x}_i, ... \\mathbf{x}_N\\). To do so, we will zero-out the weights we don’t want by creating a binary mask matrix, whose entries that are set to 0 correspond to the connections we wish to remove.\nTake a simple case when encoder and decoder are only one layer of feed-forward.\n\\[\\begin{align*}\n{\\bf h}({\\bf x}) &= {\\bf g}({\\bf b} + {\\bf Wx}) \\\\\n{\\hat{\\bf x}} &= \\text{sigm}({\\bf c} + {\\bf V h(x)})  \n\\end{align*}\\]\nwhere - \\(\\odot\\) is an element wise product\n\n\\({\\bf x}, \\hat{\\bf x}\\) is our vectors of input/output respectively\n\\(\\bf h(x)\\) is the hidden layer\n\\(\\bf g(⋅)\\) is the activation function of the hidden layer\n\\(\\text{sigm}\\) is the sigmoid activation function of the output layer\n\\(\\bf b\\), \\(\\bf c\\) are the constant biases for the hidden/output layer respectively\n\\(\\bf W\\), \\(\\bf V\\) are the weight matrices for the hidden/output layer respectively\n\nDenote \\(\\bf M^W\\), \\(\\bf M^V\\) the masks for \\(\\bf W\\) and \\(\\bf V\\) respectively. The equations with masked autoencoders become:\n\\[\\begin{align*}\n{\\bf h}({\\bf x}) &= {\\bf g}({\\bf b} + {\\bf (W \\odot M^W)x}) \\\\\n{\\hat{\\bf x}} &= \\text{sigm}({\\bf c} + {\\bf (V \\odot M^V)h(x)})  \n\\end{align*}\n\\]\nThe last problem is only find a way to construct masks \\(\\bf M^W\\), \\(\\bf M^V\\) which sastify autoregressive property.\nLet \\(m^{l}(k)\\) be the index assigned to hidden node \\(k\\) in layer \\(l\\). The condition would be as follows:\n\nFirst, for each hidden layer \\(l\\), we sample \\(m^{l}(k)\\) from a uniform distribution with range \\([1,D−1]\\). The index \\(D\\) should be never used because nothing should depend on \\(D^{th}\\) input\nFor a given node, it only connects to nodes in the previous layer that have an index less than or equal to its index.\n\n\\[\\ M^{W^l}_{k', k} =  \\begin{cases}\n                      1 \\text{ if } m^l(k') \\geq m^{l-1}(k)  \\\\\n                      0 \\text{ otherwise}\n                    \\end{cases}\n\\\n\\] - The output mask is slightly different:\n\\[\\ M^{V}_{d, k} = \\begin{cases}\n                    1 \\text{ if } d &gt; m^{L}(k)  \\\\\n                    0 \\text{ otherwise}\n                   \\end{cases}\n\\\n\\]\n\nIn figure 2: - output 1 is not connected to anything. It will just be estimated with a single constant parameter derived from the bias node. Otherwise, output 2 is only connected to hiddens which are only connected to input 1. Finally, output 3 is connected to hiddens which come from input 1 and input 2\n\nOn the other hand, input 3 is connected to nothing because no node should depend on it (autoregressive property).\n\n\n\nDiscussion\n\nDoes ordering of input matters?\n\nActually, there is no natural ordering input. We can shuffle the input dimensions, so that MADE is able to model any arbitrary ordering.\n\nHow can we generate new samples?\n\nSampling steps: - Randomly generate vector x, set \\(i=1\\) - Feed \\(\\bf x\\) into autoencoder and generate outputs \\(\\hat{\\bf x}\\) for the network, set \\(p =\\bf \\hat{x}_i\\) - Sample from a Bernoulli distribution with parameter p, set input \\({\\bf x}_i=Bernoulli(p)\\) - Increment \\(i\\) and repeat steps 2-4 until \\(i &gt; D\\).\n\n\nImplementation\n\nThe Pytorch code implementation of MADE is borrowed from this repo.\nIn this example, We will build a network training on binarized MNIST dataset\n\n\n\nCode\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\nfrom torchvision.utils import make_grid\nimport torch\nimport torch.utils.data as data\nfrom torch import optim\nfrom torch import nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef load_data():\n    \n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        lambda x: (x &gt; 0.5).float() #binarize image\n    ])\n    train_dset = MNIST('data', transform=transform, train=True, download=True)\n    test_dset = MNIST('data', transform=transform, train=False, download=True)\n\n    train_loader = data.DataLoader(train_dset, batch_size=128, shuffle=True,\n                                   pin_memory=True, num_workers=2)\n    test_loader = data.DataLoader(test_dset, batch_size=128, shuffle=True,\n                                  pin_memory=True, num_workers=2)\n\n    return train_loader, test_loader\n\ndef plot_train_curves(epochs, train_losses, test_losses, title=''):\n    x = np.linspace(0, epochs, len(train_losses))\n    plt.figure()\n    plt.plot(x, train_losses, label='train_loss')\n    if test_losses:\n        plt.plot(x, test_losses, label='test_loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title(title)\n    plt.legend()\n    plt.show()\n\ndef visualize_batch(batch_tensor, nrow=8, title='', figsize=None):\n    grid_img = make_grid(batch_tensor, nrow=nrow)\n    plt.figure(figsize=figsize)\n    plt.title(title)\n    plt.imshow(grid_img.permute(1, 2, 0))\n    plt.axis('off')\n    plt.show()\n    \ntrain_loader, test_loader = load_data()\n\n\n\n\nCode\ndef train(model, train_loader, optimizer):\n    model.train()\n    for x, _ in train_loader:\n        loss = model.nll(x)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    return model\n\n\ndef eval_loss(model, data_loader):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for x, _ in data_loader:\n            loss = model.nll(x)\n            total_loss += loss * x.shape[0]\n        avg_loss = total_loss / len(data_loader.dataset)\n    return avg_loss.item()\n\n\ndef train_epochs(model, train_loader, test_loader, train_args):\n    epochs, lr = train_args['epochs'], train_args['lr']\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    train_losses, test_losses = [], []\n    \n    samples = model.sample(64)\n    visualize_batch(samples, title=f'Intialization')\n    for epoch in range(epochs):\n        model.train()\n\n        model = train(model, train_loader, optimizer)\n        train_loss = eval_loss(model, train_loader)\n        train_losses.append(train_loss)\n\n        if test_loader is not None:\n            test_loss = eval_loss(model, test_loader)\n            test_losses.append(test_loss)\n            \n        samples = model.sample(64)\n        if epoch % 10 == 0:    \n            print(f'Epoch {epoch} Test Loss: {test_losses[epoch] / np.log(2):.4f} bits/dim')\n            visualize_batch(samples, title=f'Epoch {epoch}')\n        \n    if test_loader is not None:\n        print('Test Loss', test_loss)\n\n    plot_train_curves(epochs, train_losses, test_losses, title='Training Curve')\n\n\n\nclass MaskedLinear(nn.Linear):\n    \"\"\" same as Linear except has a configurable mask on the weights \"\"\"\n\n    def __init__(self, in_features, out_features, bias=True):\n        super().__init__(in_features, out_features, bias)\n        self.register_buffer('mask', torch.ones(out_features, in_features))\n\n    def set_mask(self, mask):\n        self.mask.data.copy_(torch.from_numpy(mask.astype(np.uint8).T))\n\n    def forward(self, input):\n        return F.linear(input, self.mask * self.weight, self.bias)\n\n\nclass MADE(nn.Module):\n    def __init__(self, device):\n        super().__init__()\n        self.nin = 784 #28 * 28\n        self.nout = 784\n        self.hidden_sizes = [512, 512, 512]\n        self.device = device\n\n        # define a simple MLP neural net\n        self.net = []\n        hs = [self.nin] + self.hidden_sizes + [self.nout]\n        for h0, h1 in zip(hs, hs[1:]):\n            self.net.extend([\n                MaskedLinear(h0, h1),\n                nn.ReLU(),\n            ])\n        self.net.pop()  # pop the last ReLU for the output layer\n        self.net = nn.Sequential(*self.net).to(device)\n\n        self.m = {}\n        self.create_mask()  # builds the initial self.m connectivity\n\n    def create_mask(self):\n        L = len(self.hidden_sizes)\n\n        # sample uniform distribution the order of the inputs and the connectivity of all neurons\n        self.m[-1] = np.arange(self.nin)\n        for l in range(L):\n            self.m[l] = np.random.randint(self.m[l - 1].min(), self.nin - 1, size=self.hidden_sizes[l])\n\n        # construct the mask matrices\n        masks = [self.m[l - 1][:, None] &lt;= self.m[l][None, :] for l in range(L)]\n        masks.append(self.m[L - 1][:, None] &lt; self.m[-1][None, :])\n\n        # set the masks in all MaskedLinear layers\n        layers = [l for l in self.net.modules() if isinstance(l, MaskedLinear)]\n        for l, m in zip(layers, masks):\n            l.set_mask(m)\n\n    def nll(self, x):\n        x = x.view(-1, 784).to(self.device) # Flatten image\n        logits = self.net(x)\n        return F.binary_cross_entropy_with_logits(logits, x)\n\n    def sample(self, n):\n        samples = torch.zeros(n, 784).to(self.device)\n        with torch.no_grad():\n            for i in range(784):\n                logits = self.net(samples)[:, i]\n                probs = torch.sigmoid(logits)\n                samples[:, i] = torch.bernoulli(probs)\n            samples = samples.view(n, 1, 28, 28)\n        return samples.cpu()\n\n\ntrain_args = {'epochs': 20, 'lr': 0.01}\ndevice = 'cuda'\nmodel = MADE(device)\n\n\ntrain_epochs(model, train_loader, test_loader, train_args)\n\n\n\n\nEpoch 0 Test Loss: 0.2613 bits/dim\nEpoch 5 Test Loss: 0.2191 bits/dim\nEpoch 10 Test Loss: 0.2124 bits/dim\nEpoch 15 Test Loss: 0.2096 bits/dim\nTest Loss 0.14515839517116547"
  },
  {
    "objectID": "posts/2020-05-20-autoregressive-generative-models.html#pixelcnn",
    "href": "posts/2020-05-20-autoregressive-generative-models.html#pixelcnn",
    "title": "Autoregressive Generative Models",
    "section": "PixelCNN",
    "text": "PixelCNN\nPixelCNN is a deep autoregressive generative model for images. Let’s consider an image of size \\(n×n\\), each pixel in image is a data point \\(\\bf x=\\big\\{{\\bf x_1,…,x_{n^2}}\\big\\}\\). The model starts generating pixels from the top left corner, from left to right and top to bottom (raster scanning).\n\nEach pixel \\(\\bf{x}_i\\) is in turn jointly determined by three values,one for each of the color channels Red, Green and Blue (RGB). Each of the colors is thus conditioned on the other channels as well as on all the previously generated pixels. \\[\\mathbf p( \\mathbf x_i| \\mathbf x_{&lt;i}) = \\mathbf p(\\mathbf x_{i,R}|\\mathbf x_{&lt;i}). \\mathbf p(x_{i,G}|\\mathbf x_{&lt;i},\\mathbf x_{i,R}). \\mathbf p(x_{i,B}|\\mathbf x_{&lt;i},\\mathbf x_{i,R},\\mathbf x_{i,G})\\]\n\n\n\n\n\n\nNote\n\n\n\nAlong with PixelCNN, the paper authors also proposed PixelRNN with the same analogy as PixelCNN. However, PixelRNN with sequential dependency between LSTM states is very expensive for the computation. So this method will not be detailed in this blog. Check the paper if you are interested in it.\n\n\n\nMasked spatial convolution\nThe masked use the convolution filter to slide over image which multiplies each element and sums them together to produce a single response. However, we cannot use this filter because a generated pixel should not know the intensities of future pixel values. To counter this issue, we use a mask on top of the filter to only choose prior pixels and zeroing the future pixels to negate them from calculation.\n\n\n\nBlind spot\nPixelCNN masking has one problem: blind spot in receptive field because the capturing of receptive field by a CNN proceed in a triangular fashion.\n\nIn order to address the blind spot, the authors use two filters (horizontal and vertical stacks) in conjunction to allow for capturing the whole receptive ﬁeld. - Vertical stack: conditions on all the pixels in the rows above the current pixel. It doesn’t have any masking, allow the receptive field to grow in a rectangular fashion without any blind spot - Horizontal stack: conditions on the current row and takes as input the output of previous layer as well as of the vertical stack.\n\n\n\nGated PixelCNN\nThe PixelCNN only takes into consideration the neighborhood region and the depth of the convolution layers to make its predictions. To improve the performance of PixelCNN, the authors replaced the rectified linear units between the masked convolutions with the following gated activation function in order to model more complex interactions: \\[\\mathbf{y} = \\tanh (\\mathbf W_{k,f} \\ast \\mathbf{x}) \\odot \\sigma (\\mathbf W_{k,g} \\ast \\mathbf{x})\\]\nwhere:\n\\(*\\) is the convolutional operator.\n\\(\\odot\\) is the element-wise product.\n\\(\\sigma\\) is the sigmoid non-linearity\n\\(k\\) is the number of the layer\n\\(tanh(W_{k,f} \\ast \\mathbf x)\\) is a classical convolution with tanh activation function.\n\\(\\sigma(W_{k,g} \\ast \\mathbf x)\\) are the gate values (0 = gate closed, 1 = gate open).\n\\(W_{k,f}\\) and \\(W_{k,g}\\) are learned weights.\n\\(f, g\\) are the different feature maps\nA gated block is represented in Figure 6. There are 2 things to notice here: 1. the vertical stack contributes to the horizontal stack with the \\(1\\times1\\) convolution while vertical stack should not access any information horizontal stack has - otherwise it will have access to pixels it shouldn’t see. However, the vertical stack can be vertically connected as it predicts pixel following those in the vertical stack. 2. The convolutions with \\(W_f\\) and \\(W_g\\) are not combined into a single operation (which is essentially the masked convolution) to increase parallelization. The parallelization splits the \\(2p\\) features maps into two groups of \\(p\\)\n.\n\n\nConditional PixelCNN\nSometimes we want to integrate some high-level information before feed the network, for example provising an image to the network with the associated classes in CIFAR datasets. During training we feed image as well as class to our network to make sure network would learn to incorporate that information as well. During inference we can specify what class our output image should belong to.\nFor a conditional PixelCNN, we represent a provided high-level image description as a latent vector \\(\\mathbf h\\), wherein the purpose of the latent vector is to model the conditional distribution \\(p(\\mathbf{x} \\vert \\mathbf{h})\\) such that we get a probability as to if the images suites this description. The conditional PixelCNN models based on the following distribution: \\[p(\\mathbf{x} \\vert \\mathbf{h}) = \\prod_{i=1}^{n^2} p(x_i \\vert x_1, \\cdots, x_{i-1}, \\mathbf{h})\\]\nAdd terms \\(\\mathbf h\\) before the non-linearities:\n\\[ \\mathbf{y} = \\tanh (W_{k,f} \\ast \\mathbf{x}  {+ V_{k,f}^\\top \\mathbf{h}} ) \\odot \\sigma (W_{k,g} \\ast \\mathbf{x} {+ V_{k,g}^\\top \\mathbf{h} })\\]\n\nIf the latent vector \\(\\mathbf h\\) is a one-hot encoding vector that provides the class labels, which is equivalent to the adding a class dependent bias at every layer. So, the conditioning is dependent on “what should the image contain” rather than the location of contents in the image.\nTo add the location dependency to the model, we use a transposed convolution to map \\(\\mathbf h\\) to a spatial representation \\(s=deconv(\\mathbf h)\\) to produce the output \\(\\mathbf s\\) of the same shape as the image:\n\n\\[\\mathbf{y} = \\tanh (W_{k,f} \\ast \\mathbf{x}  {+ V_{k,f} \\ast \\mathbf{s}} ) \\odot \\sigma (W_{k,g} \\ast \\mathbf{x} {+ V_{k,g} \\ast \\mathbf{s} })\\]\n\n\nImplementation\n\nThe Pytorch code implementation of Gated PixelCNN is borrowed from this repo.\n\n\nPixelCNN with blind spot\n\nclass MaskConv2d(nn.Conv2d):\n    def __init__(self, mask_type, *args, **kwargs):\n        assert mask_type == 'A' or mask_type == 'B'\n        super().__init__(*args, **kwargs)\n        self.register_buffer('mask', torch.zeros_like(self.weight))\n        self.create_mask(mask_type)\n\n    def forward(self, input):\n        return F.conv2d(input, self.weight * self.mask, self.bias, self.stride,\n                        self.padding, self.dilation, self.groups)\n\n    def create_mask(self, mask_type):\n        k = self.kernel_size[0]\n        self.mask[:, :, :k // 2] = 1\n        self.mask[:, :, k // 2, :k // 2] = 1\n        if mask_type == 'B':\n            self.mask[:, :, k // 2, k // 2] = 1\n\n\nclass PixelCNN(nn.Module):\n    def __init__(self, device):\n        super().__init__()\n        model = [MaskConv2d('A', 1, 64, 7, padding=3), nn.ReLU()]\n        for _ in range(3):\n            model.extend([MaskConv2d('B', 64, 64, 7, padding=3), nn.ReLU()])\n        model.append(MaskConv2d('B', 64, 1, 7,padding=3))\n        self.net = nn.Sequential(*model).to(device)\n        self.device = device\n\n    def nll(self, x):\n        x = x.to(self.device)\n        logits = self.net(x)\n        return F.binary_cross_entropy_with_logits(logits, x)\n\n    def sample(self, n):\n        samples = torch.zeros(n, 1, 28, 28).to(self.device)\n        with torch.no_grad():\n            for r in range(28):\n                for c in range(28):\n                    logits = self.net(samples)[:, :, r, c]\n                    probs = torch.sigmoid(logits)\n                    samples[:, :, r, c] = torch.bernoulli(probs)\n        return samples.cpu()\n\n\ntrain_args = {'epochs': 21, 'lr': 0.0002}\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = PixelCNN(device)\n\n\ntrain_epochs(model, train_loader, test_loader, train_args)\n\n\n\n\nEpoch 0 Test Loss: 0.1629 bits/dim\nEpoch 10 Test Loss: 0.1217 bits/dim\nEpoch 20 Test Loss: 0.1177 bits/dim\nTest Loss 0.08157001435756683\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPixelCNN without blind spot\n\nclass HoriVertStackConv2d(nn.Module):\n    def __init__(self, mask_type, in_channels, out_channels, k=3, padding=1, \n                 gated=False, residual_horizontal=False):\n        super().__init__()\n        gm = 2 if gated else 1\n        self.gated = gated\n        self.out_channels = out_channels\n        \n        self.vertical = nn.Conv2d(in_channels, gm * out_channels, kernel_size=k,\n                                  padding=padding, bias=False)\n        self.horizontal = nn.Conv2d(in_channels, gm * out_channels, kernel_size=(1, k),\n                                    padding=(0, padding), bias=False)\n        self.vtohori = nn.Conv2d(gm * out_channels, gm * out_channels, kernel_size=1, bias=False)\n        \n        self.horizontal_output = nn.Conv2d(in_channels , out_channels, 1)\n        self.residual_horizontal = residual_horizontal\n            \n        \n        self.register_buffer('vmask', self.vertical.weight.data.clone())\n        self.register_buffer('hmask', self.horizontal.weight.data.clone())\n\n        self.vmask.fill_(1)\n        self.hmask.fill_(1)\n\n        # zero the bottom half rows of the vmask\n        self.vmask[:, :, k // 2 + 1:, :] = 0\n\n        # zero the right half of the hmask\n        self.hmask[:, :, :, k // 2 + 1:] = 0\n        if mask_type == 'A':\n            self.hmask[:, :, :, k // 2] = 0\n    \n    def _gated(self, x):\n        return torch.tanh(x[:, :self.out_channels]) * torch.sigmoid(x[:, self.out_channels:])\n    \n    def down_shift(self, x):\n        x = x[:, :, :-1, :]\n        pad = nn.ZeroPad2d((0, 0, 1, 0))\n        return pad(x)\n\n    def forward(self, x):\n        vx, h = x.chunk(2, dim=1)\n\n        self.vertical.weight.data *= self.vmask\n        self.horizontal.weight.data *= self.hmask\n            \n        vx = self.vertical(vx)\n        hx = self.horizontal(h)\n        # Allow horizontal stack to see information from vertical stack\n        hx = hx + self.vtohori(self.down_shift(vx))\n        \n        if self.gated:\n            vx = self._gated(vx)\n            hx = self._gated(hx)\n        \n        if self.residual_horizontal:\n            h = self.horizontal_output(h)\n            hx = h + hx\n        \n        return torch.cat((vx, hx), dim=1)\n\n# PixelCNN using horizontal and vertical stacks to fix blind-spot\nclass GatedHoriVertStackPixelCNN(nn.Module):\n    name = 'HoriVertStackPixelCNN'\n    def __init__(self, n_layers, device):\n        super().__init__()\n        model = [HoriVertStackConv2d('A', 1, 64, 7, padding=3), nn.ReLU()]\n        for _ in range(n_layers - 1):\n            model.extend([HoriVertStackConv2d('B', 64, 64, 7, padding=3), \n                          nn.ReLU()])\n        model.append(HoriVertStackConv2d('B', 64, 1, 7,padding=3))\n        self.net = nn.Sequential(*model).to(device)\n        self.device = device\n        \n    def forward(self, x):\n        x = x.to(self.device)\n        return self.net(torch.cat((x, x), dim=1)).chunk(2, dim=1)[1]\n    \n    def nll(self, x):\n        x = x.to(self.device)\n        logits = self(x)\n        return F.binary_cross_entropy_with_logits(logits, x)\n\n    def sample(self, n):\n        samples = torch.zeros(n, 1, 28, 28) #here we sample only one channel instead of three for simplicity\n        with torch.no_grad():\n            for r in range(28):\n                for c in range(28):\n                    logits = self(samples)[:, :, r, c]\n                    probs = torch.sigmoid(logits)\n                    samples[:, :, r, c] = torch.bernoulli(probs)\n        return samples.cpu()\n\n\ntrain_args = {'epochs': 21, 'lr': 0.0002}\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = GatedHoriVertStackPixelCNN(4, device)\n\n\ntrain_epochs(model, train_loader, test_loader, train_args)\n\n\n\n\nEpoch 0 Test Loss: 0.3259 bits/dim\nEpoch 10 Test Loss: 0.3033 bits/dim\nEpoch 20 Test Loss: 0.3007 bits/dim\nTest Loss 0.20846164226531982"
  },
  {
    "objectID": "posts/2020-05-20-autoregressive-generative-models.html#pixelcnn-1",
    "href": "posts/2020-05-20-autoregressive-generative-models.html#pixelcnn-1",
    "title": "Autoregressive Generative Models",
    "section": "PixelCNN++",
    "text": "PixelCNN++\nPixelCNN typically consists of a stack of masked convolutional layers that takes an \\(N \\times N \\times 3\\) image as input and produces \\(N \\times N \\times 3 \\times 256\\) (probability of pixel intensity) predictions as output. The softmax layer in PixelCNN to compute the conditional distribution of a sub-pixel is a full of 256-softmax. There are 2 issues with this approache: First, it is very costly in terms of memory. Second, it is missing the continuity property data. It means that the model does not know that a value of 128 is close to a value of 127 or 129 but no different than 255 for instance.\nTo address this issue, the paper authors came up with the new model including following modifications compared to PixelCNN:\nDiscretized logistic mixture likelihood\n\nFor each sub-pixel, generate a continuous distribution \\(ν\\) representing the colour intensity instead of discrete distribution. For example, ν could be a mixture of logistic distribution parameterized by \\(\\mu,s\\) and the mixture weights \\(\\pi\\). \\[\n\\nu \\sim \\sum_{i=1}^K \\pi_i logistic(\\mu_i, s_i)\n\\]\nWe then convert this intensity to a mass function by assigning regions of it to the 0 to 255 pixels:\n\n\\[\n\\ P(x|\\mu,s) =\n    \\begin{cases}\n        \\sigma(\\frac{x-\\mu+0.5}{s}) & \\text{for } x = 0 \\\\\n        \\sigma(\\frac{x-\\mu+0.5}{s}) - \\sigma(\\frac{x-\\mu-0.5}{s})\n            & \\text{for } 0 &lt; x &lt; 255 \\\\\n        1 - \\sigma(\\frac{x-\\mu-0.5}{s}) & \\text{for } x = 255\n    \\end{cases}\n\\\n\\] where \\(\\sigma\\) is the sigmoid function.\nConditioning on whole pixels\nPixelCNN factorizes the model over the 3 sub pixels according to the color(RGB) which however, complicates the model. The dependency between color channels of a pixel is relatively simple and doesn’t require a deep model to train. Therefore, it is better to condition on whole pixels instead of separate colors and then output joint distributions over all 3 channels of the predicted pixel. - We first predict the red channel using a discretized mixture of logistic - Next, we predict the green channel using a predictive distribution of the same form. Here we allow the means of the mixture components to linearly depend on the value of the red sub-pixel.\n- Finally, we model the blue channel in the same way, where we again only allow linear dependency on the red and green channels.\nDownsampling versus dilated convolution\nThe PixelCNN uses convolutions with small receptive field which is good at capturing local dependencies, but not necessarily at modeling long range structure. To overcome this, we downsample the layers by using convolutions of stride 2. Downsampling reduces input size and thus improves relative size of receptive field which leads to some loss of information but it can be compensated by adding extra short-cut connections.\nAdding short-cut connections\nThe idea is pretty the same as Unet model by introducing additional short-cut connections into the model to recover the losed informations from lower layers to higher layers of the model.\nRegularization using dropout The PixelCNN model is powerful enough to overfit on training data, leads to lower perceptual quality of images while generating data. One effective way of regularizing neural networks is dropout (Srivas-tava et al., 2014)."
  },
  {
    "objectID": "posts/2020-05-20-autoregressive-generative-models.html#wavenet",
    "href": "posts/2020-05-20-autoregressive-generative-models.html#wavenet",
    "title": "Autoregressive Generative Models",
    "section": "WaveNet",
    "text": "WaveNet\nAll the examples we’ve seen so far are images generation which is 2D data. The technique of autoregressive generative model can also be applied to 1D data such as audio or text. We will take a look on similar approaches which are based on PixelCNN idea in generating raw audio waveforms, which are signals with very high temporal resolution, at least 16,000 samples per second. One of the most popular applications of this method is text-to-speech where we have to generate a audio from text input.\nSimilarly to PixelCNNs explained above, the joint probability of a waveform \\(\\bf x=\\{x_1,...,x_T\\}\\) is factorised as a product of conditional probabilities of each audio sample \\(\\bf x_t\\) which is conditioned on the samples at all previous timesteps. The conditional probability distribution is also modelled by a stack of convolutional layers.\nTo take care of autoregressive property, the wavenet model is using casual convolution. At training time, the conditional predictions for all timesteps can be made in parallel because all timesteps of ground truth \\(\\bf x\\) are known. When generating with the model, the predictions are sequential: after each sample is predicted, it is fed back into the network to predict the next sample. However, one of the problems of causal convolutions is that they require many layers, or large filters to increase the receptive field. To mitigate this problem, Wavenet used dilated convolution\n\n\nDilated convolution\nA dilated convolution (also called à trous, or convolution with holes) is a convolution where the filter is applied over an area larger than its length by skipping input values with a certain step. This technique is broadly used to increase the receptive field by orders of magnitude, without greatly increasing computational cost. In computer vision, we’ve seen it in semantic segmentation model.\nA dilated convolution effectively allows the network to operate ona coarser scale than with a normal convolution. This is similar to pooling or strided convolutions, but here the output has the same size as the input.\n\nThe intuition is that exponentially increasing the dilation factor results in exponential receptive field growth with depth. So stack these dilated convolutions blocks further increases the model capacity and the receptive field size of the model.\n\nExcept for the dilated convolution, the Wavenet model is very similar to PixelCNN such as gated activation units technique.\n\n\nSoftmax distribution\nThe raw audio output is stored as a sequence of 16-bit scalar values (one per time step), thus the softmax output is 2^16=65,536 probabilities per timestep. WaveNet applies a μ-law companding transformation to the data and then quantize it to 256 possible values:\n\\[f(\\bf{x}_t) = \\text{sign} (x_t) \\frac{\\ln(1 + \\mu |x_t|)}{\\ln(1 + \\mu)}\\] where \\(x_t \\in (−1,1), \\mu = 255\\)\n\n\nFast WaveNet Generation\nWith the implemenentation of Wavenet in Figure 9, since the computation forms a binary tree, the overall computation time for a single output is \\(O(2^L)\\), where \\(L\\) is the number of layers in the network. When \\(L\\) is large, this is extremely undesirable. This paper proposed approach removes redundant convolution operations by caching previous calculations instead of recomputing many variables that have already been computed for previous samples, thereby reducing the complexity to \\(O(L)\\) time.\nThe below figure shows the model with 2 convolutional and 2 transposed convolutional layers with strid of 2, wherein blue dots indicate the cached states and orange bots are computed in the current step.\n\n\n\nImplementation\n\nThe Pytorch code implementation of WaveNet is borrowed from this repo.\n\n\ndef append_location(x, device):\n    \"\"\"\n        Pixel Location Appended as Features\n    \"\"\"\n    idxs = torch.arange(28).float() / 27  # Scale to [0, 1]\n    locs = torch.stack(torch.meshgrid(idxs, idxs), dim=-1)\n    locs = locs.permute(2, 0, 1).contiguous().unsqueeze(0).repeat(x.shape[0], 1, 1, 1)\n    locs = locs.to(device)\n\n    x = torch.cat((x, locs), dim=1)\n    return x\n\nclass DilatedCausalConv1d(nn.Module):\n    \"\"\"Dilated Causal Convolution for WaveNet\"\"\"\n    def __init__(self, mask_type, in_channels, out_channels, dilation=1):\n        super(DilatedCausalConv1d, self).__init__()\n        self.conv = nn.Conv1d(in_channels, out_channels,\n                              kernel_size=2, dilation=dilation, padding=0)\n        self.dilation = dilation\n        self.mask_type = mask_type\n        assert mask_type in ['A', 'B']\n\n    def forward(self, x):\n        if self.mask_type == 'A':\n            return self.conv(F.pad(x, [2, 0]))[:, :, :-1]\n        else:\n            return self.conv(F.pad(x, [self.dilation, 0]))\n\n\nclass ResidualBlock(nn.Module):\n    \"\"\" Gated Unit Activation\"\"\"\n    def __init__(self, res_channels, dilation):\n        super(ResidualBlock, self).__init__()\n\n        self.dilated = DilatedCausalConv1d('B', res_channels, 2 * res_channels, dilation=dilation)\n        self.conv_res = nn.Conv1d(res_channels, res_channels, 1)\n\n    def forward(self, x):\n        output = self.dilated(x)\n\n        # PixelCNN gate\n        o1, o2 = output.chunk(2, dim=1)\n        output = torch.tanh(o1) * torch.sigmoid(o2)\n        output = x + self.conv_res(output) # Residual network\n\n        return output\n\n\nclass WaveNet(nn.Module):\n    def __init__(self, device, append_loc=True):\n        super(WaveNet, self).__init__()\n\n        in_channels = 3 if append_loc else 1\n        out_channels = 1\n        res_channels = 32\n        layer_size = 5 # Largest dilation is 16\n        stack_size = 2\n\n        self.causal = DilatedCausalConv1d('A', in_channels, res_channels, dilation=1)\n        self.res_stack = nn.Sequential(*sum([[ResidualBlock(res_channels, 2 ** i)\n                                         for i in range(layer_size)] for _ in range(stack_size)], []))\n        self.out_conv = nn.Conv1d(res_channels, out_channels, 1)\n        self.append_loc = append_loc\n        self.device = device\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        if self.append_loc:\n            x = append_location(x, self.device)\n        output = x.view(batch_size, -1, 784)\n        output = self.causal(output)\n        output = self.res_stack(output)\n        output = self.out_conv(output)\n        return output.view(batch_size, 1, 28, 28)\n\n    def nll(self, x):\n        x = x.to(self.device)\n        logits = self(x)\n        return F.binary_cross_entropy_with_logits(logits, x)\n\n    def sample(self, n):\n        with torch.no_grad():\n            samples = torch.zeros(n, 1, 28, 28).to(self.device)\n            for r in range(28):\n                for c in range(28):\n                    logits = self(samples)[:, :, r, c]\n                    probs = torch.sigmoid(logits)\n                    samples[:, :, r, c] = torch.bernoulli(probs)\n        return samples.cpu()\n\n\ntrain_args = {'epochs': 21, 'lr': 0.0002}\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = WaveNet(device).to(device)\n\n\ntrain_epochs(model, train_loader, test_loader, train_args)\n\n\n\n\nEpoch 0 Test Loss: 0.1659 bits/dim\nEpoch 10 Test Loss: 0.1265 bits/dim\nEpoch 20 Test Loss: 0.1240 bits/dim\nTest Loss 0.08596379309892654"
  },
  {
    "objectID": "posts/2020-05-20-autoregressive-generative-models.html#self-attention-autoregressive-model",
    "href": "posts/2020-05-20-autoregressive-generative-models.html#self-attention-autoregressive-model",
    "title": "Autoregressive Generative Models",
    "section": "Self-Attention Autoregressive Model",
    "text": "Self-Attention Autoregressive Model\nRecently, we’ve witnessed a tremendous application of Transformer models in NLP. The multi-head self-attention idea behind all these models is approaching many other fields. Explaining in details how it works is out of scope of this post. If you are curious about it, I recommend to take a look on this wonderful blog.\nThe idea of using Transformer on autoregressive generative model is similar with RNN generative model. However, the Transformer is the transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution.\n\nImplementation\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_hid, n_position=784):\n        super(PositionalEncoding, self).__init__()\n\n        # Not a parameter\n        self.register_buffer('pos_table', self._get_sinusoid_encoding_table(n_position, d_hid))\n\n    def _get_sinusoid_encoding_table(self, n_position, d_hid):\n        ''' Sinusoid position encoding table '''\n\n        def get_position_angle_vec(position):\n            return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n\n        sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n\n        return torch.FloatTensor(sinusoid_table).unsqueeze(0) * 0.1\n\n    def forward(self, x):\n        return x + self.pos_table[:, :x.size(1)].clone().detach()\n\nclass ScaledDotProductAttention(nn.Module):\n    ''' Scaled Dot-Product Attention '''\n\n    def __init__(self, temperature, attn_dropout=0.1):\n        super().__init__()\n        self.temperature = temperature\n        self.dropout = nn.Dropout(attn_dropout)\n\n    def forward(self, q, k, v, mask=None):\n\n        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n\n        if mask is not None:\n            attn = attn.masked_fill(mask == 0, -1e9)\n\n        attn = self.dropout(F.softmax(attn, dim=-1))\n        output = torch.matmul(attn, v)\n\n        return output, attn\n\nclass PositionwiseFeedForward(nn.Module):\n    ''' A two-feed-forward-layer module '''\n\n    def __init__(self, d_in, d_hid, dropout=0.1):\n        super().__init__()\n        self.w_1 = nn.Linear(d_in, d_hid) # position-wise\n        self.w_2 = nn.Linear(d_hid, d_in) # position-wise\n        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n\n        residual = x\n        x = self.layer_norm(x)\n\n        x = self.w_2(F.relu(self.w_1(x)))\n        x = self.dropout(x)\n        x += residual\n\n        return x\n\n\nclass MultiHeadAttention(nn.Module):\n    ''' Multi-Head Attention module '''\n\n    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n        super().__init__()\n\n        self.n_head = n_head\n        self.d_k = d_k\n        self.d_v = d_v\n\n        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n\n        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)\n\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n\n    def forward(self, q, k, v, mask=None):\n        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n\n        residual = q\n        q = self.layer_norm(q)\n\n        # Pass through the pre-attention projection: b x lq x (n*dv)\n        # Separate different heads: b x lq x n x dv\n        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n\n        # Transpose for attention dot product: b x n x lq x dv\n        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n\n        if mask is not None:\n            mask = mask.unsqueeze(0).unsqueeze(0)  # For head axis broadcasting.\n\n        q, attn = self.attention(q, k, v, mask=mask)\n\n        # Transpose to move the head dimension back: b x lq x n x dv\n        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n        q = self.dropout(self.fc(q))\n        q += residual\n\n        return q\n\nclass DecoderLayer(nn.Module):\n    ''' Compose with three layers '''\n\n    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n        super(DecoderLayer, self).__init__()\n        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n\n    def forward(self, dec_input, mask=None):\n        dec_output = self.slf_attn(dec_input, dec_input, dec_input, mask=mask)\n        dec_output = self.pos_ffn(dec_output)\n        return dec_output\n\nclass Transformer(nn.Module):\n    ''' A decoder model with self attention mechanism. '''\n\n    def __init__(self, device, mode='none'):\n\n        super().__init__()\n        n_layers = 2\n        self.input_size = 3 if mode == 'pixel_location' else 1\n\n        if mode == 'pos_encoding':\n            self.pos_enc = PositionalEncoding(1, n_position=784)\n        self.fc_in = nn.Linear(self.input_size, 64)\n        self.layer_stack = nn.ModuleList([\n            DecoderLayer(64, 64, 1, 16, 64, dropout=0.1)\n            for _ in range(n_layers)])\n        self.fc_out = nn.Linear(64, 1)\n\n        self.register_buffer('mask', torch.zeros(784, 784))\n        for i in range(784):\n            self.mask[i, :i] = 1\n\n        self.mode = mode\n        self.device = device\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        if self.mode == 'pixel_location':\n            x = append_location(x, self.device)\n            x = x.permute(0, 2, 3, 1).view(batch_size, 784, self.input_size)\n        elif self.mode == 'pos_encoding':\n            x = x.view(batch_size, 784, self.input_size)\n            x = self.pos_enc(x)\n        else:\n            x = x.view(batch_size, 784, self.input_size)\n        x = torch.cat((torch.zeros(batch_size, 1, self.input_size).to(self.device), x[:, :-1]), dim=1)\n        # -- Forward\n        x = F.relu(self.fc_in(x))\n        for i, dec_layer in enumerate(self.layer_stack):\n            x = dec_layer(x, mask=self.mask)\n        x = self.fc_out(x)\n        x = x.view(batch_size, 1, 28, 28)\n        return x\n\n    def nll(self, x):\n        x = x.to(self.device)\n        logits = self(x)\n        return F.binary_cross_entropy_with_logits(logits, x)\n\n    def sample(self, n):\n        samples = torch.zeros(n, 1, 28, 28).to(self.device)\n        with torch.no_grad():\n            for r in range(28):\n                for c in range(28):\n                    logits = self(samples)[:, :, r, c]\n                    probs = torch.sigmoid(logits)\n                    samples[:, :, r, c] = torch.bernoulli(probs)\n        return samples.cpu()\n\n\ntrain_args = {'epochs': 21, 'lr': 0.0002}\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = Transformer(device, mode='pixel_location').to(device)\n\n\ntrain_epochs(model, train_loader, test_loader, train_args)\n\n\n\n\nEpoch 0 Test Loss: 0.2513 bits/dim\nEpoch 10 Test Loss: 0.1693 bits/dim\nEpoch 20 Test Loss: 0.1486 bits/dim\nTest Loss 0.10297279804944992"
  },
  {
    "objectID": "posts/2020-05-20-autoregressive-generative-models.html#pixelsnail",
    "href": "posts/2020-05-20-autoregressive-generative-models.html#pixelsnail",
    "title": "Autoregressive Generative Models",
    "section": "PixelSNAIL",
    "text": "PixelSNAIL\nWe’ve already covered a lot til now (Yeah! I know…). The last model we will review is PixelSNAIL which adopt masked self-attention approaches inspired by SNAIL.\nThe key idea behind PixelSNAIL is to introduce attention blocks, in a style similar to Self Attention, into neural autoregressive modelling.\nThe PixelSNAIL model is composed of two blocks:\n\nResidual block: This block is as same as gated unit block in PixelCNN that we’ve seen so far.\nAttention block: This block performs a single key-value lookup. It projects the input to a lower dimensionality to produce the keys and values and then uses softmax-attention like in Transformer model.\n\n\nThe way how PixelSNAIL integrated self-attention blocks is quite interesting because it allows to model long-range depenencies between pixels in image, equip all conditionals with the ability to refer to all of their available context. Moreover, each conditional can access any pixels in its context through the attention operator, easy information access of remote pixels improves modeling of long-range statistics.\n\n\nFlexible ordering\nWe’ve seen in PixelCNN or other architectures, the pixel ordering is raster scanning where along each row left pixels come before right pixels and top rows come before bottom rows. The raster scan order-ing only has a small number neighboring pixels available in the conditioning context \\(\\bf x_1, . . . , x_{i−1}\\): only to the left and above and most of the context is wasted on regions that might have little correlation with the current pixel like the far top-right corner. Since we can access any pixels in its context through attention operator, PixelSNAIL allows zigzag ordering."
  },
  {
    "objectID": "posts/2020-06-07-tabular_feature_engineering_tricks.html",
    "href": "posts/2020-06-07-tabular_feature_engineering_tricks.html",
    "title": "Tabular feature engineering tricks",
    "section": "",
    "text": "One of the most important challenge when working with tabular dataset before feeding into the machine learning model is preprocessing data because the quality of data and the useful information that can be derived from it directly affects the ability of model to learn. In the real world application, the raw dataset is kind of messy which needs some skills to clean it.\nThis blog will cover some tricks that I found quite useful when dealing with messy data."
  },
  {
    "objectID": "posts/2020-06-07-tabular_feature_engineering_tricks.html#pandas-tricks",
    "href": "posts/2020-06-07-tabular_feature_engineering_tricks.html#pandas-tricks",
    "title": "Tabular feature engineering tricks",
    "section": "Pandas tricks",
    "text": "Pandas tricks\nMost of the time when doing tabular dataset we will usually import data into dataframe of pandas. Knowing how to deal with pandas will save us a lot of time of computation with better code readability and versatility.\n\nPandas Profile for data exploration\nPandas profiling is an open source Python module with which we can quickly do an exploratory data analysis with just a few lines of code.\n\nfrom pandas_profiling import ProfileReport\n\nprofile = ProfileReport(dataframe)\n\n\n\n\nConfigure Options & Settings\nWe usually have to work with a dataframe containing many columns which will collape when reading it. If we want to read all columns of dataframe, we can configure its options at interpreter startup.\n\npd.options.display.max_columns = 500\n\n#or \n\npd.set_option('display.max_columns', 500)\n\n\n\nSplit a column into multiple columns\nSome dataframe columns contain concatenation of data which should be splitted into multiple columns\n\ndf = pd.DataFrame({'Name_Age': ['Smith_32', 'Nadal', \n                           'Federer_36']})\n\n\ndf\n\n\n\n\n\n\n\n\nName_Age\n\n\n\n\n0\nSmith_32\n\n\n1\nNadal\n\n\n2\nFederer_36\n\n\n\n\n\n\n\n\ndf['Name_Age'].str.split('_', expand=True)\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nSmith\n32\n\n\n1\nNadal\nNone\n\n\n2\nFederer\n36\n\n\n\n\n\n\n\nIf we want to take only first column of our splits:\n\ndf['Name_Age'].str.split('_').str[1]\n\n0     32\n1    NaN\n2     36\nName: Name_Age, dtype: object\n\n\n\n\nExtract dummy variables from string columns\n\ndf = pd.Series(['Sector 1;Sector 2', 'Sector 1;Sector 3', np.nan, 'Sector 2;Sector 4'], dtype=str)\n\n\ndf\n\n0    Sector 1;Sector 2\n1    Sector 1;Sector 3\n2                  NaN\n3    Sector 2;Sector 4\ndtype: object\n\n\n\ndf.str.get_dummies(';')\n\n\n\n\n\n\n\n\nSector 1\nSector 2\nSector 3\nSector 4\n\n\n\n\n0\n1\n1\n0\n0\n\n\n1\n1\n0\n1\n0\n\n\n2\n0\n0\n0\n0\n\n\n3\n0\n1\n0\n1\n\n\n\n\n\n\n\n\n\nRegular expression with text\n\ndf = pd.DataFrame({'Location' : [\n    'Washington, D.C. 20003',\n    'Brooklyn, NY 11211-1755',\n    'Omaha, NE 68154',\n    'Pittsburgh, PA 15211'\n    ]})\n\n\ndf\n\n\n\n\n\n\n\n\nLocation\n\n\n\n\n0\nWashington, D.C. 20003\n\n\n1\nBrooklyn, NY 11211-1755\n\n\n2\nOmaha, NE 68154\n\n\n3\nPittsburgh, PA 15211\n\n\n\n\n\n\n\nIf we want to separate out the three city/state/ZIP components neatly into DataFrame fields, we should pass regex extraction into .str.extract:\n\nregex = (r'(?P&lt;city&gt;[A-Za-z ]+), '      # One or more letters\n         r'(?P&lt;state&gt;[A-Z]{2}) '        # 2 capital letters\n         r'(?P&lt;zip&gt;\\d{5}(?:-\\d{4})?)')  # Optional 4-digit extension\n\n\ndf['Location'].str.replace('.', '').str.extract(regex)\n\n\n\n\n\n\n\n\ncity\nstate\nzip\n\n\n\n\n0\nWashington\nDC\n20003\n\n\n1\nBrooklyn\nNY\n11211-1755\n\n\n2\nOmaha\nNE\n68154\n\n\n3\nPittsburgh\nPA\n15211\n\n\n\n\n\n\n\n\nNote: .str is called accessor for string (object) data. It maps to the class StringMethods which contains a lot of methods like cat, split, rsplit, replace, extract…\n\n\n\nEnhance performance with Cython\nWe usually use the lambda function to apply for each row in dataframe. Sometimes, it is not efficient in term of computation time. Actually we can boost the performance by using Cython extention.\n\ndf = pd.DataFrame({'a': np.random.randn(1000),\n                   'b': np.random.randn(1000),\n                   'N': np.random.randint(100, 1000, (1000)),\n                   'x': 'x'})\n\n\ndef f(x):\n    return x * (x - 1)\ndef integrate_f(a, b, N):\n    s = 0\n    dx = (b - a) / N\n    for i in range(N):\n        s += f(a + i * dx)\n    return s * dx\n\n\n%timeit df.apply(lambda x: integrate_f(x['a'], x['b'], x['N']), axis=1)\n\n167 ms ± 1.58 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nNow we will pass the lambda function into Cython extension.\n\n%load_ext Cython\n\nThe Cython extension is already loaded. To reload it, use:\n  %reload_ext Cython\n\n\n\n%%cython\ndef f(x):\n    return x * (x - 1)\ndef integrate_f(a, b, N):\n    s = 0\n    dx = (b - a) / N\n    for i in range(N):\n        s += f(a + i * dx)\n    return s * dx\n\n\n%timeit df.apply(lambda x: integrate_f(x['a'], x['b'], x['N']), axis=1)\n\n120 ms ± 777 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nBy simply putting cython extension before the lambda function, we reduce a significant amount of computation time\n\n\nGroupby\nGroupby is one of the powerful methods which allows us to split the data into groups based on some criteria, compute statistic aggregation on each group or do transformation like standalizing the data within a group and many more.\n\ndata = {'Team': ['Riders', 'Riders', 'Devils', 'Devils', 'Kings',\n   'kings', 'Kings', 'Kings', 'Riders', 'Royals', 'Royals', 'Riders'],\n   'Rank': [1, 2, 2, 3, 3,4 ,1 ,1,2 , 4,1,2],\n   'Year': [2014,2015,2014,2015,2014,2015,2016,2017,2016,2014,2015,2017],\n   'Points':[876,789,863,673,741,812,756,788,694,701,804,690]}\n\ndf = pd.DataFrame(data)\n\n\ndf\n\n\n\n\n\n\n\n\nTeam\nRank\nYear\nPoints\n\n\n\n\n0\nRiders\n1\n2014\n876\n\n\n1\nRiders\n2\n2015\n789\n\n\n2\nDevils\n2\n2014\n863\n\n\n3\nDevils\n3\n2015\n673\n\n\n4\nKings\n3\n2014\n741\n\n\n5\nkings\n4\n2015\n812\n\n\n6\nKings\n1\n2016\n756\n\n\n7\nKings\n1\n2017\n788\n\n\n8\nRiders\n2\n2016\n694\n\n\n9\nRoyals\n4\n2014\n701\n\n\n10\nRoyals\n1\n2015\n804\n\n\n11\nRiders\n2\n2017\n690\n\n\n\n\n\n\n\n\ndf.groupby('Year')['Points'].agg(['mean', 'sum', 'min', 'max', 'std', 'var', 'count'])\n\n\n\n\n\n\n\n\nmean\nsum\nmin\nmax\nstd\nvar\ncount\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n2014\n795.25\n3181\n701\n876\n87.439026\n7645.583333\n4\n\n\n2015\n769.50\n3078\n673\n812\n65.035888\n4229.666667\n4\n\n\n2016\n725.00\n1450\n694\n756\n43.840620\n1922.000000\n2\n\n\n2017\n739.00\n1478\n690\n788\n69.296465\n4802.000000\n2\n\n\n\n\n\n\n\nWe can also aggregate using the numpy function like np.size, np.mean, np.max…\n\ndf.groupby('Team').agg(np.size)\n\n\n\n\n\n\n\n\nRank\nYear\nPoints\n\n\nTeam\n\n\n\n\n\n\n\nDevils\n2\n2\n2\n\n\nKings\n3\n3\n3\n\n\nRiders\n4\n4\n4\n\n\nRoyals\n2\n2\n2\n\n\nkings\n1\n1\n1\n\n\n\n\n\n\n\n\ndf.groupby('Team').transform(lambda x: (x - x.mean()) / x.std()*10)\n\n\n\n\n\n\n\n\nRank\nYear\nPoints\n\n\n\n\n0\n-15.000000\n-11.618950\n12.843272\n\n\n1\n5.000000\n-3.872983\n3.020286\n\n\n2\n-7.071068\n-7.071068\n7.071068\n\n\n3\n7.071068\n7.071068\n-7.071068\n\n\n4\n11.547005\n-10.910895\n-8.608621\n\n\n5\nNaN\nNaN\nNaN\n\n\n6\n-5.773503\n2.182179\n-2.360428\n\n\n7\n-5.773503\n8.728716\n10.969049\n\n\n8\n5.000000\n3.872983\n-7.705963\n\n\n9\n7.071068\n-7.071068\n-7.071068\n\n\n10\n-7.071068\n7.071068\n7.071068\n\n\n11\n5.000000\n11.618950\n-8.157595\n\n\n\n\n\n\n\n\ndf.groupby('Team').filter(lambda x: len(x) &gt;= 3)\n\n\n\n\n\n\n\n\nTeam\nRank\nYear\nPoints\n\n\n\n\n0\nRiders\n1\n2014\n876\n\n\n1\nRiders\n2\n2015\n789\n\n\n4\nKings\n3\n2014\n741\n\n\n6\nKings\n1\n2016\n756\n\n\n7\nKings\n1\n2017\n788\n\n\n8\nRiders\n2\n2016\n694\n\n\n11\nRiders\n2\n2017\n690"
  },
  {
    "objectID": "posts/2020-06-07-tabular_feature_engineering_tricks.html#sklearn-pipeline",
    "href": "posts/2020-06-07-tabular_feature_engineering_tricks.html#sklearn-pipeline",
    "title": "Tabular feature engineering tricks",
    "section": "Sklearn pipeline",
    "text": "Sklearn pipeline\nOne of the most beautiful things I love about sklearn is its creation of pipeline. I found it very neat to use, easily for understanding and particularly very helpful for production.\n&gt;Note: Definition of **pipeline** class according to scikit-learn:\nSequentially apply a list of transforms and a final estimator. Intermediate steps of pipeline must implement fit and transform methods and the final estimator only needs to implement fit. \n\npipeline = Pipeline([('preprocessing', ColumnTransformer(\n                                        transformers = [\n                                            ('text', Pipeline([\n                                                ('tfidf', TfidfVectorizer(stop_words=['nan'])),\n                                            ]), TEXT_COLUMNS),\n                                            ('cat', \n                                             FeatureUnion([\n                                                    ('ordinal', OrdinalEncoder()),\n                                                    ('target_encoder', TargetEncoder())\n                                            ]), CAT_COLUMNS),\n                                            ('num', TruncatedSVD(n_components=100) , \n                                            NUM_COLUMNS)\n                                                    ],\n                                            remainder='drop')\n                     ),\n                     ('model', LGBMClassifier()\n                     )\n                    ])\n\npipeline.fit(Xtrain, y_train)\n\nLooking at the pipeline allows to understand right away what we want to do with our data. The example above can be interpreted as the schema below:\n\n\n\nimage.png\n\n\nWe can also pass the whole pipeline into other pipeline like RandomizedSearchCV or GridSearchCV\n\nparams_grid = {\n    'model__colsample_bytree': [0.3, 0.5, 0.7, 0.9],\n    'model__n_estimators' : [2000, 5000, 8000],\n    'model__learning_rate': [0.01, 0.02, 0.05, 0.1, 0.2],\n    'model__max_depth' : [3, 5, 7],\n    'preprocessing__num__n_components' : [100, 50, 70],\n    'preprocessing__text__select__estimator__C' : [1e-2, 1e-1, 1],\n    'preprocessing__text__select__max_features' : [10, 20, 50, None],\n    'preprocessing__text__tfidf__binary' : [False, True],\n    'preprocessing__text__tfidf__ngram_range' : [(1, 1), (1, 2)],\n    'preprocessing__text__tfidf__max_df': [0.2, 0.4, 0.6],\n    'preprocessing__text__tfidf__min_df': [20, 50]\n}\n\n\nsearch = RandomizedSearchCV(estimator=pipeline,\n                            param_distributions=params_grid,\n                            n_iter=100,\n                            n_jobs=1,\n                            cv = 5,\n                            verbose=5,\n                            scoring='roc_auc')\n\nsearch.fit(Xtrain, y_train)"
  },
  {
    "objectID": "posts/2020-06-07-tabular_feature_engineering_tricks.html#reference",
    "href": "posts/2020-06-07-tabular_feature_engineering_tricks.html#reference",
    "title": "Tabular feature engineering tricks",
    "section": "Reference",
    "text": "Reference\n\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/\nhttps://realpython.com/pandas-groupby/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Technical Blog on Machine Learning",
    "section": "",
    "text": "Everything you need to know about Time Series Forecasting\n\n\n\n\n\n\n\ntime-series\n\n\n\n\nTime Series Forecasting\n\n\n\n\n\n\nFeb 16, 2024\n\n\n\n\n\n\n  \n\n\n\n\nQuantization\n\n\n\n\n\n\n\nLLM\n\n\n\n\nQuantization\n\n\n\n\n\n\nDec 7, 2023\n\n\n\n\n\n\n  \n\n\n\n\nFinetuning and inference Zypher7B\n\n\n\n\n\n\n\nLLM\n\n\n\n\nFinetuning and inference Zypher7B\n\n\n\n\n\n\nNov 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nWhat you should know about RAG (Retrieved-Augmented Generation)\n\n\n\n\n\n\n\nLLM\n\n\nRAG\n\n\n\n\nRetrieval augmented generation (RAG)\n\n\n\n\n\n\nNov 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nEverything you need to know about Llama 2\n\n\n\n\n\n\n\nLLM\n\n\nLlama 2\n\n\n\n\nLLaMA 2, the successor of the original LLaMA 1, is a large language model created by Meta.\n\n\n\n\n\n\nAug 21, 2023\n\n\n\n\n\n\n  \n\n\n\n\nTabular feature engineering tricks\n\n\n\n\n\n\n\ntabular\n\n\nfeature-engineering\n\n\n\n\nSome useful tricks when doing feature engineering on tabular dataset\n\n\n\n\n\n\nJun 7, 2020\n\n\n\n\n\n\n  \n\n\n\n\nAutoregressive Generative Models\n\n\n\n\n\n\n\ngenerative\n\n\nautoregressive\n\n\ndeeplearning\n\n\n\n\nA complete overview on Autoregressive Generative Models (MADE, PixelCNN families, WaveNet, Seft-Attention, PixelSNAIL) with implementation code\n\n\n\n\n\n\nMay 20, 2020\n\n\n\n\n\n\n  \n\n\n\n\nKaggle Molecular Competition: Lessons Learned from Finishing in Top 5\n\n\n\n\n\n\n\nkaggle\n\n\ngraph neural network\n\n\n\n\nTricks and lessons I have learned from getting into top 5 of Kaggle Molecular Competition\n\n\n\n\n\n\nOct 17, 2019\n\n\n\n\n\n\nNo matching items"
  }
]