[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jonathan Tu Nguyen",
    "section": "",
    "text": "I’m Jonathan Tu Nguyen, a machine learning engineer.\nI’m a senior machine learning engineer at Voodoo, where I build a bidding system that allows app developers to rapidly monetize their apps by leveraging machine learning and predictive algorithms. Previously, I built a system for extracting relevant information from documents and a speech recognition system that automate the manual process which consists of translating audio calls into text and making business insights from that. My technical stack includes working with state-of-the-art machine learning models in recommendation systems, bandit, and statistical models (winning rate probability or click-through rate), computer vision, NLP and putting them into deployment at scale with modern tech frameworks to serve our very large base of users.\nIn my free time, I travel, read, build side projects and participate Kaggle.\nThis is a place where I’m documenting my learning notes, sharing my ideas, thoughts about machine learning/deep learning.\nDrop me an email at jonathan.tunguyen AT gmail.com if you have any questions or just want a discussion.\nThanks."
  },
  {
    "objectID": "posts/2019-10-17-Kaggle-Modecular-Competition.html",
    "href": "posts/2019-10-17-Kaggle-Modecular-Competition.html",
    "title": "Kaggle Molecular Competition: Lessons Learned from Finishing in Top 5",
    "section": "",
    "text": "In the recent Kaggle Predicting Molecular Properties Competition, my team has managed to finish in 5th place (out of 2749 teams). It is the first competition that I spent seriously a lot of time and I learned a lot through it. Though I don’t consider myself as a Kaggle expert by any means, I want to share some lessons, insights that hopefully can be helpful for others."
  },
  {
    "objectID": "posts/2019-10-17-Kaggle-Modecular-Competition.html#kaggle-is-the-best-school-participants-are-the-best-teachers-and-my-teammates-are-the-best-companions-ive-had",
    "href": "posts/2019-10-17-Kaggle-Modecular-Competition.html#kaggle-is-the-best-school-participants-are-the-best-teachers-and-my-teammates-are-the-best-companions-ive-had",
    "title": "Kaggle Molecular Competition: Lessons Learned from Finishing in Top 5",
    "section": "Kaggle is the best school, participants are the best teachers and my teammates are the best companions I’ve had",
    "text": "Kaggle is the best school, participants are the best teachers and my teammates are the best companions I’ve had\nKaggle is undoubtly a great platform with all sorts of interesting problems along with datasets, kernels and great discussions. Like what I saw in a post long time ago, Kaggle is definitely a home for data science enthusiats all around the world where they spend their days and nights to challenge themselves. And now I would say that I’m very proud to be one of them. Since I’m not graduated from any school formation of data science like many of others, Kaggle comes to me as a place where I can learn many things, keep me motivated in the field which is moving a lot every day. The people like Heng are the best teachers I’ve had, not only from their insights and sharing about competitions but also from the way how they work. Moreover, one of the most important features I love about Kaggle is Leaderboard where we could see where we’re standing compared to others. During this competition, I admit that the first thing I did when waking up every morning is looking at the leaderboard. Seeing many competitors passing above me in the ranking helps me to benchmark my skills, push me to learn and try new things which is very important in my career. So if you want to really get into Machine learning or Data Science, I believe that Kaggle comes in as one of the best ways."
  },
  {
    "objectID": "posts/2019-10-17-Kaggle-Modecular-Competition.html#a-good-validation-strategy-is-half-of-success",
    "href": "posts/2019-10-17-Kaggle-Modecular-Competition.html#a-good-validation-strategy-is-half-of-success",
    "title": "Kaggle Molecular Competition: Lessons Learned from Finishing in Top 5",
    "section": "1. A good validation strategy is half of success",
    "text": "1. A good validation strategy is half of success\nI read an interview of bestfitting about his strategy of winning competitions in Kaggle. He said that a good CV is half of success. I couldn’t agree more. Every try we made, it should improve both on our local CV and on the public LB. In this competition, we set aside 5000 moleculars for validation and 80000 ones for training. And luckily, validation score and public leaderboard score is very close so that we are very sure about evaluation of our models. This closeness makes us feel confident with our stacking strategy at the end.\nSo, always trust your CV more than the leaderboard. The public leaderboard represents only 29% of the actual test set, so you can’t be sure about the quality of your solution based on this percentage. Sometimes your model might be great overall, but bad on the data, specially in the public test set. The experience from here and recent finished competition make this lesson more valuable."
  },
  {
    "objectID": "posts/2019-10-17-Kaggle-Modecular-Competition.html#classical-machine-learning-ideas-possibly-work-in-deep-learning",
    "href": "posts/2019-10-17-Kaggle-Modecular-Competition.html#classical-machine-learning-ideas-possibly-work-in-deep-learning",
    "title": "Kaggle Molecular Competition: Lessons Learned from Finishing in Top 5",
    "section": "2. Classical Machine learning ideas possibly work in deep learning",
    "text": "2. Classical Machine learning ideas possibly work in deep learning\nWhen using classical machine learning models like xgboost or Lightgbm, we often heard many times about feature importance technique. While they are widely used in many tabular problems, it is less likely happen in deep learning. But my teammate Guillaume has proven the opposite. After testing feature importance (by randomize one feature at the prediction step), he noticed that the most important feature was by far the angle between an edge and the edge with the closest atom to the first edge. This insight gave us a 0.15 improvement for our best single model."
  },
  {
    "objectID": "posts/2019-10-17-Kaggle-Modecular-Competition.html#always-keep-updated-state-of-the-art-of-the-field-either-nlp-or-computer-vision",
    "href": "posts/2019-10-17-Kaggle-Modecular-Competition.html#always-keep-updated-state-of-the-art-of-the-field-either-nlp-or-computer-vision",
    "title": "Kaggle Molecular Competition: Lessons Learned from Finishing in Top 5",
    "section": "3. Always keep updated state-of-the-art of the field (either NLP or Computer Vision)",
    "text": "3. Always keep updated state-of-the-art of the field (either NLP or Computer Vision)\nI encountered many data scientists who said that since they are only working in Computer Vision, they don’t have any interest to invest their time in NLP models. For me, I don’t feel that way. When this competition was finished and the solution of top teams were released, all top 6 teams, except our team, were using a technique that is recently very popular in NLP community - Transformer. The way that they integrated Transformer in their model is quite eye opening for us."
  },
  {
    "objectID": "posts/2019-10-17-Kaggle-Modecular-Competition.html#test-ideas-with-simple-model-before-going-bigger",
    "href": "posts/2019-10-17-Kaggle-Modecular-Competition.html#test-ideas-with-simple-model-before-going-bigger",
    "title": "Kaggle Molecular Competition: Lessons Learned from Finishing in Top 5",
    "section": "4. Test ideas with simple model before going bigger",
    "text": "4. Test ideas with simple model before going bigger\nOne of the big mistakes I made during this competition is implementing a quite big models in first try. In fact, when my teammates told me about megnet models, I read the paper, write code from scatch and run it with 5 layers while trying to add some new ideas. It took me half day to run and realized that it doesn’t converge at all. Since it is quite deep, I’m kind of stuck in finding why this model doesn’t work as expected. After discussing with my team and simplifing model to only 1 layer, I figured out errors in my implementation. One of the important errors is using Normalization. Indeed, BatchNorm makes the loss fluctuate a lot while LayerNorm works much better in this case."
  },
  {
    "objectID": "posts/2019-10-17-Kaggle-Modecular-Competition.html#how-to-structure-code-when-testing-many-ideas",
    "href": "posts/2019-10-17-Kaggle-Modecular-Competition.html#how-to-structure-code-when-testing-many-ideas",
    "title": "Kaggle Molecular Competition: Lessons Learned from Finishing in Top 5",
    "section": "5. How to structure code when testing many ideas",
    "text": "5. How to structure code when testing many ideas\nOne of the problems when working in projects like Kaggle competitions is that how we can make plan of our code so that we can reproduce the results of previous ideas (even idea we tested one month ago) whenever we want. The problem will get bigger and bigger when we’re trying more sophisticated way that make taking notes difficult. The lesson I’ve learned from Heng when looking at his starter code is that create first folder of dataset which contains all dataset, seconde folder which has common functions and a third folder for my principal code. When I want to test new different idea, I will make a copy of the third model and make changes on it. Following this way can make project management easier and I can reproduce my results whenever I need."
  },
  {
    "objectID": "posts/2019-10-17-Kaggle-Modecular-Competition.html#working-in-a-team-helps-you-go-faster-and-further",
    "href": "posts/2019-10-17-Kaggle-Modecular-Competition.html#working-in-a-team-helps-you-go-faster-and-further",
    "title": "Kaggle Molecular Competition: Lessons Learned from Finishing in Top 5",
    "section": "6. Working in a team helps you go faster and further",
    "text": "6. Working in a team helps you go faster and further\nBesides learning the technical parts of the competition, a very important thing I’ve learned was how to work in a team. We all have work during the week, so we can only do Kaggle at our free time and have to do it in a smart way. Luckily, we worked at the same place, communicated every day and bounced ideas off each other. This competition is my first gold medal! I’m very glad I had this experience and very thankful to be a part of a wonderful team with Lam and Guillaume. I hope we will have more opportunities to work together in future competitions!"
  },
  {
    "objectID": "posts/2020-05-20-autoregressive-generative-models.html",
    "href": "posts/2020-05-20-autoregressive-generative-models.html",
    "title": "Autoregressive Generative Models",
    "section": "",
    "text": "Generative model is a subset of unsupervised learning which has been recieving a lot of attention for last few years. The idea is that given a training dataset, we will use models or algorithms to generate new samples with the same distribution.\n“What I cannot create, I do not understand.”\n—Richard Feynman\nSuppose we have a dataset containing images of dogs. We may wish to build a model that can generate a new image of a dog that has never existed but still looks real because the model has learned the general rules that govern the appearance of a dog. This is the kind of problem that can be solved using generative modeling.\nIn mathematical terms, generative modeling estimates \\(p(x)\\) —the probability of observing observation \\(x\\). In fact, our model tries to learn to construct an estimate \\(p_{model}(x)\\) as similar as possible to the probability density function \\(p_{data}(x)\\).\nIn this blog, we will take a deep look at one of popular approaches to tackle this problem which is Autoregressive Generative Models"
  },
  {
    "objectID": "posts/2020-05-20-autoregressive-generative-models.html#masked-autoencoder-for-distribution-estimation-made",
    "href": "posts/2020-05-20-autoregressive-generative-models.html#masked-autoencoder-for-distribution-estimation-made",
    "title": "Autoregressive Generative Models",
    "section": "Masked Autoencoder for Distribution Estimation (MADE)",
    "text": "Masked Autoencoder for Distribution Estimation (MADE)\nThe idea of MADE is built on top of autoencoder architecture. So, we’ll have a quick look on vanila autoencoder.\n\nAutoencoder\nOur primary goal is take an input sample \\(\\mathbf{x}\\) and transform it to some latent dimension \\(\\mathbf{z}\\) (encoder), which hopefully is a good representation of the original data.\nwhat is a good representation?  \n\"A good representation is one where you can reconstruct the original input!\". \nThe process of transforming the latent dimension \\(\\mathbf{z}\\) back to a reconstructed version of the input \\(\\mathbf{\\hat{x}}\\) is called the decoder. It’s an autoencoder because it’s using the same value \\(\\mathbf{x}\\) value on the input and output.\nMathematically, an encoder is mapping an input \\(\\mathbf{x}\\) to a feature vector \\(\\mathbf{h} = \\mathbf{f}_{\\theta}(\\mathbf{x})\\) while a decoder tries to map from feature space back into input space producing a reconstruction \\(\\mathbf{\\hat{x}}= \\mathbf{g}_{\\theta}(\\mathbf{h}) = \\mathbf{g}_{\\theta}\\big(\\mathbf{f}_{\\theta}(\\mathbf{x})\\big)\\). The set of parameters \\(\\theta\\) of the encoder and decoder are learned simultaneously on the task of reconstructing as well as possible the original input, i.e.attempting to incur the lowest possible reconstruction error \\(\\mathcal{L}(\\mathbf{x},\\mathbf{\\hat{x}})\\) - a measure of the discrepancy between \\(\\mathbf{x}\\) and its reconstruction \\(\\mathbf{\\hat{x}}\\) - over training examples\n\nTo train autoencoder, we use cross-entropy loss: \\[\\begin{align*}\n\\mathcal{L_{\\text{binary}}}({\\bf \\mathbf{x}}) &= \\sum_{i=1}^N -\\mathbf{x}_i\\log \\hat{\\mathbf{x}}_i - (1-\\mathbf{x}_i)\\log(1-\\hat{\\mathbf{x}_i}) \\tag{2} \\\\\n\\end{align*}\\]\nTo capture the structure of the data-generating distribution, it is therefore important that something in the training criterion or the parametrization prevents the auto-encoder from learning the identity function, which has zero reconstruction error everywhere. This is achieved through various means regularized autoencoders (refer to [3] to see more about it)\n\n\nMasked Autoencoders\nSince autoencoder is to reconstruct the input from learning a latent representation of data in an unsupervised manner, it can’t provide a proper probability distribution. Therefore, it can usually be used in applications such as denoising images but can’t generate a total new sample.\nThe reason is that in autoencoder, each output \\(\\mathbf{\\hat{x}}_i\\) could depend on any of the components input \\(\\mathbf{x}_1,…,\\mathbf{x}_n\\). So in order to convert to our autoregressive models as defined above, we can modify the structure so that \\(\\mathbf{\\hat{x}}_i\\) only depend on previous components \\(\\mathbf{x}_1,…,\\mathbf{x}_{i-1}\\) but not the future ones \\(\\mathbf{x}_{i+1},…,\\mathbf{x}_n\\).\nThe principle becomes as following: - Each output of the network \\(\\mathbf{\\hat{x}}_i\\) represents the probability distribution \\(\\mathbf{p}\\big(\\mathbf{x}_i | \\mathbf{x}_{&lt;i}\\big)\\)\n\nEach output \\(\\mathbf{\\hat{x}}_i\\) can only have connections (recursively) to smaller indexed inputs \\(\\mathbf{x}_{&lt;i}\\) and not any of the other ones.\n\nTo persuit this principle, the MADE authors came up with the idea of masked autoencoders. Since output \\(\\mathbf{\\hat{x}}_i\\) must depend only on the preceding inputs \\(\\mathbf{x}_{&lt;i}\\), it means that there must be no computational path between output unit \\(\\mathbf{\\hat{x}}_i\\) and any of the input units \\(\\mathbf{x}_i, ... \\mathbf{x}_N\\). To do so, we will zero-out the weights we don’t want by creating a binary mask matrix, whose entries that are set to 0 correspond to the connections we wish to remove.\nTake a simple case when encoder and decoder are only one layer of feed-forward.\n\\[\\begin{align*}\n{\\bf h}({\\bf x}) &= {\\bf g}({\\bf b} + {\\bf Wx}) \\\\\n{\\hat{\\bf x}} &= \\text{sigm}({\\bf c} + {\\bf V h(x)})  \n\\end{align*}\\]\nwhere - \\(\\odot\\) is an element wise product\n\n\\({\\bf x}, \\hat{\\bf x}\\) is our vectors of input/output respectively\n\\(\\bf h(x)\\) is the hidden layer\n\\(\\bf g(⋅)\\) is the activation function of the hidden layer\n\\(\\text{sigm}\\) is the sigmoid activation function of the output layer\n\\(\\bf b\\), \\(\\bf c\\) are the constant biases for the hidden/output layer respectively\n\\(\\bf W\\), \\(\\bf V\\) are the weight matrices for the hidden/output layer respectively\n\nDenote \\(\\bf M^W\\), \\(\\bf M^V\\) the masks for \\(\\bf W\\) and \\(\\bf V\\) respectively. The equations with masked autoencoders become:\n\\[\\begin{align*}\n{\\bf h}({\\bf x}) &= {\\bf g}({\\bf b} + {\\bf (W \\odot M^W)x}) \\\\\n{\\hat{\\bf x}} &= \\text{sigm}({\\bf c} + {\\bf (V \\odot M^V)h(x)})  \n\\end{align*}\n\\]\nThe last problem is only find a way to construct masks \\(\\bf M^W\\), \\(\\bf M^V\\) which sastify autoregressive property.\nLet \\(m^{l}(k)\\) be the index assigned to hidden node \\(k\\) in layer \\(l\\). The condition would be as follows:\n\nFirst, for each hidden layer \\(l\\), we sample \\(m^{l}(k)\\) from a uniform distribution with range \\([1,D−1]\\). The index \\(D\\) should be never used because nothing should depend on \\(D^{th}\\) input\nFor a given node, it only connects to nodes in the previous layer that have an index less than or equal to its index.\n\n\\[\\ M^{W^l}_{k', k} =  \\begin{cases}\n                      1 \\text{ if } m^l(k') \\geq m^{l-1}(k)  \\\\\n                      0 \\text{ otherwise}\n                    \\end{cases}\n\\\n\\] - The output mask is slightly different:\n\\[\\ M^{V}_{d, k} = \\begin{cases}\n                    1 \\text{ if } d &gt; m^{L}(k)  \\\\\n                    0 \\text{ otherwise}\n                   \\end{cases}\n\\\n\\]\n\nIn figure 2: - output 1 is not connected to anything. It will just be estimated with a single constant parameter derived from the bias node. Otherwise, output 2 is only connected to hiddens which are only connected to input 1. Finally, output 3 is connected to hiddens which come from input 1 and input 2\n\nOn the other hand, input 3 is connected to nothing because no node should depend on it (autoregressive property).\n\n\n\nDiscussion\n\nDoes ordering of input matters?\n\nActually, there is no natural ordering input. We can shuffle the input dimensions, so that MADE is able to model any arbitrary ordering.\n\nHow can we generate new samples?\n\nSampling steps: - Randomly generate vector x, set \\(i=1\\) - Feed \\(\\bf x\\) into autoencoder and generate outputs \\(\\hat{\\bf x}\\) for the network, set \\(p =\\bf \\hat{x}_i\\) - Sample from a Bernoulli distribution with parameter p, set input \\({\\bf x}_i=Bernoulli(p)\\) - Increment \\(i\\) and repeat steps 2-4 until \\(i &gt; D\\).\n\n\nImplementation\n\nThe Pytorch code implementation of MADE is borrowed from this repo.\nIn this example, We will build a network training on binarized MNIST dataset\n\n\n\nCode\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\nfrom torchvision.utils import make_grid\nimport torch\nimport torch.utils.data as data\nfrom torch import optim\nfrom torch import nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef load_data():\n    \n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        lambda x: (x &gt; 0.5).float() #binarize image\n    ])\n    train_dset = MNIST('data', transform=transform, train=True, download=True)\n    test_dset = MNIST('data', transform=transform, train=False, download=True)\n\n    train_loader = data.DataLoader(train_dset, batch_size=128, shuffle=True,\n                                   pin_memory=True, num_workers=2)\n    test_loader = data.DataLoader(test_dset, batch_size=128, shuffle=True,\n                                  pin_memory=True, num_workers=2)\n\n    return train_loader, test_loader\n\ndef plot_train_curves(epochs, train_losses, test_losses, title=''):\n    x = np.linspace(0, epochs, len(train_losses))\n    plt.figure()\n    plt.plot(x, train_losses, label='train_loss')\n    if test_losses:\n        plt.plot(x, test_losses, label='test_loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title(title)\n    plt.legend()\n    plt.show()\n\ndef visualize_batch(batch_tensor, nrow=8, title='', figsize=None):\n    grid_img = make_grid(batch_tensor, nrow=nrow)\n    plt.figure(figsize=figsize)\n    plt.title(title)\n    plt.imshow(grid_img.permute(1, 2, 0))\n    plt.axis('off')\n    plt.show()\n    \ntrain_loader, test_loader = load_data()\n\n\n\n\nCode\ndef train(model, train_loader, optimizer):\n    model.train()\n    for x, _ in train_loader:\n        loss = model.nll(x)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    return model\n\n\ndef eval_loss(model, data_loader):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for x, _ in data_loader:\n            loss = model.nll(x)\n            total_loss += loss * x.shape[0]\n        avg_loss = total_loss / len(data_loader.dataset)\n    return avg_loss.item()\n\n\ndef train_epochs(model, train_loader, test_loader, train_args):\n    epochs, lr = train_args['epochs'], train_args['lr']\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    train_losses, test_losses = [], []\n    \n    samples = model.sample(64)\n    visualize_batch(samples, title=f'Intialization')\n    for epoch in range(epochs):\n        model.train()\n\n        model = train(model, train_loader, optimizer)\n        train_loss = eval_loss(model, train_loader)\n        train_losses.append(train_loss)\n\n        if test_loader is not None:\n            test_loss = eval_loss(model, test_loader)\n            test_losses.append(test_loss)\n            \n        samples = model.sample(64)\n        if epoch % 10 == 0:    \n            print(f'Epoch {epoch} Test Loss: {test_losses[epoch] / np.log(2):.4f} bits/dim')\n            visualize_batch(samples, title=f'Epoch {epoch}')\n        \n    if test_loader is not None:\n        print('Test Loss', test_loss)\n\n    plot_train_curves(epochs, train_losses, test_losses, title='Training Curve')\n\n\n\nclass MaskedLinear(nn.Linear):\n    \"\"\" same as Linear except has a configurable mask on the weights \"\"\"\n\n    def __init__(self, in_features, out_features, bias=True):\n        super().__init__(in_features, out_features, bias)\n        self.register_buffer('mask', torch.ones(out_features, in_features))\n\n    def set_mask(self, mask):\n        self.mask.data.copy_(torch.from_numpy(mask.astype(np.uint8).T))\n\n    def forward(self, input):\n        return F.linear(input, self.mask * self.weight, self.bias)\n\n\nclass MADE(nn.Module):\n    def __init__(self, device):\n        super().__init__()\n        self.nin = 784 #28 * 28\n        self.nout = 784\n        self.hidden_sizes = [512, 512, 512]\n        self.device = device\n\n        # define a simple MLP neural net\n        self.net = []\n        hs = [self.nin] + self.hidden_sizes + [self.nout]\n        for h0, h1 in zip(hs, hs[1:]):\n            self.net.extend([\n                MaskedLinear(h0, h1),\n                nn.ReLU(),\n            ])\n        self.net.pop()  # pop the last ReLU for the output layer\n        self.net = nn.Sequential(*self.net).to(device)\n\n        self.m = {}\n        self.create_mask()  # builds the initial self.m connectivity\n\n    def create_mask(self):\n        L = len(self.hidden_sizes)\n\n        # sample uniform distribution the order of the inputs and the connectivity of all neurons\n        self.m[-1] = np.arange(self.nin)\n        for l in range(L):\n            self.m[l] = np.random.randint(self.m[l - 1].min(), self.nin - 1, size=self.hidden_sizes[l])\n\n        # construct the mask matrices\n        masks = [self.m[l - 1][:, None] &lt;= self.m[l][None, :] for l in range(L)]\n        masks.append(self.m[L - 1][:, None] &lt; self.m[-1][None, :])\n\n        # set the masks in all MaskedLinear layers\n        layers = [l for l in self.net.modules() if isinstance(l, MaskedLinear)]\n        for l, m in zip(layers, masks):\n            l.set_mask(m)\n\n    def nll(self, x):\n        x = x.view(-1, 784).to(self.device) # Flatten image\n        logits = self.net(x)\n        return F.binary_cross_entropy_with_logits(logits, x)\n\n    def sample(self, n):\n        samples = torch.zeros(n, 784).to(self.device)\n        with torch.no_grad():\n            for i in range(784):\n                logits = self.net(samples)[:, i]\n                probs = torch.sigmoid(logits)\n                samples[:, i] = torch.bernoulli(probs)\n            samples = samples.view(n, 1, 28, 28)\n        return samples.cpu()\n\n\ntrain_args = {'epochs': 20, 'lr': 0.01}\ndevice = 'cuda'\nmodel = MADE(device)\n\n\ntrain_epochs(model, train_loader, test_loader, train_args)\n\n\n\n\nEpoch 0 Test Loss: 0.2613 bits/dim\nEpoch 5 Test Loss: 0.2191 bits/dim\nEpoch 10 Test Loss: 0.2124 bits/dim\nEpoch 15 Test Loss: 0.2096 bits/dim\nTest Loss 0.14515839517116547"
  },
  {
    "objectID": "posts/2020-05-20-autoregressive-generative-models.html#pixelcnn",
    "href": "posts/2020-05-20-autoregressive-generative-models.html#pixelcnn",
    "title": "Autoregressive Generative Models",
    "section": "PixelCNN",
    "text": "PixelCNN\nPixelCNN is a deep autoregressive generative model for images. Let’s consider an image of size \\(n×n\\), each pixel in image is a data point \\(\\bf x=\\big\\{{\\bf x_1,…,x_{n^2}}\\big\\}\\). The model starts generating pixels from the top left corner, from left to right and top to bottom (raster scanning).\n\nEach pixel \\(\\bf{x}_i\\) is in turn jointly determined by three values,one for each of the color channels Red, Green and Blue (RGB). Each of the colors is thus conditioned on the other channels as well as on all the previously generated pixels. \\[\\mathbf p( \\mathbf x_i| \\mathbf x_{&lt;i}) = \\mathbf p(\\mathbf x_{i,R}|\\mathbf x_{&lt;i}). \\mathbf p(x_{i,G}|\\mathbf x_{&lt;i},\\mathbf x_{i,R}). \\mathbf p(x_{i,B}|\\mathbf x_{&lt;i},\\mathbf x_{i,R},\\mathbf x_{i,G})\\]\n\n\n\n\n\n\nNote\n\n\n\nAlong with PixelCNN, the paper authors also proposed PixelRNN with the same analogy as PixelCNN. However, PixelRNN with sequential dependency between LSTM states is very expensive for the computation. So this method will not be detailed in this blog. Check the paper if you are interested in it.\n\n\n\nMasked spatial convolution\nThe masked use the convolution filter to slide over image which multiplies each element and sums them together to produce a single response. However, we cannot use this filter because a generated pixel should not know the intensities of future pixel values. To counter this issue, we use a mask on top of the filter to only choose prior pixels and zeroing the future pixels to negate them from calculation.\n\n\n\nBlind spot\nPixelCNN masking has one problem: blind spot in receptive field because the capturing of receptive field by a CNN proceed in a triangular fashion.\n\nIn order to address the blind spot, the authors use two filters (horizontal and vertical stacks) in conjunction to allow for capturing the whole receptive ﬁeld. - Vertical stack: conditions on all the pixels in the rows above the current pixel. It doesn’t have any masking, allow the receptive field to grow in a rectangular fashion without any blind spot - Horizontal stack: conditions on the current row and takes as input the output of previous layer as well as of the vertical stack.\n\n\n\nGated PixelCNN\nThe PixelCNN only takes into consideration the neighborhood region and the depth of the convolution layers to make its predictions. To improve the performance of PixelCNN, the authors replaced the rectified linear units between the masked convolutions with the following gated activation function in order to model more complex interactions: \\[\\mathbf{y} = \\tanh (\\mathbf W_{k,f} \\ast \\mathbf{x}) \\odot \\sigma (\\mathbf W_{k,g} \\ast \\mathbf{x})\\]\nwhere:\n\\(*\\) is the convolutional operator.\n\\(\\odot\\) is the element-wise product.\n\\(\\sigma\\) is the sigmoid non-linearity\n\\(k\\) is the number of the layer\n\\(tanh(W_{k,f} \\ast \\mathbf x)\\) is a classical convolution with tanh activation function.\n\\(\\sigma(W_{k,g} \\ast \\mathbf x)\\) are the gate values (0 = gate closed, 1 = gate open).\n\\(W_{k,f}\\) and \\(W_{k,g}\\) are learned weights.\n\\(f, g\\) are the different feature maps\nA gated block is represented in Figure 6. There are 2 things to notice here: 1. the vertical stack contributes to the horizontal stack with the \\(1\\times1\\) convolution while vertical stack should not access any information horizontal stack has - otherwise it will have access to pixels it shouldn’t see. However, the vertical stack can be vertically connected as it predicts pixel following those in the vertical stack. 2. The convolutions with \\(W_f\\) and \\(W_g\\) are not combined into a single operation (which is essentially the masked convolution) to increase parallelization. The parallelization splits the \\(2p\\) features maps into two groups of \\(p\\)\n.\n\n\nConditional PixelCNN\nSometimes we want to integrate some high-level information before feed the network, for example provising an image to the network with the associated classes in CIFAR datasets. During training we feed image as well as class to our network to make sure network would learn to incorporate that information as well. During inference we can specify what class our output image should belong to.\nFor a conditional PixelCNN, we represent a provided high-level image description as a latent vector \\(\\mathbf h\\), wherein the purpose of the latent vector is to model the conditional distribution \\(p(\\mathbf{x} \\vert \\mathbf{h})\\) such that we get a probability as to if the images suites this description. The conditional PixelCNN models based on the following distribution: \\[p(\\mathbf{x} \\vert \\mathbf{h}) = \\prod_{i=1}^{n^2} p(x_i \\vert x_1, \\cdots, x_{i-1}, \\mathbf{h})\\]\nAdd terms \\(\\mathbf h\\) before the non-linearities:\n\\[ \\mathbf{y} = \\tanh (W_{k,f} \\ast \\mathbf{x}  {+ V_{k,f}^\\top \\mathbf{h}} ) \\odot \\sigma (W_{k,g} \\ast \\mathbf{x} {+ V_{k,g}^\\top \\mathbf{h} })\\]\n\nIf the latent vector \\(\\mathbf h\\) is a one-hot encoding vector that provides the class labels, which is equivalent to the adding a class dependent bias at every layer. So, the conditioning is dependent on “what should the image contain” rather than the location of contents in the image.\nTo add the location dependency to the model, we use a transposed convolution to map \\(\\mathbf h\\) to a spatial representation \\(s=deconv(\\mathbf h)\\) to produce the output \\(\\mathbf s\\) of the same shape as the image:\n\n\\[\\mathbf{y} = \\tanh (W_{k,f} \\ast \\mathbf{x}  {+ V_{k,f} \\ast \\mathbf{s}} ) \\odot \\sigma (W_{k,g} \\ast \\mathbf{x} {+ V_{k,g} \\ast \\mathbf{s} })\\]\n\n\nImplementation\n\nThe Pytorch code implementation of Gated PixelCNN is borrowed from this repo.\n\n\nPixelCNN with blind spot\n\nclass MaskConv2d(nn.Conv2d):\n    def __init__(self, mask_type, *args, **kwargs):\n        assert mask_type == 'A' or mask_type == 'B'\n        super().__init__(*args, **kwargs)\n        self.register_buffer('mask', torch.zeros_like(self.weight))\n        self.create_mask(mask_type)\n\n    def forward(self, input):\n        return F.conv2d(input, self.weight * self.mask, self.bias, self.stride,\n                        self.padding, self.dilation, self.groups)\n\n    def create_mask(self, mask_type):\n        k = self.kernel_size[0]\n        self.mask[:, :, :k // 2] = 1\n        self.mask[:, :, k // 2, :k // 2] = 1\n        if mask_type == 'B':\n            self.mask[:, :, k // 2, k // 2] = 1\n\n\nclass PixelCNN(nn.Module):\n    def __init__(self, device):\n        super().__init__()\n        model = [MaskConv2d('A', 1, 64, 7, padding=3), nn.ReLU()]\n        for _ in range(3):\n            model.extend([MaskConv2d('B', 64, 64, 7, padding=3), nn.ReLU()])\n        model.append(MaskConv2d('B', 64, 1, 7,padding=3))\n        self.net = nn.Sequential(*model).to(device)\n        self.device = device\n\n    def nll(self, x):\n        x = x.to(self.device)\n        logits = self.net(x)\n        return F.binary_cross_entropy_with_logits(logits, x)\n\n    def sample(self, n):\n        samples = torch.zeros(n, 1, 28, 28).to(self.device)\n        with torch.no_grad():\n            for r in range(28):\n                for c in range(28):\n                    logits = self.net(samples)[:, :, r, c]\n                    probs = torch.sigmoid(logits)\n                    samples[:, :, r, c] = torch.bernoulli(probs)\n        return samples.cpu()\n\n\ntrain_args = {'epochs': 21, 'lr': 0.0002}\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = PixelCNN(device)\n\n\ntrain_epochs(model, train_loader, test_loader, train_args)\n\n\n\n\nEpoch 0 Test Loss: 0.1629 bits/dim\nEpoch 10 Test Loss: 0.1217 bits/dim\nEpoch 20 Test Loss: 0.1177 bits/dim\nTest Loss 0.08157001435756683\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPixelCNN without blind spot\n\nclass HoriVertStackConv2d(nn.Module):\n    def __init__(self, mask_type, in_channels, out_channels, k=3, padding=1, \n                 gated=False, residual_horizontal=False):\n        super().__init__()\n        gm = 2 if gated else 1\n        self.gated = gated\n        self.out_channels = out_channels\n        \n        self.vertical = nn.Conv2d(in_channels, gm * out_channels, kernel_size=k,\n                                  padding=padding, bias=False)\n        self.horizontal = nn.Conv2d(in_channels, gm * out_channels, kernel_size=(1, k),\n                                    padding=(0, padding), bias=False)\n        self.vtohori = nn.Conv2d(gm * out_channels, gm * out_channels, kernel_size=1, bias=False)\n        \n        self.horizontal_output = nn.Conv2d(in_channels , out_channels, 1)\n        self.residual_horizontal = residual_horizontal\n            \n        \n        self.register_buffer('vmask', self.vertical.weight.data.clone())\n        self.register_buffer('hmask', self.horizontal.weight.data.clone())\n\n        self.vmask.fill_(1)\n        self.hmask.fill_(1)\n\n        # zero the bottom half rows of the vmask\n        self.vmask[:, :, k // 2 + 1:, :] = 0\n\n        # zero the right half of the hmask\n        self.hmask[:, :, :, k // 2 + 1:] = 0\n        if mask_type == 'A':\n            self.hmask[:, :, :, k // 2] = 0\n    \n    def _gated(self, x):\n        return torch.tanh(x[:, :self.out_channels]) * torch.sigmoid(x[:, self.out_channels:])\n    \n    def down_shift(self, x):\n        x = x[:, :, :-1, :]\n        pad = nn.ZeroPad2d((0, 0, 1, 0))\n        return pad(x)\n\n    def forward(self, x):\n        vx, h = x.chunk(2, dim=1)\n\n        self.vertical.weight.data *= self.vmask\n        self.horizontal.weight.data *= self.hmask\n            \n        vx = self.vertical(vx)\n        hx = self.horizontal(h)\n        # Allow horizontal stack to see information from vertical stack\n        hx = hx + self.vtohori(self.down_shift(vx))\n        \n        if self.gated:\n            vx = self._gated(vx)\n            hx = self._gated(hx)\n        \n        if self.residual_horizontal:\n            h = self.horizontal_output(h)\n            hx = h + hx\n        \n        return torch.cat((vx, hx), dim=1)\n\n# PixelCNN using horizontal and vertical stacks to fix blind-spot\nclass GatedHoriVertStackPixelCNN(nn.Module):\n    name = 'HoriVertStackPixelCNN'\n    def __init__(self, n_layers, device):\n        super().__init__()\n        model = [HoriVertStackConv2d('A', 1, 64, 7, padding=3), nn.ReLU()]\n        for _ in range(n_layers - 1):\n            model.extend([HoriVertStackConv2d('B', 64, 64, 7, padding=3), \n                          nn.ReLU()])\n        model.append(HoriVertStackConv2d('B', 64, 1, 7,padding=3))\n        self.net = nn.Sequential(*model).to(device)\n        self.device = device\n        \n    def forward(self, x):\n        x = x.to(self.device)\n        return self.net(torch.cat((x, x), dim=1)).chunk(2, dim=1)[1]\n    \n    def nll(self, x):\n        x = x.to(self.device)\n        logits = self(x)\n        return F.binary_cross_entropy_with_logits(logits, x)\n\n    def sample(self, n):\n        samples = torch.zeros(n, 1, 28, 28) #here we sample only one channel instead of three for simplicity\n        with torch.no_grad():\n            for r in range(28):\n                for c in range(28):\n                    logits = self(samples)[:, :, r, c]\n                    probs = torch.sigmoid(logits)\n                    samples[:, :, r, c] = torch.bernoulli(probs)\n        return samples.cpu()\n\n\ntrain_args = {'epochs': 21, 'lr': 0.0002}\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = GatedHoriVertStackPixelCNN(4, device)\n\n\ntrain_epochs(model, train_loader, test_loader, train_args)\n\n\n\n\nEpoch 0 Test Loss: 0.3259 bits/dim\nEpoch 10 Test Loss: 0.3033 bits/dim\nEpoch 20 Test Loss: 0.3007 bits/dim\nTest Loss 0.20846164226531982"
  },
  {
    "objectID": "posts/2020-05-20-autoregressive-generative-models.html#pixelcnn-1",
    "href": "posts/2020-05-20-autoregressive-generative-models.html#pixelcnn-1",
    "title": "Autoregressive Generative Models",
    "section": "PixelCNN++",
    "text": "PixelCNN++\nPixelCNN typically consists of a stack of masked convolutional layers that takes an \\(N \\times N \\times 3\\) image as input and produces \\(N \\times N \\times 3 \\times 256\\) (probability of pixel intensity) predictions as output. The softmax layer in PixelCNN to compute the conditional distribution of a sub-pixel is a full of 256-softmax. There are 2 issues with this approache: First, it is very costly in terms of memory. Second, it is missing the continuity property data. It means that the model does not know that a value of 128 is close to a value of 127 or 129 but no different than 255 for instance.\nTo address this issue, the paper authors came up with the new model including following modifications compared to PixelCNN:\nDiscretized logistic mixture likelihood\n\nFor each sub-pixel, generate a continuous distribution \\(ν\\) representing the colour intensity instead of discrete distribution. For example, ν could be a mixture of logistic distribution parameterized by \\(\\mu,s\\) and the mixture weights \\(\\pi\\). \\[\n\\nu \\sim \\sum_{i=1}^K \\pi_i logistic(\\mu_i, s_i)\n\\]\nWe then convert this intensity to a mass function by assigning regions of it to the 0 to 255 pixels:\n\n\\[\n\\ P(x|\\mu,s) =\n    \\begin{cases}\n        \\sigma(\\frac{x-\\mu+0.5}{s}) & \\text{for } x = 0 \\\\\n        \\sigma(\\frac{x-\\mu+0.5}{s}) - \\sigma(\\frac{x-\\mu-0.5}{s})\n            & \\text{for } 0 &lt; x &lt; 255 \\\\\n        1 - \\sigma(\\frac{x-\\mu-0.5}{s}) & \\text{for } x = 255\n    \\end{cases}\n\\\n\\] where \\(\\sigma\\) is the sigmoid function.\nConditioning on whole pixels\nPixelCNN factorizes the model over the 3 sub pixels according to the color(RGB) which however, complicates the model. The dependency between color channels of a pixel is relatively simple and doesn’t require a deep model to train. Therefore, it is better to condition on whole pixels instead of separate colors and then output joint distributions over all 3 channels of the predicted pixel. - We first predict the red channel using a discretized mixture of logistic - Next, we predict the green channel using a predictive distribution of the same form. Here we allow the means of the mixture components to linearly depend on the value of the red sub-pixel.\n- Finally, we model the blue channel in the same way, where we again only allow linear dependency on the red and green channels.\nDownsampling versus dilated convolution\nThe PixelCNN uses convolutions with small receptive field which is good at capturing local dependencies, but not necessarily at modeling long range structure. To overcome this, we downsample the layers by using convolutions of stride 2. Downsampling reduces input size and thus improves relative size of receptive field which leads to some loss of information but it can be compensated by adding extra short-cut connections.\nAdding short-cut connections\nThe idea is pretty the same as Unet model by introducing additional short-cut connections into the model to recover the losed informations from lower layers to higher layers of the model.\nRegularization using dropout The PixelCNN model is powerful enough to overfit on training data, leads to lower perceptual quality of images while generating data. One effective way of regularizing neural networks is dropout (Srivas-tava et al., 2014)."
  },
  {
    "objectID": "posts/2020-05-20-autoregressive-generative-models.html#wavenet",
    "href": "posts/2020-05-20-autoregressive-generative-models.html#wavenet",
    "title": "Autoregressive Generative Models",
    "section": "WaveNet",
    "text": "WaveNet\nAll the examples we’ve seen so far are images generation which is 2D data. The technique of autoregressive generative model can also be applied to 1D data such as audio or text. We will take a look on similar approaches which are based on PixelCNN idea in generating raw audio waveforms, which are signals with very high temporal resolution, at least 16,000 samples per second. One of the most popular applications of this method is text-to-speech where we have to generate a audio from text input.\nSimilarly to PixelCNNs explained above, the joint probability of a waveform \\(\\bf x=\\{x_1,...,x_T\\}\\) is factorised as a product of conditional probabilities of each audio sample \\(\\bf x_t\\) which is conditioned on the samples at all previous timesteps. The conditional probability distribution is also modelled by a stack of convolutional layers.\nTo take care of autoregressive property, the wavenet model is using casual convolution. At training time, the conditional predictions for all timesteps can be made in parallel because all timesteps of ground truth \\(\\bf x\\) are known. When generating with the model, the predictions are sequential: after each sample is predicted, it is fed back into the network to predict the next sample. However, one of the problems of causal convolutions is that they require many layers, or large filters to increase the receptive field. To mitigate this problem, Wavenet used dilated convolution\n\n\nDilated convolution\nA dilated convolution (also called à trous, or convolution with holes) is a convolution where the filter is applied over an area larger than its length by skipping input values with a certain step. This technique is broadly used to increase the receptive field by orders of magnitude, without greatly increasing computational cost. In computer vision, we’ve seen it in semantic segmentation model.\nA dilated convolution effectively allows the network to operate ona coarser scale than with a normal convolution. This is similar to pooling or strided convolutions, but here the output has the same size as the input.\n\nThe intuition is that exponentially increasing the dilation factor results in exponential receptive field growth with depth. So stack these dilated convolutions blocks further increases the model capacity and the receptive field size of the model.\n\nExcept for the dilated convolution, the Wavenet model is very similar to PixelCNN such as gated activation units technique.\n\n\nSoftmax distribution\nThe raw audio output is stored as a sequence of 16-bit scalar values (one per time step), thus the softmax output is 2^16=65,536 probabilities per timestep. WaveNet applies a μ-law companding transformation to the data and then quantize it to 256 possible values:\n\\[f(\\bf{x}_t) = \\text{sign} (x_t) \\frac{\\ln(1 + \\mu |x_t|)}{\\ln(1 + \\mu)}\\] where \\(x_t \\in (−1,1), \\mu = 255\\)\n\n\nFast WaveNet Generation\nWith the implemenentation of Wavenet in Figure 9, since the computation forms a binary tree, the overall computation time for a single output is \\(O(2^L)\\), where \\(L\\) is the number of layers in the network. When \\(L\\) is large, this is extremely undesirable. This paper proposed approach removes redundant convolution operations by caching previous calculations instead of recomputing many variables that have already been computed for previous samples, thereby reducing the complexity to \\(O(L)\\) time.\nThe below figure shows the model with 2 convolutional and 2 transposed convolutional layers with strid of 2, wherein blue dots indicate the cached states and orange bots are computed in the current step.\n\n\n\nImplementation\n\nThe Pytorch code implementation of WaveNet is borrowed from this repo.\n\n\ndef append_location(x, device):\n    \"\"\"\n        Pixel Location Appended as Features\n    \"\"\"\n    idxs = torch.arange(28).float() / 27  # Scale to [0, 1]\n    locs = torch.stack(torch.meshgrid(idxs, idxs), dim=-1)\n    locs = locs.permute(2, 0, 1).contiguous().unsqueeze(0).repeat(x.shape[0], 1, 1, 1)\n    locs = locs.to(device)\n\n    x = torch.cat((x, locs), dim=1)\n    return x\n\nclass DilatedCausalConv1d(nn.Module):\n    \"\"\"Dilated Causal Convolution for WaveNet\"\"\"\n    def __init__(self, mask_type, in_channels, out_channels, dilation=1):\n        super(DilatedCausalConv1d, self).__init__()\n        self.conv = nn.Conv1d(in_channels, out_channels,\n                              kernel_size=2, dilation=dilation, padding=0)\n        self.dilation = dilation\n        self.mask_type = mask_type\n        assert mask_type in ['A', 'B']\n\n    def forward(self, x):\n        if self.mask_type == 'A':\n            return self.conv(F.pad(x, [2, 0]))[:, :, :-1]\n        else:\n            return self.conv(F.pad(x, [self.dilation, 0]))\n\n\nclass ResidualBlock(nn.Module):\n    \"\"\" Gated Unit Activation\"\"\"\n    def __init__(self, res_channels, dilation):\n        super(ResidualBlock, self).__init__()\n\n        self.dilated = DilatedCausalConv1d('B', res_channels, 2 * res_channels, dilation=dilation)\n        self.conv_res = nn.Conv1d(res_channels, res_channels, 1)\n\n    def forward(self, x):\n        output = self.dilated(x)\n\n        # PixelCNN gate\n        o1, o2 = output.chunk(2, dim=1)\n        output = torch.tanh(o1) * torch.sigmoid(o2)\n        output = x + self.conv_res(output) # Residual network\n\n        return output\n\n\nclass WaveNet(nn.Module):\n    def __init__(self, device, append_loc=True):\n        super(WaveNet, self).__init__()\n\n        in_channels = 3 if append_loc else 1\n        out_channels = 1\n        res_channels = 32\n        layer_size = 5 # Largest dilation is 16\n        stack_size = 2\n\n        self.causal = DilatedCausalConv1d('A', in_channels, res_channels, dilation=1)\n        self.res_stack = nn.Sequential(*sum([[ResidualBlock(res_channels, 2 ** i)\n                                         for i in range(layer_size)] for _ in range(stack_size)], []))\n        self.out_conv = nn.Conv1d(res_channels, out_channels, 1)\n        self.append_loc = append_loc\n        self.device = device\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        if self.append_loc:\n            x = append_location(x, self.device)\n        output = x.view(batch_size, -1, 784)\n        output = self.causal(output)\n        output = self.res_stack(output)\n        output = self.out_conv(output)\n        return output.view(batch_size, 1, 28, 28)\n\n    def nll(self, x):\n        x = x.to(self.device)\n        logits = self(x)\n        return F.binary_cross_entropy_with_logits(logits, x)\n\n    def sample(self, n):\n        with torch.no_grad():\n            samples = torch.zeros(n, 1, 28, 28).to(self.device)\n            for r in range(28):\n                for c in range(28):\n                    logits = self(samples)[:, :, r, c]\n                    probs = torch.sigmoid(logits)\n                    samples[:, :, r, c] = torch.bernoulli(probs)\n        return samples.cpu()\n\n\ntrain_args = {'epochs': 21, 'lr': 0.0002}\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = WaveNet(device).to(device)\n\n\ntrain_epochs(model, train_loader, test_loader, train_args)\n\n\n\n\nEpoch 0 Test Loss: 0.1659 bits/dim\nEpoch 10 Test Loss: 0.1265 bits/dim\nEpoch 20 Test Loss: 0.1240 bits/dim\nTest Loss 0.08596379309892654"
  },
  {
    "objectID": "posts/2020-05-20-autoregressive-generative-models.html#self-attention-autoregressive-model",
    "href": "posts/2020-05-20-autoregressive-generative-models.html#self-attention-autoregressive-model",
    "title": "Autoregressive Generative Models",
    "section": "Self-Attention Autoregressive Model",
    "text": "Self-Attention Autoregressive Model\nRecently, we’ve witnessed a tremendous application of Transformer models in NLP. The multi-head self-attention idea behind all these models is approaching many other fields. Explaining in details how it works is out of scope of this post. If you are curious about it, I recommend to take a look on this wonderful blog.\nThe idea of using Transformer on autoregressive generative model is similar with RNN generative model. However, the Transformer is the transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs or convolution.\n\nImplementation\n\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_hid, n_position=784):\n        super(PositionalEncoding, self).__init__()\n\n        # Not a parameter\n        self.register_buffer('pos_table', self._get_sinusoid_encoding_table(n_position, d_hid))\n\n    def _get_sinusoid_encoding_table(self, n_position, d_hid):\n        ''' Sinusoid position encoding table '''\n\n        def get_position_angle_vec(position):\n            return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)]\n\n        sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n\n        return torch.FloatTensor(sinusoid_table).unsqueeze(0) * 0.1\n\n    def forward(self, x):\n        return x + self.pos_table[:, :x.size(1)].clone().detach()\n\nclass ScaledDotProductAttention(nn.Module):\n    ''' Scaled Dot-Product Attention '''\n\n    def __init__(self, temperature, attn_dropout=0.1):\n        super().__init__()\n        self.temperature = temperature\n        self.dropout = nn.Dropout(attn_dropout)\n\n    def forward(self, q, k, v, mask=None):\n\n        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n\n        if mask is not None:\n            attn = attn.masked_fill(mask == 0, -1e9)\n\n        attn = self.dropout(F.softmax(attn, dim=-1))\n        output = torch.matmul(attn, v)\n\n        return output, attn\n\nclass PositionwiseFeedForward(nn.Module):\n    ''' A two-feed-forward-layer module '''\n\n    def __init__(self, d_in, d_hid, dropout=0.1):\n        super().__init__()\n        self.w_1 = nn.Linear(d_in, d_hid) # position-wise\n        self.w_2 = nn.Linear(d_hid, d_in) # position-wise\n        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n\n        residual = x\n        x = self.layer_norm(x)\n\n        x = self.w_2(F.relu(self.w_1(x)))\n        x = self.dropout(x)\n        x += residual\n\n        return x\n\n\nclass MultiHeadAttention(nn.Module):\n    ''' Multi-Head Attention module '''\n\n    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n        super().__init__()\n\n        self.n_head = n_head\n        self.d_k = d_k\n        self.d_v = d_v\n\n        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n        self.fc = nn.Linear(n_head * d_v, d_model, bias=False)\n\n        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5)\n\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n\n    def forward(self, q, k, v, mask=None):\n        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n        sz_b, len_q, len_k, len_v = q.size(0), q.size(1), k.size(1), v.size(1)\n\n        residual = q\n        q = self.layer_norm(q)\n\n        # Pass through the pre-attention projection: b x lq x (n*dv)\n        # Separate different heads: b x lq x n x dv\n        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n\n        # Transpose for attention dot product: b x n x lq x dv\n        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n\n        if mask is not None:\n            mask = mask.unsqueeze(0).unsqueeze(0)  # For head axis broadcasting.\n\n        q, attn = self.attention(q, k, v, mask=mask)\n\n        # Transpose to move the head dimension back: b x lq x n x dv\n        # Combine the last two dimensions to concatenate all the heads together: b x lq x (n*dv)\n        q = q.transpose(1, 2).contiguous().view(sz_b, len_q, -1)\n        q = self.dropout(self.fc(q))\n        q += residual\n\n        return q\n\nclass DecoderLayer(nn.Module):\n    ''' Compose with three layers '''\n\n    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n        super(DecoderLayer, self).__init__()\n        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n\n    def forward(self, dec_input, mask=None):\n        dec_output = self.slf_attn(dec_input, dec_input, dec_input, mask=mask)\n        dec_output = self.pos_ffn(dec_output)\n        return dec_output\n\nclass Transformer(nn.Module):\n    ''' A decoder model with self attention mechanism. '''\n\n    def __init__(self, device, mode='none'):\n\n        super().__init__()\n        n_layers = 2\n        self.input_size = 3 if mode == 'pixel_location' else 1\n\n        if mode == 'pos_encoding':\n            self.pos_enc = PositionalEncoding(1, n_position=784)\n        self.fc_in = nn.Linear(self.input_size, 64)\n        self.layer_stack = nn.ModuleList([\n            DecoderLayer(64, 64, 1, 16, 64, dropout=0.1)\n            for _ in range(n_layers)])\n        self.fc_out = nn.Linear(64, 1)\n\n        self.register_buffer('mask', torch.zeros(784, 784))\n        for i in range(784):\n            self.mask[i, :i] = 1\n\n        self.mode = mode\n        self.device = device\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        if self.mode == 'pixel_location':\n            x = append_location(x, self.device)\n            x = x.permute(0, 2, 3, 1).view(batch_size, 784, self.input_size)\n        elif self.mode == 'pos_encoding':\n            x = x.view(batch_size, 784, self.input_size)\n            x = self.pos_enc(x)\n        else:\n            x = x.view(batch_size, 784, self.input_size)\n        x = torch.cat((torch.zeros(batch_size, 1, self.input_size).to(self.device), x[:, :-1]), dim=1)\n        # -- Forward\n        x = F.relu(self.fc_in(x))\n        for i, dec_layer in enumerate(self.layer_stack):\n            x = dec_layer(x, mask=self.mask)\n        x = self.fc_out(x)\n        x = x.view(batch_size, 1, 28, 28)\n        return x\n\n    def nll(self, x):\n        x = x.to(self.device)\n        logits = self(x)\n        return F.binary_cross_entropy_with_logits(logits, x)\n\n    def sample(self, n):\n        samples = torch.zeros(n, 1, 28, 28).to(self.device)\n        with torch.no_grad():\n            for r in range(28):\n                for c in range(28):\n                    logits = self(samples)[:, :, r, c]\n                    probs = torch.sigmoid(logits)\n                    samples[:, :, r, c] = torch.bernoulli(probs)\n        return samples.cpu()\n\n\ntrain_args = {'epochs': 21, 'lr': 0.0002}\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = Transformer(device, mode='pixel_location').to(device)\n\n\ntrain_epochs(model, train_loader, test_loader, train_args)\n\n\n\n\nEpoch 0 Test Loss: 0.2513 bits/dim\nEpoch 10 Test Loss: 0.1693 bits/dim\nEpoch 20 Test Loss: 0.1486 bits/dim\nTest Loss 0.10297279804944992"
  },
  {
    "objectID": "posts/2020-05-20-autoregressive-generative-models.html#pixelsnail",
    "href": "posts/2020-05-20-autoregressive-generative-models.html#pixelsnail",
    "title": "Autoregressive Generative Models",
    "section": "PixelSNAIL",
    "text": "PixelSNAIL\nWe’ve already covered a lot til now (Yeah! I know…). The last model we will review is PixelSNAIL which adopt masked self-attention approaches inspired by SNAIL.\nThe key idea behind PixelSNAIL is to introduce attention blocks, in a style similar to Self Attention, into neural autoregressive modelling.\nThe PixelSNAIL model is composed of two blocks:\n\nResidual block: This block is as same as gated unit block in PixelCNN that we’ve seen so far.\nAttention block: This block performs a single key-value lookup. It projects the input to a lower dimensionality to produce the keys and values and then uses softmax-attention like in Transformer model.\n\n\nThe way how PixelSNAIL integrated self-attention blocks is quite interesting because it allows to model long-range depenencies between pixels in image, equip all conditionals with the ability to refer to all of their available context. Moreover, each conditional can access any pixels in its context through the attention operator, easy information access of remote pixels improves modeling of long-range statistics.\n\n\nFlexible ordering\nWe’ve seen in PixelCNN or other architectures, the pixel ordering is raster scanning where along each row left pixels come before right pixels and top rows come before bottom rows. The raster scan order-ing only has a small number neighboring pixels available in the conditioning context \\(\\bf x_1, . . . , x_{i−1}\\): only to the left and above and most of the context is wasted on regions that might have little correlation with the current pixel like the far top-right corner. Since we can access any pixels in its context through attention operator, PixelSNAIL allows zigzag ordering."
  },
  {
    "objectID": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html",
    "href": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html",
    "title": "Everything you need to know about Llama 2",
    "section": "",
    "text": "LLaMA 2, the successor of the original LLaMA 1, is a massive language model created by Meta. It is open for both research and commercial purposes, made available through various providers like AWS and Hugging Face. The pretrained models of LLaMA 2 have undergone extensive training on an impressive 2 trillion tokens, offering twice the context length compared to LLaMA 1. Additionally, its fine-tuned models have been refined using over 1 million human annotations."
  },
  {
    "objectID": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#is-it-trully-open-sourced",
    "href": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#is-it-trully-open-sourced",
    "title": "Everything you need to know about Llama 2",
    "section": "Is it trully open sourced?",
    "text": "Is it trully open sourced?\nTechnically, the whole project is not open-source because the development and use of it is not fully available to the entire public. While the model is open to public, it is very useful for the open-source community, but we should call it an open release instead of open source."
  },
  {
    "objectID": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#what-is-llama-2-base-and-llama-2-chat",
    "href": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#what-is-llama-2-base-and-llama-2-chat",
    "title": "Everything you need to know about Llama 2",
    "section": "What is Llama 2 base and Llama 2 chat?",
    "text": "What is Llama 2 base and Llama 2 chat?\nThe base models are uncensored, and are not instruct-tuned or chat-tuned.\nThe chat models are censored, and have been chat-tuned, are optimized for dialogue use cases."
  },
  {
    "objectID": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#how-to-extend-context-for-llama-2-beyond-4k",
    "href": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#how-to-extend-context-for-llama-2-beyond-4k",
    "title": "Everything you need to know about Llama 2",
    "section": "How to extend context for LLama 2, beyond 4K?",
    "text": "How to extend context for LLama 2, beyond 4K?\nWe can extend context from 4K to 8k, 32k or 128k tokens with technique using Position Interpolation\n\nExtending Context is Hard…but not Impossible: awesome blog on the subject\nLong-Context: Extending LLM Context Length. A range of experiments with different schemes for extending context length capabilities of Llama, which has been pretrained on 2048 context length with the RoPE (Rotary Position Embedding) encoding."
  },
  {
    "objectID": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#what-is-training-data-for-base-model",
    "href": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#what-is-training-data-for-base-model",
    "title": "Everything you need to know about Llama 2",
    "section": "What is training data for base model?",
    "text": "What is training data for base model?\n\nOur training corpus includes a new mix of data from publicly available sources, which does not include data from Meta’s products or services. We made an effort to remove data from certain sites known to contain a high volume of personal information about private individuals. We trained on 2 trillion tokens of data as this provides a good performance–cost trade-off, up-sampling the most factual sources in an effort to increase knowledge and dampen hallucinations. source\n\nThat’s simply all details that Meta gives in their paper, although we are very curious about which datasets contain a high volume of personal information or their detailed technique about up-sampling the factural sources."
  },
  {
    "objectID": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#what-is-preference-data-for-reward-model",
    "href": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#what-is-preference-data-for-reward-model",
    "title": "Everything you need to know about Llama 2",
    "section": "What is preference data for reward model?",
    "text": "What is preference data for reward model?\nFirst of all, the reward model is the key of RLHF. In order to get a good reward model, Meta had to push hard on gathering preference data extremely upgraded from what the open-source community is working with.\nIn summary, the key points about preference data:\n\nUse multi-turn preferences, where model responses are taken from different model checkpoints with varying temperatures to generate diversity between pairs.\nbinary comparison: either their choice is significantly better, better, slightly better, or negligibly better/ unsure.\nFocus on helpfulness and safety (as opposed to honesty), using separate guidelines at data collection time for each data vendor (e.g. safety is often a much more deceptive prompting style). This is most contrasted to Anthropic’s works, where they train a model that is Helpful, Honest, and Harmless.\nIterative collection for distribution management: “Human annotations were collected in batches on a weekly basis. As we collected more preference data, our reward models improved, and we were able to train progressively better versions for Llama 2-Chat”\nThe team added additional safety metadata to the collection showcasing which responses are safe from the models at each turn. When this is passed to the modeling phase, they “do not include any examples where the chosen response was unsafe and the other response safe, as we believe safer responses will also be better/preferred by humans.”\n\n\n\n\nCapture d’écran 2023-08-03 à 00.27.00.png"
  },
  {
    "objectID": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#how-to-train-reward-models",
    "href": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#how-to-train-reward-models",
    "title": "Everything you need to know about Llama 2",
    "section": "How to train reward models?",
    "text": "How to train reward models?\nThe reward model takes a model response and its corresponding prompt (including contexts from previous turns) as inputs and outputs a scalar score to indicate the quality (e.g.,helpfulness and safety) of the model generation.\nThey train two separate reward models, one optimized for helpfulness (referred to as Helpfulness RM) and another for safety (Safety RM).\nThey initialize reward models from pretrained chat model checkpoints, as it ensures that both models benefit from knowledge acquired in pretraining. In short, the reward model “knows” what the chat model knows. The model architecture and hyper-parameters are identical to those of the pretrained language models, except that the classification head for next-token prediction is replaced with a regression head for outputting a scalar reward."
  },
  {
    "objectID": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#what-is-training-process-of-llama-2-chat",
    "href": "posts/Llama-2/2023-08-21-Everything you need to know about Llama 2.html#what-is-training-process-of-llama-2-chat",
    "title": "Everything you need to know about Llama 2",
    "section": "What is training process of Llama 2-chat?",
    "text": "What is training process of Llama 2-chat?\n\n\n\nUntitled"
  },
  {
    "objectID": "posts/2020-06-07-tabular_feature_engineering_tricks.html",
    "href": "posts/2020-06-07-tabular_feature_engineering_tricks.html",
    "title": "Tabular feature engineering tricks",
    "section": "",
    "text": "One of the most important challenge when working with tabular dataset before feeding into the machine learning model is preprocessing data because the quality of data and the useful information that can be derived from it directly affects the ability of model to learn. In the real world application, the raw dataset is kind of messy which needs some skills to clean it.\nThis blog will cover some tricks that I found quite useful when dealing with messy data."
  },
  {
    "objectID": "posts/2020-06-07-tabular_feature_engineering_tricks.html#pandas-tricks",
    "href": "posts/2020-06-07-tabular_feature_engineering_tricks.html#pandas-tricks",
    "title": "Tabular feature engineering tricks",
    "section": "Pandas tricks",
    "text": "Pandas tricks\nMost of the time when doing tabular dataset we will usually import data into dataframe of pandas. Knowing how to deal with pandas will save us a lot of time of computation with better code readability and versatility.\n\nPandas Profile for data exploration\nPandas profiling is an open source Python module with which we can quickly do an exploratory data analysis with just a few lines of code.\n\nfrom pandas_profiling import ProfileReport\n\nprofile = ProfileReport(dataframe)\n\n\n\n\nConfigure Options & Settings\nWe usually have to work with a dataframe containing many columns which will collape when reading it. If we want to read all columns of dataframe, we can configure its options at interpreter startup.\n\npd.options.display.max_columns = 500\n\n#or \n\npd.set_option('display.max_columns', 500)\n\n\n\nSplit a column into multiple columns\nSome dataframe columns contain concatenation of data which should be splitted into multiple columns\n\ndf = pd.DataFrame({'Name_Age': ['Smith_32', 'Nadal', \n                           'Federer_36']})\n\n\ndf\n\n\n\n\n\n\n\n\nName_Age\n\n\n\n\n0\nSmith_32\n\n\n1\nNadal\n\n\n2\nFederer_36\n\n\n\n\n\n\n\n\ndf['Name_Age'].str.split('_', expand=True)\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nSmith\n32\n\n\n1\nNadal\nNone\n\n\n2\nFederer\n36\n\n\n\n\n\n\n\nIf we want to take only first column of our splits:\n\ndf['Name_Age'].str.split('_').str[1]\n\n0     32\n1    NaN\n2     36\nName: Name_Age, dtype: object\n\n\n\n\nExtract dummy variables from string columns\n\ndf = pd.Series(['Sector 1;Sector 2', 'Sector 1;Sector 3', np.nan, 'Sector 2;Sector 4'], dtype=str)\n\n\ndf\n\n0    Sector 1;Sector 2\n1    Sector 1;Sector 3\n2                  NaN\n3    Sector 2;Sector 4\ndtype: object\n\n\n\ndf.str.get_dummies(';')\n\n\n\n\n\n\n\n\nSector 1\nSector 2\nSector 3\nSector 4\n\n\n\n\n0\n1\n1\n0\n0\n\n\n1\n1\n0\n1\n0\n\n\n2\n0\n0\n0\n0\n\n\n3\n0\n1\n0\n1\n\n\n\n\n\n\n\n\n\nRegular expression with text\n\ndf = pd.DataFrame({'Location' : [\n    'Washington, D.C. 20003',\n    'Brooklyn, NY 11211-1755',\n    'Omaha, NE 68154',\n    'Pittsburgh, PA 15211'\n    ]})\n\n\ndf\n\n\n\n\n\n\n\n\nLocation\n\n\n\n\n0\nWashington, D.C. 20003\n\n\n1\nBrooklyn, NY 11211-1755\n\n\n2\nOmaha, NE 68154\n\n\n3\nPittsburgh, PA 15211\n\n\n\n\n\n\n\nIf we want to separate out the three city/state/ZIP components neatly into DataFrame fields, we should pass regex extraction into .str.extract:\n\nregex = (r'(?P&lt;city&gt;[A-Za-z ]+), '      # One or more letters\n         r'(?P&lt;state&gt;[A-Z]{2}) '        # 2 capital letters\n         r'(?P&lt;zip&gt;\\d{5}(?:-\\d{4})?)')  # Optional 4-digit extension\n\n\ndf['Location'].str.replace('.', '').str.extract(regex)\n\n\n\n\n\n\n\n\ncity\nstate\nzip\n\n\n\n\n0\nWashington\nDC\n20003\n\n\n1\nBrooklyn\nNY\n11211-1755\n\n\n2\nOmaha\nNE\n68154\n\n\n3\nPittsburgh\nPA\n15211\n\n\n\n\n\n\n\n\nNote: .str is called accessor for string (object) data. It maps to the class StringMethods which contains a lot of methods like cat, split, rsplit, replace, extract…\n\n\n\nEnhance performance with Cython\nWe usually use the lambda function to apply for each row in dataframe. Sometimes, it is not efficient in term of computation time. Actually we can boost the performance by using Cython extention.\n\ndf = pd.DataFrame({'a': np.random.randn(1000),\n                   'b': np.random.randn(1000),\n                   'N': np.random.randint(100, 1000, (1000)),\n                   'x': 'x'})\n\n\ndef f(x):\n    return x * (x - 1)\ndef integrate_f(a, b, N):\n    s = 0\n    dx = (b - a) / N\n    for i in range(N):\n        s += f(a + i * dx)\n    return s * dx\n\n\n%timeit df.apply(lambda x: integrate_f(x['a'], x['b'], x['N']), axis=1)\n\n167 ms ± 1.58 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nNow we will pass the lambda function into Cython extension.\n\n%load_ext Cython\n\nThe Cython extension is already loaded. To reload it, use:\n  %reload_ext Cython\n\n\n\n%%cython\ndef f(x):\n    return x * (x - 1)\ndef integrate_f(a, b, N):\n    s = 0\n    dx = (b - a) / N\n    for i in range(N):\n        s += f(a + i * dx)\n    return s * dx\n\n\n%timeit df.apply(lambda x: integrate_f(x['a'], x['b'], x['N']), axis=1)\n\n120 ms ± 777 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nBy simply putting cython extension before the lambda function, we reduce a significant amount of computation time\n\n\nGroupby\nGroupby is one of the powerful methods which allows us to split the data into groups based on some criteria, compute statistic aggregation on each group or do transformation like standalizing the data within a group and many more.\n\ndata = {'Team': ['Riders', 'Riders', 'Devils', 'Devils', 'Kings',\n   'kings', 'Kings', 'Kings', 'Riders', 'Royals', 'Royals', 'Riders'],\n   'Rank': [1, 2, 2, 3, 3,4 ,1 ,1,2 , 4,1,2],\n   'Year': [2014,2015,2014,2015,2014,2015,2016,2017,2016,2014,2015,2017],\n   'Points':[876,789,863,673,741,812,756,788,694,701,804,690]}\n\ndf = pd.DataFrame(data)\n\n\ndf\n\n\n\n\n\n\n\n\nTeam\nRank\nYear\nPoints\n\n\n\n\n0\nRiders\n1\n2014\n876\n\n\n1\nRiders\n2\n2015\n789\n\n\n2\nDevils\n2\n2014\n863\n\n\n3\nDevils\n3\n2015\n673\n\n\n4\nKings\n3\n2014\n741\n\n\n5\nkings\n4\n2015\n812\n\n\n6\nKings\n1\n2016\n756\n\n\n7\nKings\n1\n2017\n788\n\n\n8\nRiders\n2\n2016\n694\n\n\n9\nRoyals\n4\n2014\n701\n\n\n10\nRoyals\n1\n2015\n804\n\n\n11\nRiders\n2\n2017\n690\n\n\n\n\n\n\n\n\ndf.groupby('Year')['Points'].agg(['mean', 'sum', 'min', 'max', 'std', 'var', 'count'])\n\n\n\n\n\n\n\n\nmean\nsum\nmin\nmax\nstd\nvar\ncount\n\n\nYear\n\n\n\n\n\n\n\n\n\n\n\n2014\n795.25\n3181\n701\n876\n87.439026\n7645.583333\n4\n\n\n2015\n769.50\n3078\n673\n812\n65.035888\n4229.666667\n4\n\n\n2016\n725.00\n1450\n694\n756\n43.840620\n1922.000000\n2\n\n\n2017\n739.00\n1478\n690\n788\n69.296465\n4802.000000\n2\n\n\n\n\n\n\n\nWe can also aggregate using the numpy function like np.size, np.mean, np.max…\n\ndf.groupby('Team').agg(np.size)\n\n\n\n\n\n\n\n\nRank\nYear\nPoints\n\n\nTeam\n\n\n\n\n\n\n\nDevils\n2\n2\n2\n\n\nKings\n3\n3\n3\n\n\nRiders\n4\n4\n4\n\n\nRoyals\n2\n2\n2\n\n\nkings\n1\n1\n1\n\n\n\n\n\n\n\n\ndf.groupby('Team').transform(lambda x: (x - x.mean()) / x.std()*10)\n\n\n\n\n\n\n\n\nRank\nYear\nPoints\n\n\n\n\n0\n-15.000000\n-11.618950\n12.843272\n\n\n1\n5.000000\n-3.872983\n3.020286\n\n\n2\n-7.071068\n-7.071068\n7.071068\n\n\n3\n7.071068\n7.071068\n-7.071068\n\n\n4\n11.547005\n-10.910895\n-8.608621\n\n\n5\nNaN\nNaN\nNaN\n\n\n6\n-5.773503\n2.182179\n-2.360428\n\n\n7\n-5.773503\n8.728716\n10.969049\n\n\n8\n5.000000\n3.872983\n-7.705963\n\n\n9\n7.071068\n-7.071068\n-7.071068\n\n\n10\n-7.071068\n7.071068\n7.071068\n\n\n11\n5.000000\n11.618950\n-8.157595\n\n\n\n\n\n\n\n\ndf.groupby('Team').filter(lambda x: len(x) &gt;= 3)\n\n\n\n\n\n\n\n\nTeam\nRank\nYear\nPoints\n\n\n\n\n0\nRiders\n1\n2014\n876\n\n\n1\nRiders\n2\n2015\n789\n\n\n4\nKings\n3\n2014\n741\n\n\n6\nKings\n1\n2016\n756\n\n\n7\nKings\n1\n2017\n788\n\n\n8\nRiders\n2\n2016\n694\n\n\n11\nRiders\n2\n2017\n690"
  },
  {
    "objectID": "posts/2020-06-07-tabular_feature_engineering_tricks.html#sklearn-pipeline",
    "href": "posts/2020-06-07-tabular_feature_engineering_tricks.html#sklearn-pipeline",
    "title": "Tabular feature engineering tricks",
    "section": "Sklearn pipeline",
    "text": "Sklearn pipeline\nOne of the most beautiful things I love about sklearn is its creation of pipeline. I found it very neat to use, easily for understanding and particularly very helpful for production.\n&gt;Note: Definition of **pipeline** class according to scikit-learn:\nSequentially apply a list of transforms and a final estimator. Intermediate steps of pipeline must implement fit and transform methods and the final estimator only needs to implement fit. \n\npipeline = Pipeline([('preprocessing', ColumnTransformer(\n                                        transformers = [\n                                            ('text', Pipeline([\n                                                ('tfidf', TfidfVectorizer(stop_words=['nan'])),\n                                            ]), TEXT_COLUMNS),\n                                            ('cat', \n                                             FeatureUnion([\n                                                    ('ordinal', OrdinalEncoder()),\n                                                    ('target_encoder', TargetEncoder())\n                                            ]), CAT_COLUMNS),\n                                            ('num', TruncatedSVD(n_components=100) , \n                                            NUM_COLUMNS)\n                                                    ],\n                                            remainder='drop')\n                     ),\n                     ('model', LGBMClassifier()\n                     )\n                    ])\n\npipeline.fit(Xtrain, y_train)\n\nLooking at the pipeline allows to understand right away what we want to do with our data. The example above can be interpreted as the schema below:\n\n\n\nimage.png\n\n\nWe can also pass the whole pipeline into other pipeline like RandomizedSearchCV or GridSearchCV\n\nparams_grid = {\n    'model__colsample_bytree': [0.3, 0.5, 0.7, 0.9],\n    'model__n_estimators' : [2000, 5000, 8000],\n    'model__learning_rate': [0.01, 0.02, 0.05, 0.1, 0.2],\n    'model__max_depth' : [3, 5, 7],\n    'preprocessing__num__n_components' : [100, 50, 70],\n    'preprocessing__text__select__estimator__C' : [1e-2, 1e-1, 1],\n    'preprocessing__text__select__max_features' : [10, 20, 50, None],\n    'preprocessing__text__tfidf__binary' : [False, True],\n    'preprocessing__text__tfidf__ngram_range' : [(1, 1), (1, 2)],\n    'preprocessing__text__tfidf__max_df': [0.2, 0.4, 0.6],\n    'preprocessing__text__tfidf__min_df': [20, 50]\n}\n\n\nsearch = RandomizedSearchCV(estimator=pipeline,\n                            param_distributions=params_grid,\n                            n_iter=100,\n                            n_jobs=1,\n                            cv = 5,\n                            verbose=5,\n                            scoring='roc_auc')\n\nsearch.fit(Xtrain, y_train)"
  },
  {
    "objectID": "posts/2020-06-07-tabular_feature_engineering_tricks.html#reference",
    "href": "posts/2020-06-07-tabular_feature_engineering_tricks.html#reference",
    "title": "Tabular feature engineering tricks",
    "section": "Reference",
    "text": "Reference\n\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/\nhttps://realpython.com/pandas-groupby/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine learning Blog",
    "section": "",
    "text": "Everything you need to know about Llama 2\n\n\n\n\n\n\n\nLLM\n\n\nLlama 2\n\n\n\n\nLLaMA 2, the successor of the original LLaMA 1, is a large language model created by Meta.\n\n\n\n\n\n\nAug 21, 2023\n\n\n\n\n\n\n  \n\n\n\n\nTabular feature engineering tricks\n\n\n\n\n\n\n\ntabular\n\n\nfeature-engineering\n\n\n\n\nSome useful tricks when doing feature engineering on tabular dataset\n\n\n\n\n\n\nJun 7, 2020\n\n\n\n\n\n\n  \n\n\n\n\nAutoregressive Generative Models\n\n\n\n\n\n\n\ngenerative\n\n\nautoregressive\n\n\ndeeplearning\n\n\n\n\nA complete overview on Autoregressive Generative Models (MADE, PixelCNN families, WaveNet, Seft-Attention, PixelSNAIL) with implementation code\n\n\n\n\n\n\nMay 20, 2020\n\n\n\n\n\n\n  \n\n\n\n\nKaggle Molecular Competition: Lessons Learned from Finishing in Top 5\n\n\n\n\n\n\n\nkaggle\n\n\ngraph neural network\n\n\n\n\nTricks and lessons I have learned from getting into top 5 of Kaggle Molecular Competition\n\n\n\n\n\n\nOct 17, 2019\n\n\n\n\n\n\nNo matching items"
  }
]